% Encoding: UTF-8

@Article{Amenc-et-al-2014a,
  author               = {Amenc, Noel and Goltz, Felix and Lodh, Ashish and Martellini, Lionel},
  date                 = {2014-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Towards Smart Equity Factor Indices: Harvesting Risk Premia without Taking Unrewarded Risks},
  doi                  = {10.3905/jpm.2014.40.4.106},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {106--122},
  volume               = {40},
  abstract             = {This article argues that current smart-beta investment approaches provide only a partial answer to the main shortcomings of capitalization-weighted indices and develops a new approach to equity investing, which the authors refer to as smart-factor investing. The authors then provide an assessment of the benefits of simultaneously addressing the two main problems of cap-weighted indices their undesirable factor exposures and their heavy concentration by constructing factor indices that explicitly seek exposures to rewarded risk factors, while diversifying away unrewarded risks.

The results suggest that such smart-factor indices lead to considerable improvements in risk-adjusted performance. For long-term U.S. data, smart-factor indices for a range of different factor tilts consistently outperform cap-weighted, factor-tilted indices. Compared with the broad cap-weighted index, smart-factor indices roughly double the risk-adjusted return (Sharpe ratio).

Outperformance of such indices persists at levels ranging from 2.92 percent to 4.46 percent annually, even when assuming unrealistically high transaction costs. Moreover, by providing explicit tilts to consensual factors, such indices improve upon many current smart-beta offerings where, more often than not, factor tilts exist as unintended consequences of ad hoc methodologies.},
  citeulike-article-id = {13972207},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2014.40.4.106},
  groups               = {RiskPremia_Equity},
  owner                = {cristi},
  posted-at            = {2016-03-08 09:17:35},
  timestamp            = {2020-02-25 22:17},
}

@Article{Baltas-Karyampas-2018,
  author               = {Baltas, Nick and Karyampas, Dimitrios},
  date                 = {2018-11},
  journaltitle         = {Journal of Financial Markets},
  title                = {Forecasting the equity risk premium: The importance of regime-dependent evaluation},
  doi                  = {10.1016/j.finmar.2017.11.002},
  issn                 = {1386-4181},
  number               = {March},
  pages                = {83--102},
  volume               = {38},
  abstract             = {Asset allocation is critically dependent on the ability to forecast the equity risk premium (ERP) out-of-sample. But, is superior econometric predictability across the business cycle synonymous with predictability at all times? We evaluate recently introduced ERP forecasting models, which have been shown to generate econometrically superior ERP forecasts, and find that their forecasting ability is regime-dependent. They give rise to significant relative losses during market downturns, when it matters the most for asset allocators to retain assets and their client base intact. Conversely, any economic benefit occurring during market upswings is diminished for high risk-averse and leverage-constrained investors.},
  citeulike-article-id = {14483005},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.finmar.2017.11.002},
  groups               = {Predictability_Return_Other, RiskPremia_Equity, RiskPremia_Forecast, RiskPremia_Regime, Regime_Model, Invest_Regime, FrcstQWIM_MedLngTerm, FcstQWIM_Equity, FrcstQWIM_Test, RiskRet_BusCycle},
  posted-at            = {2017-11-25 19:12:44},
  timestamp            = {2020-02-25 22:17},
}

@Article{Bianchi-et-al-2015,
  author               = {Bianchi, Robert J. and Drew, Michael E. and Walk, Adam N.},
  date                 = {2015-11},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The (Un)Predictable Equity Risk Premium},
  url                  = {https://ssrn.com/abstract=2694373},
  abstract             = {The equity risk premium (ERP) remains one of the most hotly contested ideas in finance. The disagreement, in practical and theoretical terms, centres on how best to measure the risk of an investment, how to convert this risk measure into an expected return that compensates the investor for holding that risk, and its degree of predictability. This paper provides Australian evidence for the period 1900 through 2014 and forward looking estimates.},
  citeulike-article-id = {13996953},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2694373},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2694373code457363.pdf?abstractid=2694373 and mirid=1},
  day                  = {25},
  groups               = {RiskPremia_Equity},
  owner                = {cristi},
  posted-at            = {2016-04-04 15:22:21},
  timestamp            = {2020-02-25 22:17},
}

@Article{Chaieb-et-al-2018,
  author         = {Chaieb, I and Langlois, H and Scaillet, O},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Time-Varying Risk Premia in Large International Equity Markets},
  url            = {https://ssrn.com/abstract=3103752},
  abstract       = {We estimate international factor models with time-varying factor exposures and risk premia at the individual stock level using a large unbalanced panel of 58,674 stocks in 46 countries over the 1985-2017 period. We consider market, size, value, momentum, profitability, and investment factors aggregated at the country, regional, and world level. The country market in excess of the world or regional market is required in addition to world or regional factors to capture the factor structure for both developed and emerging markets. We do not reject mixed CAPM models with regional and excess country market factors for 76\% of the countries. We do not reject mixed multi-factor models in 80\% to 94\% of countries. Value and momentum premia show more variability over time and across countries than profitability and investment premia. The excess country market premium is statistically significant in many developed and emerging markets but economically larger in emerging markets.},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_Equity},
  timestamp      = {2020-02-25 22:17},
}

@Article{Chen-et-al-2013b,
  author               = {Chen, Ming-Hsien and Lee, Chun I. and Tai, Vivian W.},
  date                 = {2013-08},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Explaining Equity Risk Premium during Financial Crises},
  url                  = {https://ssrn.com/abstract=2313664},
  abstract             = {This paper investigates the dynamics among three non-equity factors, credit, illiquidity, and foreign exchange risks, and equity returns to explore the equity risk premium. Results from both VAR and EGARCHM models demonstrate that credit and liquidity risk premia and changes in exchange rates explain equity returns in Germany, Japan, the United Kingdom, and the United States during recent financial crises. More importantly, the traditional measure of the equity market risk premium ceases to be significant in explaining equity returns when these three non-equity factors are included in the model. Although its explaining power is significant in the US, its significant level is lower. Our results offer convincing evidence that these three non-equity factors explain the equity risk premium during financial crises.},
  citeulike-article-id = {14025179},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2313664},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2318566code2108777.pdf?abstractid=2313664 and mirid=1},
  day                  = {22},
  groups               = {RiskPremia_Equity, RiskPremia_Other},
  owner                = {cristi},
  posted-at            = {2016-04-30 17:22:58},
  timestamp            = {2020-02-25 22:17},
}

@Article{Arnott-Bernstein-2002,
  author       = {Arnott, Robert D. and Bernstein, Peter L.},
  date         = {2002},
  journaltitle = {SSRN Electronic Journal},
  title        = {What Risk Premium is Normal?},
  abstract     = {We are in an industry that thrives on the expedient of forecasting the future by extrapolating the past. As a consequence, investors have grown accustomed to the idea that stocks normally produce an 8 percent real return and a 5 percent risk premium over bonds, compounded annually over many decades. Why? Because long-term historical returns have been in this range, with impressive consistency. Because investors see these same long-term historical numbers, year after year, these expectations are now embedded into the collective psyche of the investment community. Both figures are unrealistic from current market levels. Few have acknowledged that an important part of the lofty real returns of the past has stemmed from rising valuation levels and from high dividend yields which have since diminished.

As this article will demonstrate, the long-term forward-looking risk premium is nowhere near the 5 percent of the past; indeed, it may well be near-zero today, perhaps even negative. Credible studies, in the US and overseas, are now challenging this flawed conventional view, in well-researched studies by Claus and Thomas [2001] and Fama and French [2000, Working Paper], to name just two. Similarly, the long-term forward-looking real return from stocks is nowhere near history s 8 percent.

Our argument will show that, barring unprecedented economic growth or unprecedented growth in earnings as a percentage of the economy, real stock returns will probably be roughly 2-4 similar to bonds. Indeed, even this low real return figure assumes that current near-record valuation levels are fair, and likely to remain this high in the years ahead. Reversion to the mean would push future real returns lower still. Furthermore, if we examine the historical record, neither the 8 percent real return nor the 5 percent risk premium for stocks relative to government bonds has ever been a realistic expectation (except from major market bottoms or at times of crisis, such as wartime). Should investors require an 8 percent real return, or should a 5 percent risk premium be necessary to induce an investor to bear stock market risk? These returns and risk premiums are so grand that investors should perhaps have bid them away a long time ago - indeed, they may have done so in the immense bull market of 1982-1999.

Intuition suggests that investors should not require such outsize returns, and the historical evidence supports this view. This is a topic meriting careful exploration. After all, according to the Ibbotson data, investors earned 8 percent real returns over the past 75 years, and stocks have outpaced bonds by nearly 5 percent over the past 75 years. So, why shouldn t investors have expected these returns in the past and why shouldn t they continue to do so? Expressed in a slightly different way, we examine two questions.

First, can we derive an objective estimate of what investors should have had good reasons to have expected in the past? And, why should we expect less in the future than we ve earned in the past?

The answers to both questions lie in the difference between the observed excess return and the prospective risk premium, two fundamentally different concepts that unfortunately carry the same label, risk premium. If we distinguish between past excess returns and future expected risk premiums, it is not at all unreasonable that the future risk premiums should be different from past excess returns. This is a complex topic, requiring several careful steps to evaluate correctly. To gauge the risk premium for stocks relative to bonds, we need an expected real stock return and an expected real bond return.

To gauge the expected real bond return, we need both bond yields and an estimate of expected inflation through history. To gauge the expected real stock return, we need both stock dividend yields and an estimate of expected real dividend growth. Accordingly, we go through each of these steps, in reverse order, to form the building blocks for the final goal: an estimate of the objective, forward-looking equity risk premium, relative to bonds, through history.},
  groups       = {RiskPremia_FixedIncome, FrcstQWIM_MedLngTerm, FcstQWIM_Bond, FcstQWIM_Equity},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=296854},
  organization = {SSRN Electronic Journal},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 22:17},
}

@Article{Azoulay-et-al-2014,
  author               = {Azoulay, Eddy and Brenner, Menachem and Landskroner, Yoram and Stein, Roy},
  date                 = {2014-01},
  journaltitle         = {Journal of Economics and Business},
  title                = {Inflation risk premium implied by options},
  doi                  = {10.1016/j.jeconbus.2013.06.001},
  issn                 = {0148-6195},
  pages                = {90--102},
  volume               = {71},
  abstract             = {We estimate the inflation risk premium using options prices. We use volatility implied from foreign exchange (FX) option prices combined with a price of risk extracted from stock prices. Purchasing power parity theory provides the linkage between inflation and the foreign exchange rate. Using Israeli data we find a statistically and economically significant inflation risk premium.

One of the commonly used estimates of expected inflation is the yield differential between nominal bonds and inflation-indexed bonds (breakeven inflation). Breakeven inflation is however a biased estimate of expected inflation because it includes an inflation risk premium (IRP). The novelty of our approach is that we estimate the IRP using the volatility implied from foreign exchange (FX) option prices combined with a price of risk extracted from stock prices. Purchasing Power Parity theory provides the linkage between inflation and the foreign exchange rate.

Using data from the Israeli government bond market, which has a long history of liquid markets in inflation-linked and nominal bonds as well as an active FX options market, we find a statistically and economically significant positive inflation risk premium.},
  citeulike-article-id = {13940609},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jeconbus.2013.06.001},
  groups               = {RiskPremia_FixedIncome, RiskPremia_Other, Risk_Inflation},
  owner                = {zkgst0c},
  posted-at            = {2016-02-25 21:52:45},
  timestamp            = {2020-02-25 22:17},
}

@Article{Bauer-Hamilton-2017,
  author               = {Bauer, Michael D. and Hamilton, James D.},
  date                 = {2017-09-22},
  journaltitle         = {The Review of Financial Studies},
  title                = {Robust Bond Risk Premia},
  doi                  = {10.1093/rfs/hhx096},
  issn                 = {0893-9454},
  abstract             = {A consensus has recently emerged that variables beyond the level, slope, and curvature of the yield curve can help predict bond returns. This paper shows that the statistical tests underlying this evidence are subject to serious small-sample distortions. We propose more robust tests, including a novel bootstrap procedure specifically designed to test the spanning hypothesis. We revisit the analysis in six published studies and find that the evidence against the spanning hypothesis is much weaker than it originally appeared. Our results pose a serious challenge to the prevailing consensus.},
  citeulike-article-id = {14460444},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rfs/hhx096},
  day                  = {22},
  groups               = {RiskPremia_FixedIncome},
  posted-at            = {2017-10-18 17:56:36},
  timestamp            = {2020-02-25 22:17},
}

@Article{Berardi-Plazzi-2018,
  author         = {Berardi, Andrea and Plazzi, Alberto},
  date           = {2018-02-14},
  journaltitle   = {Journal of Financial Econometrics},
  title          = {Inflation risk premia, yield volatility, and macro factors},
  doi            = {10.1093/jJFInec/nby004},
  issn           = {1479-8409},
  abstract       = {We incorporate a latent stochastic volatility factor and macroeconomic expectations in an affine model for the term structure of nominal and real rates. We estimate the model over 1999-2016 on U.S. data for nominal and TIPS yields, the realized and implied volatility of T-bonds, and survey forecasts of GDP growth and inflation. We find relatively stable inflation risk premia averaging at 40 basis points at the long-end, and which are strongly related to the volatility factor and conditional mean of output growth. We also document real risk premia that turn negative in the post-crisis period, and a non-negligible variance risk premium.},
  day            = {14},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_FixedIncome, RiskPremia_Other, RiskPremia_Forecast, FrcstQWIM_MedLngTerm, FcstQWIM_Bond, Risk_Inflation, FrcstQWIM_Inflation},
  timestamp      = {2020-02-25 22:17},
}

@Article{Bianchi-et-al-2019a,
  author         = {Bianchi, Daniele and Buchner, Matthias and Tamoni, Andrea},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Bond Risk Premia with Machine Learning},
  url            = {https://ssrn.com/abstract=3232721},
  abstract       = {We propose and evaluate a machine learning framework to predict bond excess returns. We benchmark our framework against linear forecasting regressions with principal components extracted from past yields, macroeconomic variables, or both. Our results show that adopting a machine learning framework can improve out-of-sample forecasting accuracy along the full spectrum of bond maturities. We also show that the mapping between bond risk premia and past yields is non-linear and can be accurately approximated by using a deep neural network. On the contrary, non-linearity and the depth of the network seem to be of secondary importance when forecasting future bond excess returns based on macroeconomic information. Finally, we show that machine learning techniques reinforce the evidence that the information embedded in macroeconomic variables is not subsumed by the yield curve.},
  day            = {16},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_FixedIncome, RiskPremia_Forecast, ML_Forecast_QWIM, FrcstQWIM_ML, Data_NonLinear, ML_Test_OOS, FrcstQWIM_MedLngTerm, FcstQWIM_Bond, FrcstQWIM_Test, DeepLearning_QWIM},
  timestamp      = {2020-02-25 22:17},
}

@Article{Choi-et-al-2012,
  author               = {Choi, Hoyong and Mueller, Philippe and Vedolin, Andrea},
  date                 = {2012-01},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Bond Variance Risk Premiums},
  url                  = {https://ssrn.com/abstract=1787478},
  abstract             = {This paper studies variance risk premiums in the Treasury market. We first develop a theory to price variance swaps and show that the realized variance can be perfectly replicated by a static position in Treasury futures options and a dynamic position in the underlying. Pricing and hedging is robust even if the underlying jumps. Using a large options panel data set on Treasury futures with different tenors, we report the following findings: First, the term-structure of implied variances is downward sloping across maturities and increases in tenors. Moreover, the slope of the term structure is strongly linked to economic activity. Second, returns to the Treasury variance swap are negative and economically large. Shorting a variance swap produces an annualized Sharpe ratio of almost two and the associated returns cannot be explained by standard risk factors. Moreover, the returns remain highly statistically significant even when accounting for transaction costs and margin requirements.},
  citeulike-article-id = {14025447},
  citeulike-linkout-0  = {http://ssrn.com/abstract=1787478},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2754347code619506.pdf?abstractid=1787478 and mirid=1},
  day                  = {2},
  groups               = {RiskPremia_FixedIncome},
  owner                = {cristi},
  posted-at            = {2016-05-01 13:48:55},
  timestamp            = {2020-02-25 22:17},
}

@Article{Cochrane-Piazzesi-2005,
  author         = {Cochrane, John H and Piazzesi, Monika},
  date           = {2005-02},
  journaltitle   = {American Economic Review},
  title          = {Bond Risk Premia},
  doi            = {10.1257/0002828053828581},
  issn           = {0002-8282},
  number         = {1},
  pages          = {138--160},
  volume         = {95},
  abstract       = {We study time variation in expected excess bond returns. We run regressions of one-year excess returns on initial forward rates. We find that a single factor, a single tent-shaped linear combination of forward rates, predicts excess returns on one- to five-year maturity bonds with R2 up to 0.44. The return-forecasting factor is countercyclical and forecasts stock returns. An important component of the return-forecasting factor is unrelated to the level, slope, and curvature movements described by most term structure models. We document that measurement errors do not affect our central results.},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_FixedIncome, RiskPremia_Forecast, FcstQWIM_Bond},
  timestamp      = {2020-02-25 22:17},
}

@Article{Asvanunt-Richardson-2017,
  author               = {Asvanunt, Attakrit and Richardson, Scott},
  date                 = {2017-01},
  journaltitle         = {The Journal of Fixed Income},
  title                = {The Credit Risk Premium},
  doi                  = {10.3905/jfi.2017.26.3.006},
  issn                 = {1059-8596},
  number               = {3},
  pages                = {6--24},
  volume               = {26},
  abstract             = {Despite theoretical and intuitive reasons for a credit risk premium, past research has found little supporting empirical evidence. This is primarily attributable to biases in computing credit excess returns, which improperly account for term risk. Using data spanning 80 years in the United States and nearly 20 years in Europe, the authors find strong evidence of a credit risk premium after correctly adjusting for term risk. The credit risk premium is not spanned by other known risk premia, and it exhibits time variation related to economic growth and aggregate default rates. These results have important implications for asset pricing and investment decisions.},
  citeulike-article-id = {14320190},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jfi.2017.26.3.006},
  groups               = {RiskPremia_Other},
  posted-at            = {2017-03-26 14:27:03},
  timestamp            = {2020-02-25 22:17},
}

@Article{Bai-Zjou-2015,
  author               = {Bai, Jushan and Zhou, Guofu},
  date                 = {2015-04},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Fama-MacBeth Two-Pass Regressions: Improving Risk Premia Estimates},
  url                  = {https://ssrn.com/abstract=2591754},
  abstract             = {In this paper, we provide the asymptotic theory for the widely used Fama and MacBeth (1973) two-pass regression in the usual case of a large number of assets. We find that the convergence of the OLS two-pass estimator depends critically on the time series sample size in addition to the number of cross-sections. To accommodate typical relatively small time series length, we propose new OLS and GLS estimators that improve the small sample performances significantly.},
  citeulike-article-id = {13997322},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2591754},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2649134code16976.pdf?abstractid=2591754 and mirid=1},
  day                  = {10},
  groups               = {RiskPremia_Other},
  owner                = {cristi},
  posted-at            = {2016-04-05 03:59:11},
  timestamp            = {2020-02-25 22:17},
}

@Article{Barnea-Hogan-2012,
  author               = {Barnea, Amir and Hogan, Reed},
  date                 = {2012-04},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Quantifying the Variance Risk Premium in VIX Options},
  doi                  = {10.3905/jpm.2012.38.3.143},
  issn                 = {0095-4918},
  number               = {3},
  pages                = {143--148},
  volume               = {38},
  abstract             = {Barnea and Hogan use synthetically created variance swaps on VIX futures to quantify the variance risk premium in VIX options. The results of this methodology suggest that the average premium is '3.26meaning that the realized variance on VIX futures is, on average, less than the variance implied by the swap rate. This premium does not vary with time or the level of the swap rate as much as premiums in other asset classes. A negative risk premium implies that VIX option strategies that are net credit should be profitable.},
  citeulike-article-id = {13971907},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2012.38.3.143},
  groups               = {RiskPremia_Other},
  owner                = {cristi},
  posted-at            = {2016-03-08 06:05:42},
  timestamp            = {2020-02-25 22:17},
}

@Article{Bender-et-al-2015a,
  author               = {Bender, Jennifer and Briand, Remy and Nielsen, Frank and Stefek, Dan},
  date                 = {2015-01},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Portfolio of Risk Premia: A New Approach to Diversification},
  url                  = {https://ssrn.com/abstract=2543991},
  abstract             = {The traditional asset allocation to equities and bonds is characterized by high volatility and lacks sufficient diversification, particularly during periods of distress. The meltdown of 2001-2002, in which markets around the world tumbled together, amply demonstrated this fact. As a consequence, there has been a gradual shift in strategic allocations towards alternative asset classes, such as private equity, hedge funds and commodities. Unfortunately, these new allocations only partially provide the needed diversification. Several researchers, including Asness, Krail, and Liew [2001] and Anson [2007], have shown that even alternative asset classes are exposed to traditional equities and bonds.In an ideal world, a portfolio would be composed of a wide range of return-producing units, each of which is risky but independent of the others. Such a portfolio would result in high returns with low volatility. These return-producing units would also have capacity large enough for allocations by large funds. So, where do we find such independent, return-producing units? One simple answer is that many of these return-producing elements or risk premia already exist in traditional asset class portfolios. However, they are accompanied and dominated by broad equity or bond returns. We only need to separate them.The objective of this paper is to explore risk premia as basic units in investment management. First, we define and classify risk premia. We then identify a number of premia across different asset classes and discuss how an investor may capture them. Next, we study their risk and return characteristics, both as standalone entities and in a portfolio context. Lastly, we illustrate the potential benefits of building a portfolio of risk premia for asset allocation.},
  citeulike-article-id = {13512812},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2543991},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2543991code569387.pdf?abstractid=2543991 and mirid=1},
  day                  = {1},
  groups               = {RiskPremia_Other, Risk_Capacity, Invest_Diversif},
  owner                = {zkgst0c},
  posted-at            = {2016-08-31 15:34:07},
  timestamp            = {2020-02-25 22:17},
}

@Article{Boon-Ielpo-2017,
  author               = {Boon, Ling-Ni and Ielpo, Florian},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Assessing the Structural Fundamentals of Realized Risk Premiums},
  url                  = {https://ssrn.com/abstract=2970397},
  abstract             = {In investment-decision-making, it is customary to define economic regimes of high or low realized risk premiums with economic indicators such as inflation and growth. We generalize this discrete categorization to a continuous one by estimating a linear relation between economic fundamentals and realized standard and alternative risk premiums. Using a bootstrap method, we estimate that changes in macroeconomic indicators explain about 50\% of the variation in realized risk premiums. We apply our estimates to comment on the implication of a prolonged low growth scenario on the prospective level of risk premiums.},
  citeulike-article-id = {14514243},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2970397},
  groups               = {RiskPremia_Other, RiskPremia_Regime},
  posted-at            = {2018-01-09 23:04:57},
  timestamp            = {2020-02-25 22:17},
}

@Article{Chaves-2017,
  author               = {Chaves, Denis B.},
  date                 = {2017-03},
  journaltitle         = {The Journal of Alternative Investments},
  title                = {Time-Varying Risk Premiums and Term Premiums in Commodity Futures},
  doi                  = {10.3905/jai.2017.19.4.039},
  issn                 = {1520-3255},
  number               = {4},
  pages                = {39--52},
  volume               = {19},
  abstract             = {Term premiums, defined as the excess return of long-dated contracts over short-dated contracts, in commodity futures are strongly predictable, both in the time series and in the cross section, by roll yield spreads. Strategies that exploit this predictability show sizable Sharpe ratios and are uncorrelated with strategies that exploit predictability in risk premiums using the basis in futures prices, that is, use contango and backwardation conditions in futures market to develop their strategies.},
  citeulike-article-id = {14320193},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jai.2017.19.4.039},
  groups               = {Predictability_Return_Other, RiskPremia_Other, Invest_Cmdty},
  posted-at            = {2017-03-26 14:32:44},
  timestamp            = {2020-02-25 22:17},
}

@Article{Chen-et-al-2010,
  author               = {Chen, Ren-Raw and Liu, Bo and Cheng, Xiaolin},
  date                 = {2010-09},
  journaltitle         = {Journal of Empirical Finance},
  title                = {Pricing the term structure of inflation risk premia: Theory and evidence from TIPS},
  doi                  = {10.1016/j.jempfin.2010.01.002},
  issn                 = {0927-5398},
  number               = {4},
  pages                = {702--721},
  volume               = {17},
  abstract             = {In this paper, we study inflation risk and the term structure of inflation risk premia in the United States' nominal interest rates through the Treasury Inflation Protection Securities (TIPS) with a multi-factor, modified quadratic term structure model with correlated real and inflation rates. We derive closed form solutions to the real and nominal term structures of interest rates that drastically facilitate the estimation of model parameters and improve the accuracy of the valuation of nominal rates and TIPS prices. In addition, we contribute to the literature by estimating the term structure of inflation risk premia implied from the TIPS market. The empirical evidence using data from the period of January 1998 through October 2007 indicates that the expected inflation rate, contrary to data derived from the consumer price indices, is very stable and the inflation risk premia exhibit a positive term structure.},
  citeulike-article-id = {6629807},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jempfin.2010.01.002},
  day                  = {28},
  groups               = {RiskPremia_Other},
  owner                = {cristi},
  posted-at            = {2016-06-18 15:52:04},
  timestamp            = {2020-02-25 22:17},
}

@Article{Andreou-et-al-2017,
  author               = {Andreou, Panayiotis C. and Kagkadis, Anastasios and Philip, Dennis and Taamouti, Abderrahim},
  date                 = {2017-04},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Forward Moments and Risk Premia Predictability},
  url                  = {https://ssrn.com/abstract=2927454},
  abstract             = {We rely on the recently established aggregation property of the second and third moments of returns to estimate forward variances as well as forward skewnesses from option prices. Motivated by the theoretical implications of affine no-arbitrage models, we construct from each of the term structures of forward variances and skewnesses a single predictive factor for the equity and the variance risk premium. The factors are constructed using partial least squares and hence encapsulate only those components driving the forward moments that are also relevant for forecasting purposes. Empirically, we show that both factors exhibit significant in-sample and out-of-sample predictability for the equity and the variance premium. More importantly, the forward skewness factor exhibits equity premium predictability that is stronger than and incremental to that offered by the forward variance factor. This is not true in the case of the variance premium predictability.},
  citeulike-article-id = {14402022},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2927454},
  groups               = {Predictability_Return_Other, RiskPremia_Forecast, Predictability_FinInfo, FcstQWIM_Equity},
  posted-at            = {2017-07-29 19:01:37},
  timestamp            = {2020-02-25 22:18},
}

@Article{Arnott-Ryan-2001,
  author               = {Arnott, Robert D. and Ryan, Ronald J.},
  date                 = {2001-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Death of the Risk Premium},
  doi                  = {10.3905/jpm.2001.319802},
  issn                 = {0095-4918},
  number               = {3},
  pages                = {61--74},
  volume               = {27},
  abstract             = {The authors contend that most of the institutional investing community is expecting far higher returns than are realistic from current market levels. Extrapolating the past is the easiest, and worst, way to forecast the future. Unfortunately, most investors' return expectations are shaped by a simple extrapolation of either recent or long-term past returns. If, instead, the constituent parts of equity market returns are examined, we find that it is remarkably difficult to make a case for a positive equity risk premium (the premium of future stock market returns relative to bond yields) from current market levels. None of this analysis is contingent on any assumption that market P/E ratios or dividend yields should return to historical levels. If market levels are fair and are fully sustained in the years ahead, there is still little or no room for a positive equity risk premium. If there is not a positive risk premium, then actuarial return assumptions are likely to be too optimistic, with far-reaching implications for pension funding ratios, corporate earnings, future pension contributions, and appropriate asset allocation policy.},
  citeulike-article-id = {14337433},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2001.319802},
  groups               = {RiskPremia_Forecast, FrcstQWIM_MedLngTerm, FcstQWIM_Bond, FcstQWIM_Equity},
  posted-at            = {2017-04-14 20:39:56},
  timestamp            = {2020-02-25 22:18},
}

@Article{Chen-et-al-2015c,
  author               = {Chen, Jian and Shen, Liya and Wang, Xiaoke and Zuo, Haomiao},
  date                 = {2015-11},
  journaltitle         = {Applied Economics Letters},
  title                = {The role of variance risk premium in predicting excess stock market return: out-of-sample evidences},
  doi                  = {10.1080/13504851.2015.1034831},
  number               = {17},
  pages                = {1382--1388},
  volume               = {22},
  abstract             = {This paper examines the out-of-sample performance of variance risk premium in predicting excess stock market returns across nine international markets. We assess the out-of-sample predictability through statistical and economic significance tests and find that the variance risk premium has strong forecasting power at the 4-month horizon for most of the international markets considered in this study. In addition, we find the predictability is even stronger during the recent financial crisis period.},
  citeulike-article-id = {13989298},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/13504851.2015.1034831},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/13504851.2015.1034831},
  day                  = {22},
  groups               = {RiskPremia_Forecast, FcstQWIM_Equity},
  owner                = {cristi},
  posted-at            = {2016-03-28 03:29:26},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 22:18},
}

@Article{Baltas-Scherer-2019,
  author         = {Baltas, Nick and Scherer, Bernd},
  date           = {2019},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Tail Risk in the Cross Section of Alternative Risk Premium Strategies},
  doi            = {10.3905/jpm.2018.45.2.093},
  url            = {https://jpm.pm-research.com/content/45/2/93},
  abstract       = {In this article, the authors attempt to get a better understanding of the cross-section of alternative risk premiums using a multi-asset version of the downside risk capital asset pricing model (CAPM). In line with the empirical literature, they find that the cross-section of realized returns is much better explained when using the downside CAPM, rather than relying on the traditional CAPM. However, in contrast to the empirical literature, the authors cannot always recover the required signs in their cross-sectional regressions. In particular, they find that taking on downside risk is not always systematically rewarded. This might be due to the limited availability of time series that essentially overweight the exceptional events of 2008 or a direct result of creating backtests with attractive in-sample features that are impossible to repeat out-of-sample.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Risk, Invest_TailRisk, RiskPremia_Alt},
  timestamp      = {2020-02-25 22:18},
}

@Article{Bird-et-al-2013a,
  author               = {Bird, Ron and Liem, Harry and Thorp, Susan},
  date                 = {2013-04},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Tortoise and the Hare: Risk Premium versus Alternative Asset Portfolios},
  doi                  = {10.3905/jpm.2013.39.3.112},
  issn                 = {0095-4918},
  number               = {3},
  pages                = {112--122},
  volume               = {39},
  abstract             = {Does diversification using a basket of the most common alternative investments outperform diversification using low-cost, liquid risk premia? Investment banks have recently begun offering access to such risk premia at low cost. First, the authors confirm that alternative assets may reduce portfolio risk, based on historical experience. Second, they compare the risk-reduction benefits of alternative investments and risk premium portfolios out of sample, using equally weighted and least-risk optimized portfolios. They find that risk premia diversify more efficiently than do alternative asset portfolios. The authors suggest that an optimal portfolio combines the benefits of both risk premium and alternative asset portfolios, as some alternative assets (such as timber or managed futures) continue to provide exposure to unique sources of return},
  citeulike-article-id = {14398708},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2013.39.3.112},
  groups               = {RiskPremia_Alt},
  posted-at            = {2017-07-24 05:23:38},
  timestamp            = {2020-02-25 22:18},
}

@InCollection{Blin-et-al-2017,
  author               = {Blin, Olivier and Ielpo, Florian and Lee, Joan and Teiletche, Jerome},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {A Macro Risk-Based Approach to Alternative Risk Premia Allocation},
  doi                  = {10.1016/b978-1-78548-201-4.50012-x},
  isbn                 = {9781785482014},
  pages                = {285--316},
  publisher            = {Elsevier},
  abstract             = {Alternative risk premia are encountering growing interest from investors. The vast majority of academic literature has been focusing on describing the alternative risk premia (typically, momentum, carry and value strategies) individually. In this chapter, we investigate the question of the allocation across a range of cross-asset alternative risk premia. For this, we design an active macro risk-based framework that notably aims to exploit alternative risk premia's varying behavior in different macro regimes. We build long-term strategic portfolios across economic regimes, which we dynamically tilt based on point-in-time signals related to regimes nowcasting and current carry. We perform back tests of the allocation strategy in an out-of-sample setting.},
  citeulike-article-id = {14499084},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50012-x},
  groups               = {RiskPremia_Alt, RiskPremia_Regime},
  posted-at            = {2017-12-08 00:38:50},
  timestamp            = {2020-02-25 22:18},
}

@TechReport{Blin-et-al-2017a,
  author               = {Blin, Olivier and Lee, Joan and Teiletche, Jerome},
  date                 = {2017},
  institution          = {Unigestion},
  title                = {Alternative risk premia investing: from theory to practice},
  url                  = {https://www.unigestion.com/publication/alternative-risk-premia-investing-from-theory-to-practice/2},
  abstract             = {Alternative risk premia investing has grown rapidly in popularity in recent years. But what exactly does it involve, and what should investors look for when considering which alternative risk premia strategies to invest in?

In this paper we look at the theory behind alternative risk premia before discussing some of the practical considerations that should help investors get the most out of their allocation to these innovative investment strategies.},
  citeulike-article-id = {14356750},
  groups               = {RiskPremia_Alt},
  posted-at            = {2017-05-14 03:38:40},
  timestamp            = {2020-02-25 22:18},
}

@Article{Blin-et-al-2018,
  author         = {Blin, Olivier and Ielpo, Florian and Lee, Joan and Teiletche, Jerome},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Factor timing revisited: alternative risk premia allocation based on nowcasting and valuation signals},
  doi            = {10.2139/ssrn.3247010},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3247010},
  abstract       = {Alternative risk premia are encountering growing interest from investors. The vast majority of the academic literature has been focusing on describing the alternative risk premia (typically, momentum, carry and value strategies) individually. In this article, we investigate the question of allocation across a diversified range of cross-asset alternative risk premia over the period 1990-2018. For this, we design an active (macro risk-based) allocation framework that notably aims to exploit alternative risk premia varying behavior in different macro regimes and their valuations over time. We perform backtests of the allocation strategy in an out-of-sample setting, shedding light on the significance of both sources of information.},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_Alt, RiskPremia_Regime},
  timestamp      = {2020-02-25 22:18},
}

@Article{Bruder-et-al-2016a,
  author               = {Bruder, Benjamin and Kostyuchyk, Nazar and Roncalli, Thierry},
  date                 = {2016-09},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Risk Parity Portfolios with Skewness Risk: An Application to Factor Investing and Alternative Risk Premia},
  url                  = {https://ssrn.com/abstract=2813384},
  abstract             = {This article develops a model that takes into account skewness risk in risk parity portfolios. In this framework, asset returns are viewed as stochastic processes with jumps or random variables generated by a Gaussian mixture distribution. This dual representation allows us to show that skewness and jump risks are equivalent. As the mixture representation is simple, we obtain analytical formulas for computing asset risk contributions of a given portfolio. Therefore, we define risk budgeting portfolios and derive existence and uniqueness conditions. We then apply our model to the equity/bond/volatility asset mix policy. When assets exhibit jump risks like the short volatility strategy, we show that skewness-based risk parity portfolios produce better allocation than volatility-based risk parity portfolios. Finally, we illustrate how this model is suitable to manage the skewness risk of long-only equity factor portfolios and to allocate between alternative risk premia.},
  citeulike-article-id = {14146618},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2813384},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2813384code903940.pdf?abstractid=2813384 and mirid=1},
  day                  = {23},
  groups               = {Factor based investing, Risk_Budgeting, Invest_Risk, RiskPremia_Alt},
  owner                = {cristi},
  posted-at            = {2016-09-26 14:53:36},
  timestamp            = {2020-02-25 22:18},
}

@TechReport{Unigestion-2017b,
  author               = {Blin, Olivier and Lee, Joan and Teiletche, Jerome},
  date                 = {2017},
  institution          = {Unigestion},
  title                = {Alternative risk premia strategies: why the results are so different?},
  url                  = {https://www.unigestion.com/insight/alternative-risk-premia-strategies-why-the-results-are-so-different/},
  abstract             = {The popularity of alternative risk premia (ARP) is growing and strategies that focus on ARP are increasingly being incorporated into institutional portfolios. The attraction of this approach is that ARP can mimic strategies that have historically only been available through hedge fund vehicles, but with more favourable liquidity and cost characteristics. However, as investors familiarise themselves with this newer universe of investment strategies, the disparity of results that is being delivered by these solutions in practice is raising some ques tions. This paper aims to highlight some of these variations, provide an explanation for them and to also provide some suggestions for investors considering this exciting new investment approach. In our view, there are three elements an investor should consider:

It is important to scrutinise a manager's definition of individual risk premia; despite being broadly categorised under the same name, we have found that risk premia can have significantly different performances across investment solutions.

The methodology that managers use to allocate between risk premia should be closely scrutinised; our observations have shown that even portfolios managed using "equal risk contributions" can be defined in a number of ways, which can have implications on the end re sult.

The benefits of diversifying across several alternative risk premia managers should be considered, as the resulting low correlation could help investors obtain improved risk- adjusted performance without significantly increasing their portfolio's bet a to traditional equity markets.},
  citeulike-article-id = {14510379},
  groups               = {RiskPremia_Alt},
  posted-at            = {2017-12-30 13:43:48},
  timestamp            = {2020-02-25 22:18},
}

@InCollection{Angelidis-Tessaromatis-2014a,
  author      = {Angelidis, Timotheos and Tessaromatis, Nikolaos},
  booktitle   = {Proceedings of Economics and Finance Conferences},
  date        = {2014},
  title       = {Global portfolio management under state dependent multiple risk premia},
  number      = {0400966},
  url         = {https://ideas.repec.org/p/sek/iefpro/0400966.html},
  abstract    = {In this paper, we assess the benefits from international factor diversification under a regime based portfolio construction framework that takes into account the dynamic changes in stock markets. We show that there are significant costs to investors who fail to (a) pursue an international diversification strategy using sources of return other than the market premium and (b) take into account the existence of regimes in portfolio construction and asset allocation. Short sale and tracking error constraints reduce but do not eliminate the gains from a dynamic global factor portfolio. Implementation through commercially available, investable factor indices to provide efficient and low cost building blocks to construct a dynamic diversified factor portfolio in practice preserves most of the benefits from state dependent portfolio construction.},
  groups      = {RiskPremia_Regime, Market_States},
  institution = {International Institute of Social and Economic Sciences},
  keywords    = {Diversification benefits; Factor returns; Regime Switching Models.},
  timestamp   = {2020-02-25 22:18},
  type        = {Proceedings of Economics and Finance Conferences},
}

@Article{Damodaran-2016,
  author               = {Damodaran, Aswath},
  date                 = {2016-03},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Equity Risk Premiums (ERP): Determinants, Estimation and Implications The 2016 Edition},
  url                  = {https://ssrn.com/abstract=2742186},
  abstract             = {Equity risk premiums are a central component of every risk and return model in finance and are a key input in estimating costs of equity and capital in both corporate finance and valuation. Given their importance, it is surprising how haphazard the estimation of equity risk premiums remains in practice. We begin this paper by looking at the economic determinants of equity risk premiums, including investor risk aversion, information uncertainty and perceptions of macroeconomic risk. In the standard approach to estimating the equity risk premium, historical returns are used, with the difference in annual returns on stocks versus bonds over a long time period comprising the expected risk premium. We note the limitations of this approach, even in markets like the United States, which have long periods of historical data available, and its complete failure in emerging markets, where the historical data tends to be limited and volatile. We look at two other approaches to estimating equity risk premiums the survey approach, where investors and managers are asked to assess the risk premium and the implied approach, where a forward-looking estimate of the premium is estimated using either current equity prices or risk premiums in non-equity markets. In the next section, we look at the relationship between the equity risk premium and risk premiums in the bond market (default spreads) and in real estate (cap rates) and how that relationship can be mined to generated expected equity risk premiums. We close the paper by examining why different approaches yield different values for the equity risk premium, and how to choose the - right- number to use in analysis.},
  citeulike-article-id = {14034232},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2742186},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2742186code20838.pdf?abstractid=2742186 and mirid=1},
  day                  = {5},
  groups               = {RiskPremia_Equity},
  owner                = {cristi},
  posted-at            = {2016-05-12 20:47:15},
  timestamp            = {2020-02-25 22:22},
}

@Article{Derrig-Orr-2004,
  author       = {Derrig, Richard A. and Orr, Elisha D.},
  date         = {2004},
  journaltitle = {North American Actuarial Journal},
  title        = {Equity risk premium: expectations great and small},
  number       = {1},
  pages        = {45--126},
  volume       = {8},
  abstract     = {The equity risk premium (ERP) is an essential building block of the market value of risk. In theory, the collective action of all investors results in an equilibrium expectation for the return on the market portfolio excess of the risk-free return, the ERP. The ability of the valuation actuary to choose a sensible value for the ERP, whether as a required input to capital asset pricing model valuation, or any of its descendants, is as important as choosing risk-free rates and risk relatives (betas) to the ERP for the asset at hand. The historical realized ERP for the stock market appears to be at odds with pricing theory parameters for risk aversion. Since 1985, there has been a constant stream of research, each of which reviews theories of estimating market returns, examines historical data periods, or both. Those ERP value estimates vary widely from about 1 percent to about 9 based on a geometric or arithmetic averaging, short or long horizons, short- or long-run expectations, unconditional or conditional distributions, domestic or international data, data periods, and real or nominal returns. This paper examines the principal strains of the recent research on the ERP and catalogues the empirical values of the ERP implied by that research. In addition, the paper supplies several time series analyses of the standard Ibbotson Associates 1926-2002 ERP data using short Treasuries for the risk-free rate. Recommendations for ERP values to use in common actuarial valuation problems also are offered.},
  groups       = {RiskPremia_Equity},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 22:22},
  web_url      = {http://www.casact.org/dare/index.cfm?abstrID=5706 and fuseaction=view},
}

@Article{Gagliardini-et-al-2016a,
  author               = {Gagliardini, Patrick and Ossola, Elisa and Scaillet, Olivier},
  date                 = {2016},
  journaltitle         = {Econometrica},
  title                = {Time-Varying Risk Premium in Large Cross-Sectional Equity Data Sets},
  doi                  = {10.3982/ecta11069},
  issn                 = {0012-9682},
  number               = {3},
  pages                = {985--1046},
  volume               = {84},
  abstract             = {We develop an econometric methodology to infer the path of risk premia from a large unbalanced panel of individual stock returns. We estimate the time-varying risk premia implied by conditional linear asset pricing models where the conditioning includes both instruments common to all assets and asset-specific instruments. The estimator uses simple weighted two-pass cross-sectional regressions, and we show its consistency and asymptotic normality under increasing cross-sectional and time series dimensions. We address consistent estimation of the asymptotic variance by hard thresholding, and testing for asset pricing restrictions induced by the no-arbitrage assumption. We derive the restrictions given by a continuum of assets in a multi-period economy under an approximate factor structure robust to asset repackaging. The empirical analysis on returns for about ten thousand U.S. stocks from July 1964 to December 2009 shows that risk premia are large and volatile in crisis periods. They exhibit large positive and negative strays from time-invariant estimates, follow the macroeconomic cycles, and do not match risk premia estimates on standard sets of portfolios. The asset pricing restrictions are rejected for a conditional four-factor model capturing market, size, value, and momentum effects.},
  citeulike-article-id = {14387168},
  citeulike-linkout-0  = {http://dx.doi.org/10.3982/ecta11069},
  groups               = {RiskPremia_Equity},
  posted-at            = {2017-07-03 15:38:01},
  timestamp            = {2020-02-25 22:22},
}

@Article{Gomes-Ribeiro-2018,
  author         = {Gomes, Leandro and Ribeiro, Ruy},
  date           = {2018-03-19},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Term Structure(s) of the Equity Risk Premium},
  url            = {https://ssrn.com/abstract=3144028},
  abstract       = {By simultaneously using dividend and variance swap data, we show how the term structure of the equity risk premium varies over time and how its shape is affected by liquidity risk premia. The term structure is always positively sloped, while funding liquidity premia and betas explain the high unconditional returns for all dividend claims. Alphas for short-dated dividend claims become negative, whereas alphas for long-dated claims seem to be positive. The term structure slope varies positively with the market risk premium, but it is never negative relative to the first contract - due to the nearly zero risk premium in the first maturity - and rarely hump-shaped in some empirical models. We demonstrate how the maturity term structure - the risk premium for dividend strips with different maturities - is connected to both the horizon term structure - linked to the variance swap term structure - and various funding liquidity measures. The risk premium is on average increasing with investment horizon, while the risk premium depends primarily on the short-horizon risk premium, implying that short-horizon investors are the marginal ones. All our results hold in the US, the UK, Europe and Japan. All these facts are consistent with, for instance, a long-run risk model with jump risks.},
  day            = {19},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_Equity},
  timestamp      = {2020-02-25 22:22},
}

@Article{Graham-Harvey-2016,
  author               = {Graham, John R. and Harvey, Campbell R.},
  date                 = {2016-08},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The Equity Risk Premium in 2016},
  url                  = {https://ssrn.com/abstract=2816603},
  abstract             = {We analyze the history of the equity risk premium from surveys of U.S. Chief Financial Officers (CFOs) conducted every quarter from June 2000 to June 2016. The risk premium is the expected 10-year SandP 500 return relative to a 10-year U.S. Treasury bond yield. The average risk premium in 2016, 4.02 is slightly higher than the average observed over the past 16 years. We also provide results on the risk premium disagreement among respondents as well as asymmetry or skewness of risk premium estimates. We also link our risk premium results to survey-based measures of the weighted average cost of capital and investment hurdle rates. The hurdle rates are significantly higher than the cost of capital implied by the market risk premium estimates.},
  citeulike-article-id = {14108497},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2816603},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2816603code16198.pdf?abstractid=2816603 and mirid=1},
  day                  = {4},
  groups               = {RiskPremia_Equity, RiskPremia_FixedIncome},
  owner                = {zkgst0c},
  posted-at            = {2016-08-05 14:33:53},
  timestamp            = {2020-02-25 22:22},
}

@Article{Granelli-Veraart-2016,
  author               = {Granelli, Andrea and Veraart, Almut E. D.},
  date                 = {2016-01},
  journaltitle         = {SIAM Journal on Financial Mathematics},
  title                = {Modeling the Variance Risk Premium of Equity Indices: The Role of Dependence and Contagion},
  doi                  = {10.1137/15m1011822},
  issn                 = {1945-497X},
  number               = {1},
  pages                = {382--417},
  volume               = {7},
  abstract             = {The variance risk premium (VRP) refers to the premium demanded for holding assets whose variance is exposed to stochastic shocks. This paper identifies a new modeling framework for equity indices and presents for the first time explicit analytical formulas for their VRP in a multivariate stochastic volatility setting, which includes multivariate non-Gaussian Ornstein--Uhlenbeck processes and Wishart processes. Moreover, we propose to incorporate contagion within the equity index via a multivariate Hawkes process and find that the resulting dynamics of the VRP represent a convincing alternative to the models studied in the literature up to date. We show that our new model can explain the key stylized facts of both equity indices and individual assets and their corresponding VRP, while some popular (multivariate) stochastic volatility models may fail.

Read More: http://epubs.siam.org/doi/abs/10.1137/15M1011822},
  citeulike-article-id = {14333836},
  citeulike-linkout-0  = {http://dx.doi.org/10.1137/15m1011822},
  groups               = {RiskPremia_Equity, Contagion},
  posted-at            = {2017-04-08 03:20:11},
  timestamp            = {2020-02-25 22:22},
}

@InCollection{DeSilva-et-al-2017,
  author               = {{De Silva}, Harindra and McMurran, Gregory M. and Miller, Megan N.},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {Diversification and the Volatility Risk Premium},
  doi                  = {10.1016/b978-1-78548-201-4.50014-3},
  isbn                 = {9781785482014},
  pages                = {365--387},
  publisher            = {Elsevier},
  abstract             = {The volatility risk premium (VRP) found in options has paid off persistently across different assets, different asset classes and over time. A consistent short volatility position using options or volatility swaps has produced attractive risk-adjusted returns because of exposure to VRP. In this chapter, we have extended the study of the VRP to include not only equity indices but also commodities, government bonds and currencies. Using volatility swap returns as a measure of the payoff to the VRP, we see that the returns to a short volatility position are correlated to the volatility of the underlying instrument and to other VRPs in the same asset class. We also find that the returns are relatively uncorrelated to the VRPs of other asset classes and to the traditional equity factors represented by pure factor portfolios (PFPs). Finally, we show that the multiasset class VRP portfolio studied in this chapter has very competitive risk-adjusted returns},
  citeulike-article-id = {14499078},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50014-3},
  groups               = {Diversified_Invest, RiskPremia_FixedIncome, RiskPremia_Other, Invest_Diversif, [nbkcbu3:]},
  posted-at            = {2017-12-08 00:25:33},
  timestamp            = {2020-02-25 22:22},
}

@Article{Feldhutter-et-al-2016,
  author               = {Feldhutter, Peter and Heyerdahl-Larsen, Christian and Illeditsch, Philipp},
  date                 = {2016-10},
  journaltitle         = {Review of Finance},
  title                = {Risk Premia and Volatilities in a Nonlinear Term Structure Model},
  doi                  = {10.1093/rof/rfw052},
  issn                 = {1572-3097},
  pages                = {rfw052+},
  abstract             = {We introduce a reduced-form term structure model with closed-form solutions for yields where the short rate and market prices of risk are nonlinear functions of Gaussian state variables. The nonlinear model with three factors matches the time-variation in expected excess returns and yield volatilities of US Treasury bonds from 1961 to 2014. Yields and their variances depend on only three factors, yet the model exhibits features consistent with Unspanned Risk Premia (URP) and Unspanned Stochastic Volatility (USV).},
  citeulike-article-id = {14333879},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rof/rfw052},
  day                  = {29},
  groups               = {RiskPremia_FixedIncome},
  posted-at            = {2017-04-08 06:07:52},
  timestamp            = {2020-02-25 22:22},
}

@Article{Huang-et-al-2018d,
  author         = {Huang, Dashan and Jiang, Fuwei and Tong, Guoshi},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Real time macro factors in bond risk premium},
  doi            = {10.2139/ssrn.3107612},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3107612},
  abstract       = {The notion that bond risk premium varies with macroeconomic condition is challenged once real time macro data are used. In this paper, we argue that the macro factors extracted by using the conventional PCA are not the most relevant for forecasting bond risk premium, because the PCA factors are designed to explain the most variation of macro data instead of the variation of bond risk premium. With the latter objective in mind, we propose a scaled PCA (sPCA) approach, which incorporates the information in bond risk premium in the factor extraction procedure. The real time sPCA factors have much stronger predictive power than the PCA factors, both in- and out-of-sample, and generate sizeable utility gains. Alterative approaches, target PCA and PLS, obtain similar results. The forecasting power of the sPCA factors appear countercycilical, with expected bond returns high in recessions and low in expansions. The sPCA factors also strongly forecast future macro data revision and macroeconomic conditions, consistent with implications of standard asset pricing theories.},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_FixedIncome, RiskPremia_Forecast, FrcstQWIM_MedLngTerm, FcstQWIM_Bond, FrcstQWIM_Test},
  timestamp      = {2020-02-25 22:22},
}

@Article{Hunt-Hoisington-2003,
  author               = {Hunt, Lacy H. and Hoisington, David M.},
  date                 = {2003-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Estimating the Stock/Bond Risk Premium},
  doi                  = {10.3905/jpm.2003.319870},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {28--34},
  volume               = {29},
  abstract             = {New light shed on the risk premium of stocks over long U.S. Treasury bonds indicates that most research has overstated the advantages of stocks over bonds. Over long periods, bonds have actually outperformed equities. Certain conditions are found to have likely produced these results.},
  citeulike-article-id = {14309994},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2003.319870},
  groups               = {RiskPremia_FixedIncome},
  posted-at            = {2017-03-13 01:14:48},
  timestamp            = {2020-02-25 22:22},
}

@Article{Dahlquist-Penasse-2017,
  author               = {Dahlquist, Magnus and Penasse, Julien},
  date                 = {2017-09},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The Missing Risk Premium in Exchange Rates},
  url                  = {https://ssrn.com/abstract=2884890},
  abstract             = {It is well known that the interest rate differential (the forward premium) predicts currency returns. However, we find that the real exchange rate, not the interest rate differential, is the main predictor of currency returns at longer horizons. We relate this finding to other puzzling features of currency markets, namely that the real exchange rate contemporaneously appreciates with the interest rate differential and that the positive relationship between currency risk premia and the interest rate differential reverses over longer horizons. Models in which the currency risk premium depends on the interest rate differential and a missing risk premium, capturing deviations from the purchasing power parity, can rationalize these observations.},
  citeulike-article-id = {14322603},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2884890},
  groups               = {RiskPremia_Other},
  posted-at            = {2017-03-29 16:00:54},
  timestamp            = {2020-02-25 22:23},
}

@Article{DeJong-Driessen-2012,
  author       = {{De Jong}, F. and Driessen, J.},
  date         = {2012},
  journaltitle = {Quarterly Journal of Finance},
  title        = {Liquidity Risk Premia in Corporate Bond Markets},
  number       = {2},
  pages        = {1--34},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=686681},
  volume       = {2},
  abstract     = {This paper explores the role of liquidity risk in the pricing of corporate bonds. We show that corporate bond returns have significant exposures to fluctuations in treasury bond liquidity and equity market liquidity.

Further, this liquidity risk is a priced factor for the expected returns on corporate bonds, and the associated liquidity risk premia help to explain the credit spread puzzle. In terms of expected returns, the total estimated liquidity risk premium is around 0.6 percent per annum for US long-maturity investment grade bonds. For speculative grade bonds, which have higher exposures to the liquidity factors, the liquidity risk premium is around 1.5 percent per annum.

We find very similar evidence for the liquidity risk exposure of corporate bonds for a sample of European corporate bond prices.},
  groups       = {Characteristics and return prediction, RiskPremia_Other, [nbkcbu3:]},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 22:23},
}

@Article{DellaCorte-et-al-2016,
  author               = {Della Corte, Pasquale and Ramadorai, Tarun and Sarno, Lucio},
  date                 = {2016-04},
  journaltitle         = {Journal of Financial Economics},
  title                = {Volatility risk premia and exchange rate predictability},
  doi                  = {10.1016/j.jfineco.2016.02.015},
  issn                 = {0304-405X},
  number               = {1},
  pages                = {21--40},
  volume               = {120},
  abstract             = {We discover a new currency strategy with highly desirable return and diversification properties, which uses the predictive ability of currency volatility risk premia for currency returns. The volatility risk premium the difference between expected realized volatility and model-free implied volatility reflects the costs of insuring against currency volatility fluctuations. The strategy sells high insurance-cost currencies and buys low insurance-cost currencies. A distinctive feature of the strategy's returns is that they are mainly generated by movements in spot exchange rates instead of interest rate differentials. We explore explanations for the profitability of the strategy, which cannot be understood using traditional risk factors.},
  citeulike-article-id = {14071834},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jfineco.2016.02.015},
  groups               = {Predictability_Return_Other, RiskPremia_Other},
  owner                = {cristi},
  posted-at            = {2016-06-18 18:51:25},
  timestamp            = {2020-02-25 22:23},
}

@Article{Ebner-2016,
  author               = {Ebner, Markus},
  date                 = {2016-01},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Risk and Risk Premia: A Cross-Asset Class Analysis},
  url                  = {https://ssrn.com/abstract=2711624},
  abstract             = {The existence of risk premia has been widely documented in the academic literature over the past decades. Until now they have typically been handled as separate phenomena for specific markets or asset classes and thus examined independently. This study analyses risk premia across a variety of asset classes and risk styles to uncover their common performance characteristics, underlying risk sources and return's sensitivity to economic factors.

Based on a set of 16 risk premia over a 22 year sample period we were able to illustrate that risk premia's expected returns are significantly influenced by their volatility and their sensitivity to funding liquidity and market volatility. Furthermore we show that macroeconomic factors such as industrial production and inflation have a significant effect on the expected returns of the entire set of risk premia. Finally, analysing the link between premia and the global market portfolio shows that premia with unfavourable comoments possess superior expected returns.},
  citeulike-article-id = {13931085},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2711624},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2729758code446620.pdf?abstractid=2711624 and mirid=1},
  day                  = {7},
  groups               = {RiskPremia_Other},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2711624},
  owner                = {cristi},
  posted-at            = {2016-02-11 07:21:29},
  timestamp            = {2020-02-25 22:23},
}

@Article{Grishchenko-Huang-2013,
  author               = {Grishchenko, Olesya V. and Huang, Jing-Zhi},
  date                 = {2013-03},
  journaltitle         = {The Journal of Fixed Income},
  title                = {The Inflation Risk Premium: Evidence from the TIPS Market},
  doi                  = {10.3905/jfi.2013.22.4.005},
  issn                 = {1059-8596},
  number               = {4},
  pages                = {5--30},
  volume               = {22},
  abstract             = {This article estimates inflation risk premia using data on prices of Treasury Inflation-Protected Securities (TIPS) from 2000 to 2008. The estimation approach is arbitrage-free, largely model-free, and easy to implement. It also distinguishes between TIPS yields and real yields by explicitly taking into account the three-month indexation lag of TIPS in the analysis. In addition, we consider three measures of TIPS liquidity, including one new measure based on TIPS prices only.

We estimate the liquidity premium to be around 13 basis points over the full sample but substantially higher in the first subperiod. We find that the inflation risk premium is time-varying and, on average, considerably lower than suggested by various structural models. Depending on the proxy used for expected inflation, the unconditional 10-year inflation risk premium ranges from 9 basis points to 4 basis points over the full sample, and between 1 basis point and 6 basis points over the 2004 2008 subperiod.},
  citeulike-article-id = {13970970},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jfi.2013.22.4.005},
  groups               = {RiskPremia_Other, Risk_Inflation},
  owner                = {cristi},
  posted-at            = {2016-03-06 23:54:02},
  timestamp            = {2020-02-25 22:23},
}

@Article{Hamdan-et-al-2016a,
  author               = {Hamdan, Rayann and Pavlowsky, Fabien and Roncalli, Thierry and Zheng, Ban},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {A Primer on Alternative Risk Premia},
  url                  = {https://ssrn.com/abstract=2766850},
  abstract             = {The concept of alternative risk premia can be viewed as an extension of the factor investing approach. Factor investing is a term that is generally dedicated to long-only equity risk factors. A typical example is the value equity strategy. Alternative risk premia designate non-traditional risk premia other than long exposure to equities and bonds. They may concern equities, rates, credit, currencies or commodities and correspond to long/short portfolios. For instance, the value strategy can be extended to credit, currencies and commodities. This paper provides an overview of the different alternative risk premia to be found in the academic and professional spheres. Using a database of commercial indices, we estimate the generic cumulative returns of 59 alternative risk premia in order to analyze their risk, diversification power and payoff function. From this, it is clear that the term "alternative risk premia"encompasses two different types of risk factor: skewness risk premia and market anomalies. We then reconsider portfolio allocation in light of this framework. Indeed, we show that skewness aggregation is considerably more complex than volatility aggregation, and we illustrate that the volatility risk measure is less appropriate and pertinent when managing a portfolio with these risk premia. The development of alternative risk premia shall also affect the risk/return analysis of non-linear strategies, e.g. hedge fund strategies. In particular, using alternative risk factors instead of traditional risk factors leads to an extension of the alternative beta framework. Therefore, we apply the previously estimated risk premia to a universe of hedge fund indices. To that end, we develop a model selection based on the lasso regression to identify the most pertinent risk premia for each hedge fund strategy. It appears that many traditional risk factors, with the exception of long equity and credit exposure on developed markets, vanish when we include alternative risk premia.},
  citeulike-article-id = {14332540},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2766850},
  groups               = {RiskPremia_Other, RiskPremia_Alt},
  posted-at            = {2017-04-05 21:36:42},
  timestamp            = {2020-02-25 22:23},
}

@Article{Haubrich-et-al-2012,
  author               = {Haubrich, Joseph and Pennacchi, George and Ritchken, Peter},
  date                 = {2012-05},
  journaltitle         = {Review of Financial Studies},
  title                = {Inflation Expectations, Real Rates, and Risk Premia: Evidence from Inflation Swaps},
  doi                  = {10.1093/rfs/hhs003},
  issn                 = {1465-7368},
  number               = {5},
  pages                = {1588--1629},
  volume               = {25},
  abstract             = {We develop a model of nominal and real bond yield curves that has four stochastic drivers but seven factors: three factors primarily determine the cross-section of yields, whereas four volatility factors solely determine risk premia. The model is estimated using nominal Treasury yields, survey inflation forecasts, and inflation swap rates and has attractive empirical properties. Time-varying volatility is particularly apparent in short-term real rates and expected inflation. Also, we detail the different economic forces that drive short- and long-term real and inflation risk premia and provide evidence that Treasury inflation-protected securities were undervalued prior to 2004 and during the recent financial crisis.},
  citeulike-article-id = {14071745},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rfs/hhs003},
  citeulike-linkout-1  = {http://rfs.oxfordjournals.org/content/25/5/1588.abstract},
  citeulike-linkout-2  = {http://rfs.oxfordjournals.org/content/25/5/1588.full.pdf},
  day                  = {01},
  groups               = {RiskPremia_Other, FrcstQWIM_ShortTerm, FcstQWIM_Bond, Real_Yield_Curve},
  owner                = {cristi},
  posted-at            = {2016-06-18 16:05:54},
  publisher            = {Oxford University Press},
  timestamp            = {2020-02-25 22:23},
}

@Article{Ho-et-al-2014,
  author               = {Ho, Hsiao-Wei and Huang, Henry H. and Yildirim, Yildiray},
  date                 = {2014-05},
  journaltitle         = {European Journal of Operational Research},
  title                = {Affine model of inflation-indexed derivatives and inflation risk premium},
  doi                  = {10.1016/j.ejor.2013.12.010},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {159--169},
  volume               = {235},
  abstract             = {Propose affine model capturing inflation premium, nominal and real interest rates. Pricing of zero-coupon inflation-indexed swaps. Pricing of year-on-year inflation-indexed swaps. Pricing of inflation-indexed swaptions. Pricing of inflation-indexed caps and floors.

This paper proposes an affine-based approach which jointly captures the nominal interest rate, the real interest rate, and the inflation risk premium to price inflation-indexed derivatives, including zero-coupon inflation-indexed swaps, year-on-year inflation-indexed swaps, inflation-indexed swaptions, and inflation-indexed caps and floors.

We provide an example and explain how to use traded zero-coupon inflation-indexed swap rates to estimate inflation risk premiums.},
  citeulike-article-id = {13940607},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2013.12.010},
  groups               = {RiskPremia_Other},
  owner                = {zkgst0c},
  posted-at            = {2016-02-25 21:52:05},
  timestamp            = {2020-02-25 22:23},
}

@Article{Hordahl-Tristani-2012,
  author               = {Hordahl, Peter and Tristani, Oreste},
  date                 = {2012-06},
  journaltitle         = {Journal of the European Economic Association},
  title                = {Inflation risk premia in the term structure of interest rates},
  doi                  = {10.1111/j.1542-4774.2012.01067.x},
  number               = {3},
  pages                = {634--657},
  url                  = {https://www.ijcb.org/journal/ijcb14q3a1.pdf},
  volume               = {10},
  abstract             = {This paper estimates the size and dynamics of inflation risk premia in the euro area, based on a joint model of macroeconomic and term structure dynamics. Information from both nominal and index-linked yields is used in the empirical analysis. Our results indicate that the inflation risk premium on euro area 10-year nominal yields was approximately equal to 20 basis points on average over the 1999-2007 period. The inflation premium has also been subject to moderate, but statistically significant fluctuations.

For the post-2003 period in which reliable index-linked bond prices are available, our results suggest that increases in the raw break-even inflation rate above 2 the upper bound of the European Central Bank's definition for price stability, have mostly reflected variations in the inflation risk premium, while long-term inflation expectations have remained well anchored.},
  citeulike-article-id = {13940606},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/j.1542-4774.2012.01067.x},
  day                  = {1},
  groups               = {RiskPremia_Other},
  owner                = {zkgst0c},
  posted-at            = {2016-02-25 21:50:15},
  publisher            = {Blackwell Publishing Inc},
  timestamp            = {2020-02-25 22:23},
}

@Article{Hordahl-Tristani-2014,
  author               = {Hordahl, Peter and Tristani, Oreste},
  date                 = {2014-06},
  journaltitle         = {International Journal of Central Banking},
  title                = {Inflation Risk Premia in the Euro Area and the United States},
  url                  = {https://www.ijcb.org/journal/ijcb14q3a1.pdf},
  abstract             = {We use a joint model of macroeconomic and term structure dynamics to estimate inflation risk premia and inflation expectations in the United States and the euro area. To sharpen our estimation, we include in the information set macro data and survey data on inflation and interest rate expectations at various future horizons, as well as term structure data from both nominal and index-linked bonds.

Our results indicate that, over the post-2004 period when index-linked bond markets were sufficiently developed in both monetary areas, inflation risk premia across various maturities had strikingly similar properties in the United States and in the euro area: their dynamics and their levels, especially over the years until mid-2011, have remained quite close to each other, even if premia appear to be subject to somewhat greater high-frequency volatility in the United States. After correcting for liquidity and inflation risk premia, long-term inflation expectations extracted from bond prices have remained remarkably stable at the peak of the financial crisis and throughout the Great Recession

. For the United States, we also document a downward shift in the perceived inflation target, from approximately 3 percent until 2011 to levels closer to 2 percent following the FOMC announcement of a numerical long-term inflation goal.},
  citeulike-article-id = {13940606},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/j.1542-4774.2012.01067.x},
  day                  = {1},
  groups               = {RiskPremia_Other, Risk_Inflation},
  owner                = {zkgst0c},
  posted-at            = {2016-02-25 21:50:15},
  publisher            = {Blackwell Publishing Inc},
  timestamp            = {2020-02-25 22:23},
}

@Article{Fallon-Park-2016,
  author               = {Fallon, William and Park, James},
  date                 = {2016-10-31},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {An Asset Class Characterization of the U.S. Equity Index Volatility Risk Premium},
  doi                  = {10.3905/jpm.2016.43.1.072},
  issn                 = {0095-4918},
  number               = {1},
  pages                = {72--84},
  volume               = {43},
  abstract             = {The authors use a novel 32 year return series to study the risk, return, and predictability of a strategy that sells onemonth SandP 500 variance swaps with fixed exante tail risk. They find that unconditional short exposure in their sample is characterized primarily by two features: (1) a very high Sharpe ratio exceeding 1.2 and (2) a severe but infrequent crash risk. From a forecasting perspective, the authors find a generally lower premium following market selloffs and crashes. However, they fail to find significant evidence linking returns to the level of either implied or realized volatility.},
  citeulike-article-id = {14504246},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2016.43.1.072},
  day                  = {31},
  groups               = {RiskPremia_Forecast, FrcstQWIM_ShortTerm, FcstQWIM_Equity},
  posted-at            = {2017-12-17 21:52:54},
  timestamp            = {2020-02-25 22:23},
}

@Article{Faria-Verona-2017a,
  author               = {Faria, Goncalo and Verona, Fabio},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The Equity Risk Premium and the Low Frequency of the Term Spread},
  url                  = {https://ssrn.com/abstract=3030760},
  abstract             = {The term spread has long been of interest to policymakers and financial markets participants. Despite being a strong business cycle leading indicator, it is known to be a poor out-of-sample predictor of the equity risk premium. In this paper we show that its low-frequency component, when properly extracted from the data, is the best out-of-sample equity risk premium predictor to the date. Concretely, it obtains out-of-sample R-squares (versus the historical mean benchmark) of 2.03\% and 21.9\% for monthly and annual data, respectively, and generates utility gains of 585 basis points per annum for a mean-variance investor. Its outperformance is consistently stable throughout an out-of-sample period comprising more than 20 years of monthly data. Remarkably, it also forecasts well in expansions and outperforms several variables that have recently been proposed as good equity risk premium predictors.},
  citeulike-article-id = {14514964},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3030760},
  groups               = {RiskPremia_Forecast, FrcstQWIM_MedLngTerm, FcstQWIM_Equity},
  posted-at            = {2018-01-10 23:31:23},
  timestamp            = {2020-02-25 22:23},
}

@Article{Guimaraes-2016,
  author               = {Guimaraes, Rodrigo},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Expectations, Risk Premia and Information Spanning in Dynamic Term Structure Model Estimation},
  doi                  = {10.2139/ssrn.2420379},
  issn                 = {1556-5068},
  abstract             = {In this paper I show that the difficulty in estimating unconditional means from time series data alone is the cause for the lack of robustness in empirical estimates of the workhorse model in macro-finance. Using US and UK yield curve data and an extensive Monte Carlo study I show that using survey forecasts is effective in producing robust estimates of term premia, while alternative estimation methods and model restrictions are ineffective. I propose a new way to measure robustness that is simple and allows sharper comparisons across models and sheds new light on the nature of hidden and unspanned factors.},
  citeulike-article-id = {14486429},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2420379},
  groups               = {RiskPremia_Forecast},
  posted-at            = {2017-11-30 19:21:55},
  timestamp            = {2020-02-25 22:23},
}

@Article{Kozhan-et-al-2013,
  author               = {Kozhan, Roman and Neuberger, Anthony and Schneider, Paul},
  date                 = {2013-09},
  journaltitle         = {Review of Financial Studies},
  title                = {The Skew Risk Premium in the Equity Index Market},
  doi                  = {10.1093/rfs/hht039},
  issn                 = {1465-7368},
  number               = {9},
  pages                = {2174--2203},
  volume               = {26},
  abstract             = {We develop a new method for measuring moment risk premiums. We find that the skew premium accounts for over 40 percent of the slope in the implied volatility curve in the SandP 500 market. Skew risk is tightly related to variance risk, in the sense that strategies designed to capture the one and hedge out exposure to the other earn an insignificant risk premium. This provides a new testable restriction for asset pricing models trying to capture, in particular, disaster risk premiums. We base our results on a general trading strategy by replicating contracts that swap implied for realized conditional asset moments.},
  citeulike-article-id = {13993194},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rfs/hht039},
  citeulike-linkout-1  = {http://rfs.oxfordjournals.org/content/26/9/2174.abstract},
  citeulike-linkout-2  = {http://rfs.oxfordjournals.org/content/26/9/2174.full.pdf},
  day                  = {01},
  groups               = {RiskPremia_Equity},
  owner                = {cristi},
  posted-at            = {2016-04-02 08:44:20},
  publisher            = {Oxford University Press},
  timestamp            = {2020-02-25 22:24},
}

@Article{Neely-et-al-2014,
  author               = {Neely, Christopher J. and Rapach, David E. and Tu, Jun and Zhou, Guofu},
  date                 = {2014-07},
  journaltitle         = {Management Science},
  title                = {Forecasting the Equity Risk Premium: The Role of Technical Indicators},
  doi                  = {10.1287/mnsc.2013.1838},
  issn                 = {0025-1909},
  number               = {7},
  pages                = {1772--1791},
  volume               = {60},
  abstract             = {Academic research relies extensively on macroeconomic variables to forecast the U.S. equity risk premium, with relatively little attention paid to the technical indicators widely employed by practitioners. Our paper fills this gap by comparing the predictive ability of technical indicators with that of macroeconomic variables. Technical indicators display statistically and economically significant in-sample and out-of-sample predictive power, matching or exceeding that of macroeconomic variables. Furthermore, technical indicators and macroeconomic variables provide complementary information over the business cycle: technical indicators better detect the typical decline in the equity risk premium near business-cycle peaks, whereas macroeconomic variables more readily pick up the typical rise in the equity risk premium near cyclical troughs. Consistent with this behavior, we show that combining information from both technical indicators and macroeconomic variables significantly improves equity risk premium forecasts versus using either type of information alone. Overall, the substantial countercyclical fluctuations in the equity risk premium appear well captured by the combined information in technical indicators and macroeconomic variables.},
  citeulike-article-id = {13982381},
  citeulike-linkout-0  = {http://dx.doi.org/10.1287/mnsc.2013.1838},
  groups               = {RiskPremia_Equity, RiskPremia_Forecast, FrcstQWIM_Equity},
  owner                = {cristi},
  posted-at            = {2016-10-17 16:01:05},
  timestamp            = {2020-02-25 22:24},
  year                 = {2014},
}

@Article{Joslin-et-al-2014,
  author               = {Joslin, Scott and Priebsch, Marcel and Singleton, Kenneth J.},
  date                 = {2014-06},
  journaltitle         = {The Journal of Finance},
  title                = {Risk Premiums in Dynamic Term Structure Models with Unspanned Macro Risks},
  doi                  = {10.1111/jofi.12131},
  number               = {3},
  pages                = {1197--1233},
  volume               = {69},
  abstract             = {This paper quantifies how variation in economic activity and inflation in the United States influences the market prices of level, slope, and curvature risks in Treasury markets. We develop a novel arbitrage-free dynamic term structure model in which bond investment decisions are influenced by output and inflation risks that are unspanned by (imperfectly correlated with) information about the shape of the yield curve.

Our model reveals that, between 1985 and 2007, these risks accounted for a large portion of the variation in forward terms premiums, and there was pronounced cyclical variation in the market prices of level and slope risks.},
  citeulike-article-id = {13943182},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/jofi.12131},
  day                  = {1},
  groups               = {RiskPremia_FixedIncome},
  owner                = {cristi},
  posted-at            = {2016-02-28 03:23:52},
  timestamp            = {2020-02-25 22:24},
}

@Article{Lee-2018,
  author         = {Lee, Jaehoon},
  date           = {2018-02},
  journaltitle   = {Journal of Financial and Quantitative Analysis},
  title          = {Risk Premium Information from Treasury-Bill Yields},
  doi            = {10.1017/S0022109017000813},
  issn           = {0022-1090},
  number         = {01},
  pages          = {437--454},
  volume         = {53},
  abstract       = {I find that short-maturity Treasury-bill yields have unique information about risk premiums that is not spanned by long-maturity Treasury-bond yields. I estimate 2 components of risk premiums: long term and short term. The long-term component steepens the slope of yield curves and has a forecastability horizon of longer than 1 year. In contrast, the short-term component affects Treasury-bill yields but is almost invisible from Treasury bonds, has a forecastability horizon of less than 1 quarter, and is related to bond liquidity premiums.},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_FixedIncome, RiskPremia_Forecast, FrcstQWIM_ShortTerm, FrcstQWIM_MedLngTerm, FcstQWIM_Bond},
  timestamp      = {2020-02-25 22:24},
}

@Article{Lemperiere-et-al-2014,
  author               = {Lemperiere, Yves and Deremble, Cyril and Nguyen, Trung-Tu and Seager, Philip A. and Potters, Marc and Bouchaud, Jean-Philippe},
  date                 = {2014-09},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Risk Premia: Asymmetric Tail Risks and Excess Returns},
  url                  = {https://ssrn.com/abstract=2502743},
  abstract             = {We present extensive evidence that Risk Premium is strongly correlated with tail-risk skewness, but very little with volatility. We introduce a new, intuitive definition of skewness, and elicit a linear relationship between the Sharpe ratio of various risk premium strategies (Equity, Fama-French, FX Carry, short vol, bonds, credit) and their negative skewness. We find a clear exception to this rule: Trend Following (and perhaps the Fama-French High minus Low ), that has positive skewness and positive excess return, suggesting that some strategies are not risk premia, but genuine market anomalies. Based on our results, we propose an objective criterion to assess the quality of a risk premium portfolio.},
  citeulike-article-id = {13982661},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2502743},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2502743code1657787.pdf?abstractid=2502743 and mirid=1},
  day                  = {30},
  groups               = {RiskPremia_FixedIncome, RiskPremia_Other},
  owner                = {cristi},
  posted-at            = {2016-03-17 21:21:09},
  timestamp            = {2020-02-25 22:24},
}

@Article{Ludvigson-Ng-2009,
  author               = {Ludvigson, Sydney C. and Ng, Serena},
  date                 = {2009-12},
  journaltitle         = {Review of Financial Studies},
  title                = {Macro Factors in Bond Risk Premia},
  doi                  = {10.1093/rfs/hhp081},
  issn                 = {1465-7368},
  number               = {12},
  pages                = {5027--5067},
  volume               = {22},
  abstract             = {Are there important cyclical fluctuations in bond market premiums and, if so, with what macroeconomic aggregates do these premiums vary? We use the methodology of dynamic factor analysis for large datasets to investigate possible empirical linkages between forecastable variation in excess bond returns and macroeconomic fundamentals. We find that - real- and - inflation- factors have important forecasting power for future excess returns on U.S. government bonds, above and beyond the predictive power contained in forward rates and yield spreads. This behavior is ruled out by commonly employed affine term structure models where the forecastability of bond returns and bond yields is completely summarized by the cross-section of yields or forward rates. An important implication of these findings is that the cyclical behavior of estimated risk premia in both returns and long-term yields depends importantly on whether the information in macroeconomic factors is included in forecasts of excess bond returns. Without the macro factors, risk premia appear virtually acyclical, whereas with the estimated factors risk premia have a marked countercyclical component, consistent with theories that imply investors must be compensated for risks associated with macroeconomic activity.},
  citeulike-article-id = {14167253},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rfs/hhp081},
  citeulike-linkout-1  = {http://rfs.oxfordjournals.org/content/22/12/5027.abstract},
  citeulike-linkout-2  = {http://rfs.oxfordjournals.org/content/22/12/5027.full.pdf},
  day                  = {01},
  groups               = {RiskPremia_FixedIncome, RiskPremia_Forecast, FrcstQWIM_MedLngTerm, FcstQWIM_Bond},
  owner                = {cristi},
  posted-at            = {2016-10-18 08:47:55},
  publisher            = {Oxford University Press},
  timestamp            = {2020-02-25 22:24},
}

@Article{Murphy-et-al-2014,
  author               = {Murphy, Austin and Fu, Liang and Benzschawel, Terry},
  date                 = {2014-05},
  journaltitle         = {The Journal of Investing},
  title                = {An Empirical Examination of Ex Ante Estimates of the Market Risk Premium},
  doi                  = {10.3905/joi.2014.23.2.051},
  issn                 = {1068-0896},
  number               = {2},
  pages                = {51--58},
  volume               = {23},
  abstract             = {This research investigates a new method for estimating the price of risk in the market using information on bond yield spreads. An empirical examination of the model indicates the ex-ante risk premium estimates, which vary significantly over time, have an average one-to-one relationship with future market returns. There is strong evidence that the procedure supplies forecasts that can be very useful in predicting future returns on stocks, high-yield bonds, and the overall market. It may therefore also assist in estimating the required return on assets needed to compute the present value of investments.},
  citeulike-article-id = {14322287},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2014.23.2.051},
  groups               = {RiskPremia_FixedIncome, RiskPremia_Forecast, FrcstQWIM_MedLngTerm, FrcstQWIM_Bond, ExAnte_ExPost},
  posted-at            = {2017-03-29 08:04:00},
  timestamp            = {2020-02-25 22:24},
}

@Article{Jorda-et-al-2018,
  author         = {Jorda, Oscar and Schularick, Moritz and Taylor, Alan M. and Ward, Felix},
  date           = {2018-06-01},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Global Financial Cycles and Risk Premiums},
  url            = {https://ssrn.com/abstract=3190214},
  abstract       = {This paper studies the synchronization of financial cycles across 17 advanced economies over the past 150 years. The comovement in credit, house prices, and equity prices has reached historical highs in the past three decades. The sharp increase in the comovement of global equity markets is particularly notable. We demonstrate that fluctuations in risk premiums, and not risk-free rates and dividends, account for a large part of the observed equity price synchronization after 1990. We also show that U.S. monetary policy has come to play an important role as a source of fluctuations in risk appetite across global equity markets. These fluctuations are transmitted across both fixed and floating exchange rate regimes, but the effects are more muted in floating rate regimes.},
  day            = {1},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_Other, RiskPremia_Regime},
  timestamp      = {2020-02-25 22:24},
}

@Article{Joyce-et-al-2010,
  author               = {Joyce, Michael A. S. and Lildholdt, Peter and Sorensen, Steffen},
  date                 = {2010-02},
  journaltitle         = {Journal of Banking \& Finance},
  title                = {Extracting inflation expectations and inflation risk premia from the term structure: A joint model of the UK nominal and real yield curves},
  doi                  = {10.1016/j.jbankfin.2009.07.018},
  issn                 = {0378-4266},
  number               = {2},
  pages                = {281--294},
  volume               = {34},
  abstract             = {This paper analyses the UK interest rate term structure over the period since October 1992, when the United Kingdom adopted an explicit inflation target, using an affine term structure model estimated using both government bond yields and survey data. The model imposes no-arbitrage restrictions across nominal and real yields, which enables interest rates to be decomposed into expected real policy rates, expected inflation, real term premia and inflation risk premia. The model is used to shed light on major developments over the period, including the impact of Bank of England independence and the low real bond yield 'conundrum'.},
  citeulike-article-id = {5342247},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbankfin.2009.07.018},
  day                  = {30},
  groups               = {RiskPremia_Other, Real_Yield_Curve},
  owner                = {zkgst0c},
  posted-at            = {2016-02-25 20:47:27},
  timestamp            = {2020-02-25 22:24},
}

@Article{Jurek-Xu-2014,
  author               = {Jurek, Jakub W. and Xu, Zhikai},
  date                 = {2013-10},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Option-Implied Currency Risk Premia},
  url                  = {https://ssrn.com/abstract=2338585},
  abstract             = {We obtain ex ante estimates of risk premia for G10 currency pairs using cross-sectional data on exchange rate options. Option prices are well-matched by a non-Gaussian, two-factor model, consistent with evidence from realized currency returns. We find that option-implied currency risk premia provide an unbiased forecast of monthly currency excess returns, and achieve cross-sectional forecasting R2s of up to 44 percent. Despite prominent non-normalities in option data, less than 20 percent of the model HML-FX risk premium, or roughly 70bps per annum, is due to the asymmetries and higher-moments of global risks.},
  citeulike-article-id = {13948060},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2338585},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2510951code449598.pdf?abstractid=2338585 and mirid=1},
  day                  = {13},
  groups               = {RiskPremia_Other, RiskPremia_Forecast, FrcstQWIM_Other},
  owner                = {cristi},
  posted-at            = {2016-03-01 22:44:05},
  timestamp            = {2020-02-25 22:24},
}

@Article{Kang-Pan-2013,
  author               = {Kang, Sang B. and Pan, Xuhui N.},
  date                 = {2013-07},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Commodity Variance Risk Premia and Expected Futures Returns: Evidence from the Crude Oil Market},
  url                  = {https://ssrn.com/abstract=2296932},
  abstract             = {We develop an extended mean-variance model to investigate the relationship between variance risk premia (VRP) and expected futures returns in the commodity market. In the presence of stochastic variance, commodity producers trade both futures and options to hedge their exposure to commodity price and volatility risk; speculators provide liquidity and ask for risk premia. This model reveals a negative relationship between VRP and expected futures returns. Empirically, we measure VRP using options and high-frequency futures data in the crude oil market. Consistent with our model, we find that VRP negatively predict futures returns even after controlling for other predictor variables.},
  citeulike-article-id = {14025457},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2296932},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2643085code742850.pdf?abstractid=2296932 and mirid=1},
  day                  = {24},
  groups               = {RiskPremia_Other, Invest_Cmdty},
  owner                = {cristi},
  posted-at            = {2016-05-01 14:43:52},
  timestamp            = {2020-02-25 22:24},
}

@Article{Lamponi-Latto-2017,
  author               = {Lamponi, Daniele and Latto, David},
  date                 = {2017-07},
  journaltitle         = {The Journal of Wealth Management},
  title                = {The Life Cycle of Dividend Futures and the Dividend Risk Premium: A Practitioner's Perspective},
  doi                  = {10.3905/jwm.2017.20.2.067},
  issn                 = {1534-7524},
  number               = {2},
  pages                = {67--75},
  volume               = {20},
  abstract             = {Dividend futures contracts, which allow investors to gain direct exposure to the dividend stream of equity indexes, has recently gained relevance for the investment community. In this article, we discuss their lifecycle and show how they are exposed to multiple factors that drive their prices. For example, far from maturity, their prices are driven by earnings growth and inflation expectations. As the expiration dates approach, the visibility on earnings and dividends improves and prices converge toward realized dividends. We empirically show that this convergence starts between one and two years before expiration.},
  citeulike-article-id = {14402607},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2017.20.2.067},
  groups               = {RiskPremia_Other},
  posted-at            = {2017-07-30 07:50:46},
  timestamp            = {2020-02-25 22:24},
}

@Article{Lettau-et-al-2014,
  author               = {Lettau, Martin and Maggiori, Matteo and Weber, Michael},
  date                 = {2014-11},
  journaltitle         = {Journal of Financial Economics},
  title                = {Conditional risk premia in currency markets and other asset classes},
  doi                  = {10.1016/j.jfineco.2014.07.001},
  issn                 = {0304-405X},
  number               = {2},
  pages                = {197--225},
  volume               = {114},
  abstract             = {The downside risk capital asset pricing model (DR-CAPM) can price the cross section of currency returns. The market-beta differential between high and low interest rate currencies is higher conditional on bad market returns, when the market price of risk is also high, than it is conditional on good market returns. Correctly accounting for this variation is crucial for the empirical performance of the model. The DR-CAPM can jointly rationalize the cross section of equity, equity index options, commodity, sovereign bond and currency returns, thus offering a unified risk view of these asset classes. In contrast, popular models that have been developed for a specific asset class fail to jointly price other asset classes.},
  citeulike-article-id = {14025480},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jfineco.2014.07.001},
  groups               = {RiskPremia_Other},
  owner                = {cristi},
  posted-at            = {2016-05-01 15:03:03},
  timestamp            = {2020-02-25 22:24},
}

@Article{Liu-et-al-2011,
  author               = {Liu, Peixin P. and Xu, Kuan and Zhao, Yonggan},
  date                 = {2011-04},
  journaltitle         = {International Journal of Managerial Finance},
  title                = {Market regimes, sectorial investments, and time varying risk premiums},
  doi                  = {10.1108/17439131111122120},
  issn                 = {1743-9132},
  number               = {2},
  pages                = {107--133},
  volume               = {7},
  abstract             = {This paper aims to extend the Fama and French (FF) three factor model in studying time varying risk premiums of Sector Select Exchange Traded Funds (ETFs) under a Markov regime switching framework.

First, the original FF model is augmented to include three additional macro factors market volatility, yield spread, and credit spread. Then, the FF model is extended to a model with a Markov regime switching mechanism for bull, bear, and transition market regimes.

It is found that all market regimes are persistent, with the bull market regime being the most persistent, and the bear market regime being the least persistent. Both the risk premiums of the Sector Select ETFs and their sensitivities to the risk factors are highly regime dependent.

The regime switching model has a superior performance in capturing the risk sensitivities of the Sector Select ETFs, that would otherwise be missed by both the FF and the augmented FF models.

This is the first research on Sector Select ETFs with Markov regime switching.},
  citeulike-article-id = {13987916},
  citeulike-linkout-0  = {http://dx.doi.org/10.1108/17439131111122120},
  day                  = {05},
  groups               = {RiskPremia_Other, RiskPremia_Regime},
  owner                = {cristi},
  posted-at            = {2016-03-25 09:11:31},
  timestamp            = {2020-02-25 22:24},
}

@Article{Londono-Zhou-2012,
  author               = {Londono, Juan M. and Zhou, Hao},
  date                 = {2012-08},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Variance Risk Premiums and the Forward Premium Puzzle},
  url                  = {https://ssrn.com/abstract=2133569},
  abstract             = {We provide new empirical evidence that world currency and U.S. stock variance risk premiums have nonredundant and significant predictive power for the appreciation rates of twenty-two currencies with respect to the U.S. dollar, especially at the 4-month and 1-month horizons, respectively. The heterogeneous exposures of currencies to the currency variance risk premium seem to be systematic and asymmetric along the line of inflation risk. We rationalize these findings in a consumption-based international asset pricing model, with orthogonal local consumption uncertainty and asymmetric global inflation uncertainty characterized, respectively, by the stock and currency variance risk premiums.},
  citeulike-article-id = {14025463},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2133569},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2622541code1376087.pdf?abstractid=2133569 and mirid=1},
  day                  = {22},
  groups               = {RiskPremia_Other},
  owner                = {cristi},
  posted-at            = {2016-05-01 14:48:02},
  timestamp            = {2020-02-25 22:24},
}

@Article{Mijatovic-Schneider-2014,
  author               = {Mijatovic, Aleksandar and Schneider, Paul},
  date                 = {2014-06},
  journaltitle         = {Journal of Financial Econometrics},
  title                = {Empirical Asset Pricing with Nonlinear Risk Premia},
  doi                  = {10.1093/jjfinec/nbt018},
  issn                 = {1479-8417},
  number               = {3},
  pages                = {479--506},
  volume               = {12},
  abstract             = {We introduce a new model for the joint dynamics of the S and P 100 index and the VXO implied volatility index. The nonlinear specification of the variance process is designed to simultaneously accommodate extreme persistence and strong mean reversion. This grants superior forecasting power over the standard (linear) specifications for implied variance forecasting. We obtain statistically significant predictions in an out-of-sample exercise spanning several market crashes starting 1986 and including the recent subprime crisis. The model specification is possible through a simple continuous-time no-arbitrage asset pricing framework that combines semi-analytic pricing with a nonlinear specification for the market price of risk.},
  citeulike-article-id = {14071822},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/jjfinec/nbt018},
  citeulike-linkout-1  = {http://jfec.oxfordjournals.org/content/12/3/479.abstract},
  citeulike-linkout-2  = {http://jfec.oxfordjournals.org/content/12/3/479.full.pdf},
  day                  = {01},
  groups               = {RiskPremia_Other, RiskPremia_Forecast},
  owner                = {cristi},
  posted-at            = {2016-06-18 18:43:59},
  publisher            = {Oxford University Press},
  timestamp            = {2020-02-25 22:24},
}

@Article{Naya-Tuchschmid-2017,
  author               = {Naya, Francesc and Tuchschmid, Nils},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Alternative Risk Premia: Is the Selection Process Important?},
  url                  = {https://ssrn.com/abstract=3045057},
  abstract             = {Alternative Risk Premia (ARP) are systematic or rule-based strategies. They originate from the principle that investors should be rewarded for the sources of systematic risk they are exposed to. However, as opposed to traditional market risk factors to which anyone is familiar with, ARP must compensate investors for taking non-traditional risks. Equity size or volatility carry strategies are two examples of ARP. Allocation to ARP is yet not as straightforward as one thinks. Investors quickly realize that they not only have to decide to which risk premia they want to allocate capital but that they must also select the "indices" into which money will be directed. The latter problem sounds trivial a priori. Yet, it is not. First, the list of "indices" proposed by a series of different providers is quite long and it is not sure that indices that aim at capturing the same risk premium truly behave the same. Second, it is not because an index is proposed that it really mimics the performance of an existing risk premium. Stated otherwise, nothing guarantees that the performance achieved over time by an index is truly sustainable or persistent. It might be illusory and stemming from selection or overfitting bias. In this research paper, we perform a series of correlations and drawdown tests and apply clustering techniques to analyze the degree of homogeneity among the ARP indices. Our findings confirm the initial suspicion. Some indices show risk-return characteristics that are rather homogeneous indicating in turn that their underlying risk premia is well defined. FX carry for instance belongs to this category. Yet, results are not so clear-cut for others. Some indices are highly heterogeneous even if they claim to capture the same risk premium. Stated otherwise, performance is highly provider's dependent making the choice of an index an important component of the allocation process. Equity Value is an example. We then test for the existence of overfitting biases. We compare performance between "backtest" and "life" periods for a series of 255 indices. Results are indisputable. Performance decreases quasi-systematically once the ARP indices are launched, and the drop is severe. Investors should apply an 80\% discount on performance measures shown in a backtest if they wish to manage well their expectation. To summarize, this research paper shows that investors need to take no short cuts. When it comes to allocate capital to ARP, an extensive due diligence process is required to select both the indices and their providers.},
  citeulike-article-id = {14449179},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3045057},
  groups               = {RiskPremia_Other, RiskPremia_Alt},
  posted-at            = {2017-10-11 23:05:23},
  timestamp            = {2020-02-25 22:24},
}

@Article{Nielsen-2018,
  author         = {Nielsen, Andreas Bang},
  date           = {2018-04-20},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Systematic Currency Volatility Risk Premia},
  url            = {https://ssrn.com/abstract=3171795},
  abstract       = {I show that volatility risk of the dollar factor --- an equally weighted basket of developed U.S. dollar exchange rates --- carries a significant risk premium and that it is priced in the cross-section of currency volatility excess returns. The dollar factor volatility risk premium is negative on average with an upward sloping and concave term structure. Consistent with this pattern, I find that dollar factor volatility risk is most significantly priced in the cross-section of volatility excess returns at shorter maturities. A trading strategy that sells (buys) volatility insurance on currencies with high (low) exposure to dollar factor volatility risk delivers high mean excess returns and Sharpe ratios. At shorter maturities, the profitability of this strategy cannot be explained by exposure to traditional currency factors, equity factors, or currency volatility carry factors.},
  day            = {20},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_Other},
  timestamp      = {2020-02-25 22:24},
}

@Article{Jacquier-et-al-2005,
  author               = {Jacquier, E. and Kane, Alex and Marcus, Alan J.},
  date                 = {2005-01},
  journaltitle         = {Journal of Financial Econometrics},
  title                = {Optimal Estimation of the Risk Premium for the Long Run and Asset Allocation: A Case of Compounded Estimation Risk},
  doi                  = {10.1093/jjfinec/nbi001},
  issn                 = {1479-8409},
  number               = {1},
  pages                = {37--55},
  volume               = {3},
  abstract             = {It is well known that an unbiased forecast of the terminal value of a portfolio requires compounding at the arithmetic mean return over the investment horizon. However, the maximum-likelihood practice, common with academics, of compounding at the estimator of mean return results in upward biased and highly inefficient estimates of long-term expected returns. We derive analytically both an unbiased and a small-sample efficient estimator of long-term expected returns for a given sample size and horizon. Both estimators entail penalties that reduce the annual compounding rate as the investment horizon increases. The unbiased estimator, which is far lower than the compounded arithmetic average, is still very inefficient, often more so than a simple geometric estimator known to practitioners. Our small-sample efficient estimator is even lower. These results compound the sobering evidence in recent work that the equity risk premium is lower than suggested by post-1926 data. Our methodology and results are robust to extensions such as predictable returns. We also confirm analytically that parameter uncertainty, properly incorporated, produces optimal asset allocations, in stark contrast to conventional wisdom. Longer investment horizons require lower, not higher, allocations to risky assets.},
  citeulike-article-id = {78201},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/jjfinec/nbi001},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/oup/jfinec/2005/00000003/00000001/art00037},
  day                  = {01},
  groups               = {RiskPremia_Forecast, FrcstQWIM_MedLngTerm, FcstQWIM_Equity},
  posted-at            = {2017-05-13 17:56:58},
  publisher            = {Oxford University Press},
  timestamp            = {2020-02-25 22:24},
}

@Article{Maneesoonthorn-et-al-2012,
  author               = {Maneesoonthorn, Worapree and Martin, Gael M. and Forbes, Catherine S. and Grose, Simone D.},
  date                 = {2012-12},
  journaltitle         = {Journal of Econometrics},
  title                = {Probabilistic forecasts of volatility and its risk premia},
  doi                  = {10.1016/j.jeconom.2012.06.006},
  issn                 = {0304-4076},
  number               = {2},
  pages                = {217--236},
  volume               = {171},
  abstract             = {The object of this paper is to produce distributional forecasts of asset price volatility and its associated risk premia using a non-linear state space approach. Option and spot market information on the latent variance process is captured by using dual 'model-free' variance measures to define a bivariate observation equation in the state space model. The premium for variance diffusive risk is defined as linear in the latent variance (in the usual fashion) whilst the premium for variance jump risk is specified as a conditionally deterministic dynamic process, driven by a function of past measurements. The inferential approach adopted is Bayesian, implemented via a Markov chain Monte Carlo algorithm that caters for the multiple sources of non-linearity in the model and for the bivariate measure. The method is applied to spot and option price data on the SandP 500 index from 1999 to 2008, with conclusions drawn about investors' required compensation for variance risk during the recent financial turmoil. The accuracy of the probabilistic forecasts of the observable variance measures is demonstrated, and compared with that of forecasts yielded by alternative methods. To illustrate the benefits of the approach, it is used to produce forecasts of prices of derivatives on volatility itself. In addition, the posterior distribution is augmented by information on daily returns to produce value at risk predictions. Linking the variance risk premia to the risk aversion parameter in a representative agent model, probabilistic forecasts of (approximate) relative risk aversion are also produced.},
  citeulike-article-id = {10867330},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jeconom.2012.06.006},
  groups               = {RiskPremia_Forecast, RiskPremia_Alt, FrcstQWIM_MedLngTerm},
  owner                = {cristi},
  posted-at            = {2016-06-18 18:43:09},
  timestamp            = {2020-02-25 22:24},
}

@Article{Oikonomikou-2016,
  author               = {Oikonomikou, Leoni E.},
  date                 = {2016-03},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Forecasting the Market Risk Premium with Artificial Neural Networks},
  url                  = {https://ssrn.com/abstract=2743374},
  abstract             = {This paper aims to forecast the Market Risk premium (MRP) in the US stock market by applying machine learning techniques, namely the Multilayer Perceptron Network (MLP), the Elman Network (EN) and the Higher Order Neural Network (HONN). Furthermore, Univariate ARMA and Exponential Smoothing models are also tested.The Market Risk Premium is defined as the historical differential between the return of the benchmark stock index over a short-term interest rate. Data are taken in daily frequency from January 2007 through December 2014. All these models outperform a Naive benchmark model. The Elman network outperforms all the other models during the insample period, whereas the MLP network provides superior results in the out-of-sample period.The contribution of this paper to the existing literature is twofold. First, it is the first study that attempts to forecast the Market Risk Premium in a daily basis using Artificial Neural Networks (ANNs). Second, it is not based on a theoretical model but is mainly data driven. The chosen calculation approach fits quite well with the characteristics of ANNs. The forecasting model is tested with data from the US stock market. The proposed model-based forecasting method aims to capture patterns in the data that improve the forecasting accuracy of the Market Risk Premium in the tested market and indicates potential key metrics for investment and trading purposes for short time horizons.},
  citeulike-article-id = {13979743},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2743374},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2743374code2517965.pdf?abstractid=2743374 and mirid=1},
  day                  = {12},
  groups               = {Machine learning and investment strategies, RiskPremia_Forecast, FrcstQWIM_ShortTerm, FrcstQWIM_Equity},
  owner                = {cristi},
  posted-at            = {2016-03-14 16:24:12},
  timestamp            = {2020-02-25 22:24},
}

@Book{Jurczenko-et-al-2017,
  author               = {{Jurczenko, E. et al.}},
  date                 = {2017},
  title                = {Factor Investing: From Traditional to Alternative Risk Premia},
  editor               = {Jurczenko, Emmanuel},
  publisher            = {Elsevier},
  url                  = {https://www.elsevier.com/books/factor-investing/jurczenko/978-1-78548-201-4},
  abstract             = {This new edited volume consists of a collection of original articles written by leading industry experts in the area of factor investing. The chapters introduce readers to some of the latest research developments in the area of equity and alternative investment strategies.Each chapter deals with new methods for constructing and harvesting traditional and alternative risk premia, building strategic and tactical multifactor portfolios, and assessing related systematic investment performances. This volume will be of help to portfolio managers, asset owners and consultants, as well as academics and students who want to improve their knowledge and understanding of systematic risk factor investing.},
  citeulike-article-id = {14498481},
  groups               = {Invest_Factor, Factor_Types, Factor_Selection, Factor_Test, RiskPremia_Alt, Multifactor_Invest},
  posted-at            = {2017-12-06 23:30:12},
  timestamp            = {2020-02-25 22:25},
}

@Article{Maeso-Martellini-2017,
  author               = {Maeso, Jean-Michel and Martellini, Lionel},
  date                 = {2017-05},
  journaltitle         = {The Journal of Alternative Investments},
  title                = {Factor Investing and Risk Allocation: From Traditional to Alternative Risk Premiums Harvesting},
  doi                  = {10.3905/jai.2017.2017.1.055},
  issn                 = {1520-3255},
  abstract             = {Sophisticated institutional investors have a growing interest in factor investing, a disciplined approach to portfolio management that is broadly meant to allow investors to harvest risk premiums across and within asset classes through liquid and cost-efficient systematic strategies without having to invest with active managers. Although it is now well accepted that the average long-term performance of active mutual fund managers can to a large extent be replicated through a static exposure to traditional long-only risk premiums, an outstanding question remains with respect to the best possible approach for harvesting alternative long-short risk premiums.

The focus of this article is to empirically analyze (1) whether systematic rules-based strategies based on investable versions of traditional and alternative factors allow for satisfactory in-sample and out-of-sample replication of hedge fund performance, and more generally (2) whether suitably designed risk allocation strategies may provide a cost-efficient way for investors to get attractive exposure to alternative factors, regardless of whether they can be regarded as proxies for any particular hedge fund strategy. The main findings can be summarized as follows. On one hand, the authors find that replication models for hedge funds strategies generally achieve a relatively low out-of-sample explanatory power, regardless of the set of factors and the methodologies used. On the other hand, they that find that heuristic allocation strategies, such as risk parity strategies, applied to alternative risk factors could be a better alternative to hedge fund replication for efficiently harvesting alternative risk premiums.},
  citeulike-article-id = {14368294},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jai.2017.2017.1.055},
  day                  = {31},
  groups               = {Invest_Risk, RiskPremia_Alt},
  posted-at            = {2017-06-05 22:30:34},
  timestamp            = {2020-02-25 22:25},
}

@Article{Melas-Subramanian-2012,
  author               = {Melas, Dimitris and Subramanian, Madhu},
  date                 = {2012},
  journaltitle         = {ETF and Indexing},
  title                = {Harvesting Risk Premia in Emerging Markets},
  volume               = {1},
  abstract             = {Emerging markets play an increasingly important role in the global economy and in the asset allocation of institutional investors. However, high allocation to emerging markets typically leads to high portfolio volatility. The authors examine alternative index strategies that could potentially mitigate the inherent volatility of this asset class. By reweighting a standard benchmark, such as the MSCI Emerging Markets Index, some of these passive strategies have historically experienced lower volatility or better performance compared with standard benchmarks. In addition, the authors examine methods of gaining indirect exposure to emerging markets through an index of developed market companies that derive a high percentage of their revenues from emerging markets. Innovations in indices and ETFs offer new ways of accessing emerging markets while addressing specific investment objectives and constraints.},
  citeulike-article-id = {14419908},
  edition              = {118-122},
  groups               = {RiskPremia_Alt},
  posted-at            = {2017-08-27 10:43:10},
  timestamp            = {2020-02-25 22:25},
}

@Article{Rauch-Alexander-2016,
  author               = {Rauch, Johannes and Alexander, Carol},
  date                 = {2016},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Tail Risk Premia for Long-Term Equity Investors},
  eprint               = {1602.00865},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1602.00865},
  abstract             = {We use the P and L on a particular class of swaps, representing variance and higher moments for log returns, as estimators in our empirical study on the S\&P 500 that investigates the factors determining variance and higher-moment risk premia. This class is the discretisation invariant sub-class of swaps with Neuberger's aggregating characteristics. Besides the market excess return, momentum is the dominant driver for both skewness and kurtosis risk premia, which exhibit a highly significant negative correlation. By contrast, the variance risk premium responds positively to size and negatively to growth, and the correlation between variance and tail risk premia is relatively low compared with previous research, particularly at high sampling frequencies. These findings extend prior research on determinants of these risk premia. Furthermore, our meticulous data-construction methodology avoids unwanted artefacts which distort results.},
  citeulike-article-id = {14025486},
  citeulike-linkout-0  = {http://arxiv.org/abs/1602.00865},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1602.00865},
  day                  = {2},
  groups               = {Invest_TailRisk, RiskPremia_Equity},
  owner                = {cristi},
  posted-at            = {2016-05-01 15:08:41},
  timestamp            = {2020-02-25 22:25},
}

@Article{Rozeff-1984,
  author       = {Rozeff, Michael S},
  date         = {1984},
  journaltitle = {Journal of Portfolio Management},
  title        = {Dividend yields are equity risk premiums},
  groups       = {RiskPremia_Equity, [nbkcbu3:]},
  timestamp    = {2020-02-25 22:25},
}

@Article{Sauer-Tyrell-2018,
  author         = {Sauer, Maximilian and Tyrell, Marcel},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Risk premia to political uncertainty in global equity markets},
  doi            = {10.2139/ssrn.3266461},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3266461},
  abstract       = {Political uncertainty drives markets. Among macroeconomic forces, it is one of the fewfactors that systematically affect most assets - hence it qualifies as a state variable in the senseof the ICAPM and should carry a risk premium. We employ static and conditional factormodels using data in Emerging Markets, the European Monetary Union and the United Statesof America. Through Fama-Macbeth regressions we find a significant risk premium whichincreases in bad economic times when central banks are expected to ease monetary policy andthe general level of uncertainty is high.},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_Equity},
  timestamp      = {2020-02-25 22:25},
}

@TechReport{Siegel-2017,
  author               = {Siegel, Laurence B.},
  date                 = {2017},
  institution          = {CFA Institute Research Foundation},
  title                = {The Equity Risk Premium: A Contextual Literature Review},
  url                  = {https://www.cfainstitute.org/en/research/foundation/2017/equity-risk-premium},
  abstract             = {Research into the equity risk premium, often considered the most important number in finance, falls into three broad groupings. First, researchers have measured the margin by which equity total returns have exceeded fixed-income or cash returns over long historical periods and have projected this measure of the equity risk premium into the future. Second, the dividend discount model-or a variant of it, such as an earnings discount model-is used to estimate the future return on an equity index, and the fixed-income or cash yield is then subtracted to arrive at an equity risk premium expectation or forecast. Third, academics have used macroeconomic techniques to estimate what premium investors might rationally require for taking the risk of equities. Current thinking emphasizes the second, or dividend discount, approach and projects an equity risk premium centered on 3.5\% to 4\%.},
  citeulike-article-id = {14501237},
  groups               = {RiskPremia_Equity, RiskPremia_Forecast, FrcstQWIM_MedLngTerm, FrcstQWIM_Equity},
  posted-at            = {2017-12-11 19:34:00},
  series               = {Research Foundation Reviews},
  timestamp            = {2020-02-25 22:25},
}

@Article{Vaucher-Medvedev-2016,
  author               = {Vaucher, Benoit and Medvedev, Alexey},
  date                 = {2015-02},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Efficient Integration of Risk Premia Exposures into Equity Portfolios},
  url                  = {https://ssrn.com/abstract=2558062},
  abstract             = {We present a stock selection methodology that maximizes the expected returns of equity portfolios by efficiently managing their exposures to a given ensemble of risk premia, also known as factors. Our approach is mathematically grounded, robust in its design, and applicable in practice. It addresses several issues specific to factor-investing, such as cross-sectional interactions between factors, the mismatch between the factors performance cycles and typical rebalancing periods, or the mitigation of interactions between the capital allocation schemes and factor exposures.},
  citeulike-article-id = {14134637},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2558062},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2836850code1625414.pdf?abstractid=2558062 and mirid=1},
  day                  = {1},
  groups               = {RiskPremia_Equity},
  owner                = {cristi},
  posted-at            = {2016-09-11 15:33:20},
  timestamp            = {2020-02-25 22:25},
}

@Article{Vaucher-Medvedev-2017,
  author               = {Vaucher, B. and Medvedev, A.},
  date                 = {2017},
  journaltitle         = {Journal of Asset Management},
  title                = {Efficient integration of risk premia exposures into equity portfolios},
  doi                  = {10.1057/s41260-017-0052-9},
  pages                = {1--9},
  abstract             = {We present a stock selection methodology that maximizes the expected returns of equity portfolios by efficiently managing their exposures to a given ensemble of risk premia, also known as factors. Our approach is mathematically grounded, robust in its design, and applicable in practice. It addresses several issues specific to factor investing, such as cross-sectional interactions between factors, the mismatch between the factors performance cycles and typical rebalancing periods, or the mitigation of interactions between the capital allocation schemes and factor exposures.},
  citeulike-article-id = {14381618},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/s41260-017-0052-9},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/s41260-017-0052-9},
  groups               = {RiskPremia_Equity},
  posted-at            = {2017-06-23 15:29:17},
  publisher            = {Palgrave Macmillan UK},
  timestamp            = {2020-02-25 22:25},
}

@Article{Zanella-2017,
  author               = {Zanella, Nicola},
  date                 = {2017},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Dividend-Price Ratio and Interest Rate Movements:Explaining the Equity Risk Premium},
  doi                  = {10.3905/jwm.2017.19.4.061},
  number               = {4},
  pages                = {61--71},
  volume               = {19},
  abstract             = {This article shows that, over time and across many countries, only a subset of long-term interest rate fall years is responsible for the high equity premium realized over the following periods. These are the years when the dividend-price ratio increases. This observed price behavior could be due to a tendency for prospective equity risk premium to increase when the interest rate declines. The reverse is true: Only a subset of interest rate rise years is responsible for the low or negative subsequent equity premium. These are the years when the dividend-price ratio decreases and the expected equity risk premium falls. This equity premium predictability is present at the international level over the 1971-2014 period, and contrary to previous studies, it does not appear to be only a recessionary phenomenon.},
  citeulike-article-id = {14270111},
  citeulike-linkout-0  = {http://www.iijournals.com/doi/abs/10.3905/jwm.2017.19.4.061},
  groups               = {RiskPremia_Equity},
  posted-at            = {2017-02-01 21:38:57},
  timestamp            = {2020-02-25 22:25},
}

@Article{Pericoli-2012,
  author               = {Pericoli, Marcello},
  date                 = {2012-02},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Expected Inflation and Inflation Risk Premium in the Euro Area and in the United States},
  url                  = {https://ssrn.com/abstract=2012460},
  abstract             = {This paper uses the celebrated no-arbitrage affine Gaussian term structure model applied to index-linked and standard government bonds to derive expected inflation rates and inflation risk premia, in the euro area and in the US. Maximum likelihood estimates show that the model describes the evolution of the nominal and real term structures by using three latent factors which can be interpreted as two real factors and one inflation factor. These provide important information on expected inflation and inflation risk premia.

The results highlight some striking differences between the euro area and the US. In the US, forward inflation risk premia become sizable around the start of the late-2000s financial crisis and considerably increase just before the adoption of the first unconventional monetary policy measures in March 2009. By contrast, in the euro area forward inflation risk premia remain unchanged even after the adoption of the unconventional monetary policy measures following the most acute phases of the financial crisis, in October 2008 and in May 2010. However, long-term inflation expectations have been well anchored over the past years.},
  citeulike-article-id = {13940615},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2012460},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2012460code606534.pdf?abstractid=2012460 and mirid=1},
  day                  = {29},
  groups               = {RiskPremia_FixedIncome, RiskPremia_Other, Risk_Inflation},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2012460},
  owner                = {zkgst0c},
  posted-at            = {2016-02-25 21:58:52},
  timestamp            = {2020-02-25 22:26},
}

@Article{Qin-et-al-2016,
  author               = {Qin, Likuan and Linetsky, Vadim and Nie, Yutian},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Long Forward Probabilities, Recovery and the Term Structure of Bond Risk Premiums},
  doi                  = {10.2139/ssrn.2721366},
  issn                 = {1556-5068},
  abstract             = {We show that the martingale component in the long-term factorization of the stochastic discount factor due to Alvarez and Jermann (2005) and Hansen and Scheinkman (2009) is highly volatile, produces a downward-sloping term structure of bond Sharpe ratios, and implies that the long bond is far from growth optimality. In contrast, the long forward probabilities forecast an upward sloping term structure of bond Sharpe ratios that starts from zero for short-term bonds and implies that the long bond is growth optimal. Thus, transition independence and degeneracy of the martingale component are implausible assumptions in the bond market.},
  citeulike-article-id = {14025427},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2721366},
  groups               = {RiskPremia_FixedIncome, FrcstQWIM_MedLngTerm, FrcstQWIM_Bond},
  owner                = {cristi},
  posted-at            = {2016-05-01 13:20:50},
  timestamp            = {2020-02-25 22:26},
}

@Article{Qin-et-al-2018,
  author         = {Qin, Likuan and Linetsky, Vadim and Nie, Yutian},
  date           = {2018-04-06},
  journaltitle   = {The Review of Financial Studies},
  title          = {Long forward probabilities, recovery, and the term structure of bond risk premiums},
  doi            = {10.1093/rfs/hhy042},
  issn           = {0893-9454},
  abstract       = {This paper examines the assumption of transition independence of the stochastic discount factor (SDF) in the bond market. This assumption underlies the recovery result of Ross 2015. Following the methodology of Alvarez and Jermann 2005 and Hansen and Scheinkman 2009, we estimate the martingale component in the long-term factorization of the SDF using U.S. Treasury data. The empirically estimated martingale component is highly volatile and produces a downward-sloping term structure of bond Sharpe ratios. In contrast, the transition independence assumption implies a degenerate martingale component and an upward-sloping term structure of bond Sharpe ratios. Thus, transition independence is inconsistent with our empirical results.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_FixedIncome, Implied_Fwd_Distrib},
  timestamp      = {2020-02-25 22:26},
}

@Article{Zhu-2015a,
  author               = {Zhu, Xiaoneng},
  date                 = {2015-03},
  journaltitle         = {Journal of International Money and Finance},
  title                = {Out-of-sample bond risk premium predictions: A global common factor},
  doi                  = {10.1016/j.jimonfin.2014.11.004},
  issn                 = {0261-5606},
  pages                = {155--173},
  volume               = {51},
  abstract             = {A global Cochrane Piazzesi factor predicts international bond risk premia. The global factor delivers systematic economic value. The global factor is related to international economic activity. This paper investigates the out-of-sample predictability of international bond risk premia. We endogenously construct a global common Cochrane and Piazzesi (2005) factor. We find that the global factor strongly predicts international bond risk premia and delivers economically significant gains relative to the historical average. The forecasting power of the global factor is above and beyond the predictive power contained in country-specific factors. As predicted by economic theories, bond return forecasts appear countercyclical. We also find that the global factor is related to international economic activity.},
  citeulike-article-id = {13990133},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jimonfin.2014.11.004},
  groups               = {RiskPremia_FixedIncome, RiskPremia_Forecast, Predictability_FinInfo, FrcstQWIM_Bond, FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:51:46},
  timestamp            = {2020-02-25 22:26},
}

@Article{Prokopczuk-Simen-2013,
  author               = {Prokopczuk, Marcel and Simen, Chardin Wese},
  date                 = {2013-01},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Variance Risk Premia in Commodity Markets},
  url                  = {https://ssrn.com/abstract=2195691},
  abstract             = {We use a large panel of commodity option prices to study the market price of variance risk. We construct synthetic variance swaps and find significantly negative variance risk premia in nearly all commodity markets. An equally-weighted portfolio of short commodity variance swaps earns an annualized Sharpe Ratio of around 40 percent. We document increasing comovements across bonds, commodities and equity variance swap returns, suggesting that the variance swap markets are increasingly integrated. Finally, we show that commodity variance risk premia are distinct from price risk premia, indicating that variance risk is unspanned by commodity futures.},
  citeulike-article-id = {14025460},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2195691},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2421182code1776557.pdf?abstractid=2195691 and mirid=1},
  day                  = {4},
  groups               = {RiskPremia_Other, Invest_Cmdty},
  owner                = {cristi},
  posted-at            = {2016-05-01 14:46:56},
  timestamp            = {2020-02-25 22:26},
}

@Article{Pyun-2016,
  author               = {Pyun, Sungjune},
  date                 = {2016-08},
  journaltitle         = {SSRN Electronic Journal},
  title                = {What the Variance Risk Premium Tells Us about the Expected Market Returns},
  url                  = {https://ssrn.com/abstract=2641638},
  abstract             = {Bollerslev, Tauchen, and Zhou (2009) find that the variance risk premium (VRP) performs well in predicting short-term market returns. This paper shows the predictive relation is strongly related to the negative contemporaneous relation between returns and variance innovations, known as the leverage effect, in several ways. First, the predictive beta depends on how sensitive market returns react to unexpected changes in variance. These two betas are extremely close so that forecasts can be made accurately even when the predictive relation changes over time. In fact, when the contemporaneous variance beta is used as a slope for the VRP, one-month returns can be predicted out of sample in a statistically and economically significant manner. Second, the predictive power is determined by the correlation between returns and variance innovations. Moreover, this result is extendable to return predictions of other asset classes. In the cross-section of 21 currency returns, the VRP of the equity index can predict returns that highly correlates with changes in market variance. The predictive beta is connected to how much currency returns react to changes in variance.},
  citeulike-article-id = {14070608},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2641638},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2786774code2432994.pdf?abstractid=2641638 and mirid=1},
  day                  = {20},
  groups               = {RiskPremia_Other, RiskPremia_Forecast, FrcstQWIM_ShortTerm, FrcstQWIM_Equity},
  owner                = {zkgst0c},
  posted-at            = {2016-06-16 23:35:29},
  timestamp            = {2020-02-25 22:26},
}

@MastersThesis{Sandqvist-Bystrom-2014,
  author               = {Sandqvist, Joakim and Bystrom, Erik},
  date                 = {2014},
  institution          = {KTH, School of Industrial Engineering and Management},
  title                = {A Framework For Analysing Investable Risk Premia Strategies},
  abstract             = {The focus of this study is to map, classify and analyse how different risk premia strategies that are fully implementable, perform and are affected by different economic environments. The results are of interest for practitioners who currently invest in or are thinking about investing in risk premia strategies. The study also makes a theoretical contribution since there currently is a lack of publicised work on this subject.

A combination of the statistical methods cluster tree, spanning tree and principal component analysis are used to first categorise the investigated risk premia strategies into different clusters based on their correlation characteristics and secondly to find the strategies' most important return drivers. Lastly, an analysis of how the clusters of strategies perform in different macroeconomic environments, here represented by inflation and growth, is conducted.

The results show that the three most important drivers for the investigated risk premia strategies are a crisis factor, an equity directional factor and an interest rate factor. These three components explained about 18 percent, 14 percent and 10 percent of the variation in the data, respectively.

The results also show that all four clusters, despite containing different types of risk premia strategies, experienced positive total returns during all macroeconomic phases sampled in this study. These results can be seen as indicative of a lower macroeconomic sensitivity among the risk premia strategies and more of an "alpha-like"behaviour.},
  citeulike-article-id = {14314306},
  groups               = {RiskPremia_Other},
  posted-at            = {2017-03-20 03:39:39},
  timestamp            = {2020-02-25 22:26},
}

@Article{Sarno-2012,
  author       = {Sarno, Lucio and Schneider, Paul and Wagner, Christian},
  date         = {2012},
  journaltitle = {Journal of Financial Economics},
  title        = {Properties of foreign exchange risk premiums},
  pages        = {279--310},
  volume       = {105},
  abstract     = {We study the properties of foreign exchange risk premiums that can explain the forward bias puzzle, defined as the tendency of high-interest rate currencies to appreciate rather than depreciate. These risk premiums arise endogenously from the no-arbitrage condition relating the term structure of interest rates and exchange rates. Estimating affine (multi-currency) term structure models reveals a noticeable tradeoff between matching depreciation rates and accuracy in pricing bonds. Risk premiums implied by our global affine model generate unbiased predictions for currency excess returns and are closely related to global risk aversion, the business cycle, and traditional exchange rate fundamentals.},
  groups       = {RiskPremia_Other},
  owner        = {u197139},
  timestamp    = {2020-02-25 22:26},
}

@Article{Schneider-2015,
  author               = {Schneider, Paul},
  date                 = {2015-06},
  journaltitle         = {Journal of Financial Economics},
  title                = {Generalized risk premia},
  doi                  = {10.1016/j.jfineco.2015.03.003},
  issn                 = {0304-405X},
  number               = {3},
  pages                = {487--504},
  volume               = {116},
  abstract             = {This paper develops an optimal trading strategy explicitly linked to an agent s preferences and assessment of the distribution of asset returns. The price of this strategy is a portfolio of implied moments, and its expected excess returns naturally accommodate compensation for higher-order moment risk. Variance risk and the equity premium approximate it to first order and it nests cross-sectional asset pricing models such as the linear Capital Asset Pricing Model (CAPM). An empirical study in the US index market compares the investment behavior of an agent with recursive long-run risk preferences to one who merely uses an identically independently distributed time series model and takes market prices as given. The two agents exhibit very similar behavior during crises and can be distinguished mostly during calm periods.},
  citeulike-article-id = {13993195},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jfineco.2015.03.003},
  groups               = {RiskPremia_Other},
  owner                = {cristi},
  posted-at            = {2016-04-02 08:44:40},
  timestamp            = {2020-02-25 22:26},
}

@Article{Triantafyllou-et-al-2015,
  author               = {Triantafyllou, Athanasios and Dotsis, George and Sarris, Alexandros H.},
  date                 = {2015-06},
  journaltitle         = {Journal of Agricultural Economics},
  title                = {Volatility Forecasting and Time-varying Variance Risk Premiums in Grains Commodity Markets},
  doi                  = {10.1111/1477-9552.12101},
  number               = {2},
  pages                = {329--357},
  volume               = {66},
  abstract             = {In this paper we examine empirically the predictive power of model-free option-implied variance and skewness in wheat, maize and soybeans derivative markets. We find that option-implied risk-neutral variance outperforms historical variance as a predictor of future realised variance for these three commodities. In addition, we find that risk-neutral option-implied skewness significantly improves variance forecasting when added in the information variable set. Variance risk premia add significant predictive power when included as an additional factor for predicting future commodity returns.},
  citeulike-article-id = {14025456},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/1477-9552.12101},
  day                  = {1},
  groups               = {RiskPremia_Other, RiskPremia_Forecast},
  owner                = {cristi},
  posted-at            = {2016-05-01 14:43:34},
  timestamp            = {2020-02-25 22:26},
}

@Article{Zeng-2013,
  author               = {Zeng, Zheng},
  date                 = {2013-05},
  journaltitle         = {The Quarterly Review of Economics and Finance},
  title                = {New tips from TIPS: Identifying inflation expectations and the risk premia of break-even inflation},
  doi                  = {10.1016/j.qref.2013.02.005},
  issn                 = {1062-9769},
  number               = {2},
  pages                = {125--139},
  volume               = {53},
  abstract             = {The 2006 housing bubble raised inflation risk but the following crisis raised liquidity risk. Liquidity risk rises during economic busts and remains low during economic booms. Break-even inflations overestimate expected inflation due to liquidity risk. The model implied-inflation expectations have the stronger forecast performance than any proposed data.

This paper decomposes the break-even inflation rates derived from inflation-indexed bonds into inflation risk premia, liquidity risk premia, and inflation expectations. I estimate a common factor model with autoregressive conditionally heteroscedastic (ARCH) errors that extracts co-movements from twenty-two monthly and quarterly indicators to identify these three components.

The results indicate that the sharp declines in the 10-year and 5-year break-even inflation rates in 2009 reflect a substantial increase in liquidity risk rather than a decrease in inflation expectations. Break-even inflation rates underestimate inflation expectations over nearly the entire sample due to the liquidity risk premia carried by the inflation indexed bond yields.

Also, the model-implied inflation expectations show better forecast performance for the average annual inflation rates than raw break-even inflation rates, the Survey of Professional Forecasters, and the Surveys of Consumers inflation forecasts.},
  citeulike-article-id = {13943181},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.qref.2013.02.005},
  groups               = {RiskPremia_Other, RiskPremia_Forecast, FrcstQWIM_Bond, Risk_Inflation, FrcstQWIM_Inflation},
  owner                = {cristi},
  posted-at            = {2016-02-28 03:06:57},
  timestamp            = {2020-02-25 22:26},
}

@Article{Pinter-2018,
  author         = {Pinter, Gabor},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Macroeconomic shocks and risk premia},
  doi            = {10.2139/ssrn.3201780},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3201780},
  abstract       = {This paper integrates models of empirical asset pricing with structural vector autoregressions (VAR) to explore the macroeconomic forces behind the cross-sectional and time-series variation in expected excess returns. First, I use an unconditional asset pricing framework to construct an orthogonal shock in a macroeconomic VAR that best explains the cross-sectional variation in expected returns. The obtained lambda-shock closely resembles identified monetary policy surprises and does not explain the recent US recessions. Second, I integrate return-forecasting methods to construct a second shock in the VAR, which best explains time-variation in expected returns. The obtained gamma-shock turns out to be virtually orthogonal to the lambda-shock, closely resembles demand-type financial shocks identified by macroeconomists, and explains most US recessions. I find that the lambda-shock and the gamma-shock jointly explain up to 80\% of aggregate consumption fluctuations in the US.},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_Forecast},
  timestamp      = {2020-02-25 22:27},
}

@Article{Prokopczuk-Simen-2014,
  author               = {Prokopczuk, Marcel and Simen, Chardin Wese},
  date                 = {2014-03},
  journaltitle         = {Journal of Banking and Finance},
  title                = {The importance of the volatility risk premium for volatility forecasting},
  doi                  = {10.1016/j.jbankfin.2013.12.002},
  issn                 = {0378-4266},
  pages                = {303--320},
  volume               = {40},
  abstract             = {In this paper, we study the role of the volatility risk premium for the forecasting performance of implied volatility. We introduce a non-parametric and parsimonious approach to adjust the model-free implied volatility for the volatility risk premium and implement this methodology using more than 20 years of options and futures data on three major energy markets. Using regression models and statistical loss functions, we find compelling evidence to suggest that the risk premium adjusted implied volatility significantly outperforms other models, including its unadjusted counterpart. Our main finding holds for different choices of volatility estimators and competing time-series models, underlying the robustness of our results.},
  citeulike-article-id = {14025227},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbankfin.2013.12.002},
  groups               = {RiskPremia_Forecast},
  owner                = {cristi},
  posted-at            = {2016-04-30 19:07:31},
  timestamp            = {2020-02-25 22:27},
}

@Article{Sarno-et-al-2016,
  author               = {Sarno, Lucio and Schneider, Paul and Wagner, Christian},
  date                 = {2016-06},
  journaltitle         = {Journal of Empirical Finance},
  title                = {The economic value of predicting bond risk premia},
  doi                  = {10.1016/j.jempfin.2016.02.001},
  issn                 = {0927-5398},
  pages                = {247--267},
  volume               = {37},
  abstract             = {Does predictability of bond risk premia translate into economic gains for investors? Novel estimation strategy for affine models to jointly fit yields and excess returns Models predict bond risk premia with high statistical accuracy, but: Models cannot beat the expectations hypothesis (EH) in terms of economic value. Given this contrast (statistical vs.economic), the EH remains a useful benchmark. This paper studies whether the evident statistical predictability of bond risk premia translates into economic gains for investors. We propose a novel estimation strategy for affine term structure models that jointly fits yields and bond excess returns, thereby capturing predictive information otherwise hidden to standard estimations. The model predicts excess returns with high regression R2s and high forecast accuracy but cannot outperform the expectations hypothesis out-of-sample in terms of economic value, showing a general contrast between statistical and economic metrics of forecast evaluation. More specifically, the model mostly generates positive (negative) economic value during times of high (low) macroeconomic uncertainty. Overall, the expectations hypothesis remains a useful benchmark for investment decisions in bond markets, especially in low uncertainty states.},
  citeulike-article-id = {14166994},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jempfin.2016.02.001},
  groups               = {RiskPremia_Forecast, FrcstQWIM_Bond},
  owner                = {cristi},
  posted-at            = {2016-10-17 18:40:24},
  timestamp            = {2020-02-25 22:27},
}

@InCollection{Roncalli-2017a,
  author               = {Roncalli, Thierry},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {Alternative Risk Premia: What Do We Know?},
  doi                  = {10.1016/b978-1-78548-201-4.50010-6},
  isbn                 = {9781785482014},
  pages                = {227--264},
  publisher            = {Elsevier},
  abstract             = {The concept of alternative risk premia (ARP) is an extension of the factor investing approach. Factor investing consists of building long-only equity portfolios which are directly exposed to common risk factors such as size, value or momentum. ARP designate non-traditional risk premia other than a long exposure to equities and bonds. They may involve equities, rates, credit, currencies or commodities and correspond to long-short portfolios. However, contrary to traditional risk premia, it is more difficult to define ARP in terms of which risk premia really matter. In fact, the term "alternative risk premia" encompasses two different types of systematic risk factor: skewness risk premia and market anomalies. For example, the most frequent ARP are carry and momentum, which are, respectively, a skewness risk premium and a market anomaly. Because the returns of ARP exhibit heterogeneous patterns in terms of statistical properties, option profiles and drawdown, asset allocation is more complex than with traditional risk premia. In this context, risk diversification cannot be reduced to volatility diversification and skewness risk becomes a key component of portfolio optimization. Understanding these different concepts and how they interconnect is essential for improving multi-asset allocation.},
  citeulike-article-id = {14499086},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50010-6},
  groups               = {Invest_Risk, RiskPremia_FixedIncome, RiskPremia_Other, RiskPremia_Alt, MultiFactor_Invest},
  posted-at            = {2017-12-08 00:41:21},
  timestamp            = {2020-02-25 22:27},
}

@TechReport{Teiletche-et-al-2017,
  author               = {Teiletche, Jeroem and Blin, Oliver and Ielpo, Florian and Lee, Joan},
  date                 = {2017},
  institution          = {Unigestion},
  title                = {A macro risk-based approach to alternative risk premia allocation},
  abstract             = {Alternative risk premia are encountering growing interest from investors. They mimic strategies formerly available through investment in hedge fund vehicles but with more favourable liquidity and cost characteristics. In this paper, we investigate the question of the allocation across a range of cross-asset alternative risk premia. For this, we design an active macro risk-based framework that aims to exploit varying behaviour in different macro regimes. We then build long-term strategic portfolios across economic regimes, which we dynamically tilt based on point-in-time signals related to regimes nowcasting and current carry. Finally, we perform backtests of the allocation strategy.},
  citeulike-article-id = {14388574},
  groups               = {Regime based investing, RiskPremia_Alt, RiskPremia_Regime},
  posted-at            = {2017-07-05 21:58:19},
  timestamp            = {2020-02-25 22:27},
}

@Article{Vatanen-Suhonen-2017,
  author               = {Vatanen, Kari and Suhonen, Antti},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {A Framework for Risk Premia Investing},
  url                  = {https://ssrn.com/abstract=3091653},
  abstract             = {We propose a new framework for alternative risk premia investing to facilitate the construction of balanced portfolios of commonly known strategies across asset classes. The categories of the framework, fundamental, behavioral, and structural premia, describe the nature and the robustness of the premia within the category. Each of the categories is further divided into a defensive and offensive compartment depending on the risk characteristics of the premia.},
  citeulike-article-id = {14509440},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3091653},
  groups               = {Invest_Risk, RiskPremia_Alt},
  posted-at            = {2017-12-28 10:10:20},
  timestamp            = {2020-02-25 22:27},
}

@Article{Pastor-Veronesi-2009,
  author               = {Pastor, Lubos and Veronesi, Pietro},
  date                 = {2009},
  journaltitle         = {Annual Review of Financial Economics},
  title                = {Learning in Financial Markets},
  doi                  = {10.1146/annurev.financial.050808.114428},
  number               = {1},
  pages                = {361--381},
  volume               = {1},
  abstract             = {We survey the recent literature on learning in financial markets. Our main theme is that many financial market phenomena that appear puzzling at first sight are easier to understand once we recognize that parameters in financial models are uncertain and subject to learning. We discuss phenomena related to the volatility and predictability of asset returns, stock price bubbles, portfolio choice, mutual fund flows, trading volume, and firm profitability, among others.},
  citeulike-article-id = {6389089},
  citeulike-linkout-0  = {http://www.annualreviews.org/doi/abs/10.1146/annurev.financial.050808.114428},
  citeulike-linkout-1  = {http://dx.doi.org/10.1146/annurev.financial.050808.114428},
  groups               = {Machine learning and investment strategies, ML_AssetPricing},
  owner                = {cristi},
  posted-at            = {2016-03-26 15:10:14},
  timestamp            = {2020-02-25 23:12},
}

@Thesis{Porsani-2018,
  author         = {Porsani, Rafael Amaral},
  date           = {2018},
  institution    = {UCLA},
  title          = {Machine Learning and Asset Pricing Models},
  type           = {phdthesis},
  url            = {https://escholarship.org/uc/item/124940r0},
  urldate        = {2019-04-22},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:12},
}

@Article{Pyo-Lee-2018,
  author         = {Pyo, Sujin and Lee, Jaewook},
  date           = {2018-10},
  journaltitle   = {Pacific-Basin Finance Journal},
  title          = {Exploiting the low-risk anomaly using machine learning to enhance the Black-Litterman framework: Evidence from South Korea},
  doi            = {10.1016/j.pacfin.2018.06.002},
  issn           = {0927-538X},
  pages          = {1--12},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S0927538X18301239}},
  urldate        = {2019-09-01},
  volume         = {51},
  abstract       = {Abstract Many studies have revealed that global financial markets are experiencing low-risk anomalies. In the Korean market, for example, even the portfolios of high-risk stocks recorded a loss of about 70\% between 2000 and 2016. In this study, we construct a low-risk portfolio that responds to low-risk anomalies in the Korean market using the Black-Litterman framework. We use three machine-learning predictive and traditional time-series models to predict the volatility of assets listed in the Korean Stock Price Index 200 (KOSPI 200) and select the best-performing one. Then, we use the model to classify assets into high- and low-risk groups and create a Black-Litterman portfolio that reflects the investor's view where low-risk stocks outperform high-risk stocks. The experiment shows that reflecting the low-risk view in the market equilibrium portfolio improves profitability and that this view dominates the market portfolio.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, ML_InvestSelect},
  timestamp      = {2020-02-25 23:12},
}

@PhdThesis{Ren-2016,
  author               = {Ren, Dongmeng},
  date                 = {2016},
  institution          = {University of Guelph},
  title                = {Essays in Asset Pricing and Financial Econometrics},
  url                  = {https://atrium.lib.uoguelph.ca/xmlui/handle/10214/9614},
  abstract             = {In the first chapter, we compare the finite sample power of short and long-horizon tests in nonlinear predictive regression models of regime switching between bull and bear markets, allowing for time varying transition probabilities. As a point of reference, we also provide a similar comparison in a linear predictive regression model without regime switching. Overall, our results do not support the contention of higher power in longer horizon tests in either the linear or nonlinear regime switching models. Nonetheless, it is possible that other plausible nonlinear models provide stronger justification for long-horizon tests. Using finite sample simulation methods, we assess the power of long-horizon predictive tests and compare them to their short-run counterparts, when the true underlying model contains financial asset bubbles. Our results indicate that long-run predictive test using valuation predictors -- specifically the dividend price ratio-- do pick up the return predictability inherent in the asset bubbles. However, after size-adjustment, the long-run predictive framework has a small advantage over its short-run counterpart when the predictor is highly persistent and provides a larger, yet still modest power improvement when the predictor is moderately persistent. The third chapter proposes a simple Bayesian learning framework to assess leverage ratios in the presence of parameter uncertainty about mean log cash flow. In particular it can explain why firm's leverage ratios have been observed to increase with firm age. Market values are increasing in uncertainty about mean cash flow and leverage ratios are decreasing with market values. Over the life period of firm, the managers and investors rationally learn from realized cash flows. Due to the convex relationship between cash flow and firm value, ceteris paribus, this results in a decrease in market value and an increase in the leverage ratio. Firm level panel data provides empirical evidence consistent with the model predictions after correcting for the endogeneity of the book to market and profitability control variates. The empirical results suggest that the firm leverage ratio increases over firm age due to learning.},
  citeulike-article-id = {14516032},
  groups               = {ML_AssetPricing},
  posted-at            = {2018-01-12 22:58:31},
  timestamp            = {2020-02-25 23:12},
}

@Article{Sornette-et-al-2016,
  author               = {Sornette, Didier and Andraszewicz, Sandra and Murphy, Ryan O. and Rindler, Philipp B. and Sanadgol, Dorsa},
  date                 = {2016-02},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Resolving Persistent Uncertainty by Self-Organized Consensus to Mitigate Market Bubbles},
  url                  = {https://ssrn.com/abstract=2731748},
  abstract             = {We propose a new paradigm to study coordination in complex social systems, such as financial markets, that accounts for fundamental uncertainty. This new context has features from prediction markets that have been shown previously to mitigate price bubbles in classical asset market experiments. Our setup is more realistic as it offers multiple securities that are continuously traded over days and, importantly, there is no true underlying price. Nonetheless, the market is designed such that its rationality can be evaluated. Quick consensus emerges early yielding pronounced market bubbles. The overpricing diminishes over time, indicating learning, but does not disappear completely. Traders' price estimates become progressively more independent via a collective realization of communal ignorance, pushing the market much closer to rationality, with forecasts that are close to the realized outcomes.},
  citeulike-article-id = {13980863},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2731748},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2731748code623849.pdf?abstractid=2731748 and mirid=1},
  day                  = {14},
  groups               = {Bubble_Crash, ML_AssetPricing},
  owner                = {cristi},
  posted-at            = {2016-03-15 22:06:21},
  timestamp            = {2020-02-25 23:12},
}

@Article{Sun-2018,
  author         = {Sun, Chuanping},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Regularising the Factor Zoo with OWL},
  doi            = {10.2139/ssrn.3263420},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3263420},
  abstract       = {Hundreds of anomaly variables have been proposed, claiming explanatory power to the cross-section of average returns in equity market. Cochrane (2011) dubs this phenomenon the "factor zoo" and further argues that the characteristic related factors to explain the average returns are in disarray. This paper introduces a newly developed machine learning tool, ordered and weighted L1 norm regularisation (OWL) to "regularise" this chaotic "factor zoo". The innovation of OWL is that high correlations among explanatory variables are permitted. Highly correlated variables will be identified and grouped together, while useless/redundant variables will be shrunk off simultaneously. This is important because factor correlation prevails in high dimensionality and biases standard estimators (FM regression, LA, etc.). Monte Carlo experiments show OWL outperforms LA, adaptive LA and Elastic Net in various settings, particularly when factors are highly correlated. Empirical evidence suggests that 'liquidity' related factors are primary to drive asset prices. Following Freyberge et al. (2017), out-of-sample Sharpe ratio of hedge portfolio, formed using OWL selected factors as predictors in the past two decades, is around 3.5 (annualised) considering all stocks, and around 2.3 when excluding micro stocks.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:12},
}

@Article{Tran-et-al-2017b,
  author               = {Tran, Dat T. and Magris, Martin and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
  date                 = {2017-10},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Tensor Representation in High-Frequency Financial Data for Price Change Prediction},
  eprint               = {1709.01268},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1709.01268},
  abstract             = {Nowadays, with the availability of massive amount of trade data collected, the dynamics of the financial markets pose both a challenge and an opportunity for high frequency traders. In order to take advantage of the rapid, subtle movement of assets in High Frequency Trading (HFT), an automatic algorithm to analyze and detect patterns of price change based on transaction records must be available. The multichannel, time-series representation of financial data naturally suggests tensor-based learning algorithms. In this work, we investigate the effectiveness of two multilinear methods for the mid-price prediction problem against other existing methods. The experiments in a large scale dataset which contains more than 4 millions limit orders show that by utilizing tensor representation, multilinear models outperform vector-based approaches and other competing ones.},
  citeulike-article-id = {14447822},
  citeulike-linkout-0  = {http://arxiv.org/abs/1709.01268},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1709.01268},
  day                  = {2},
  groups               = {ML_AssetPricing},
  posted-at            = {2017-10-09 12:54:11},
  timestamp            = {2020-02-25 23:12},
}

@Article{Vo-Maurer-2013,
  author               = {Vo, Huy T. and Maurer, Raimond},
  date                 = {2013-10},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Dynamic Asset Allocation under Regime Switching, Predictability and Parameter Uncertainty},
  url                  = {https://ssrn.com/abstract=2165029},
  abstract             = {This paper solves the dynamic asset allocation problem under stock return predictability based on the dividend price ratio with regime shifts and parameter uncertainty in a fully Bayesian framework. Intertemporal hedging demands are simultaneously induced by predictability, regime shifts, parameter uncertainty, and learning about the regimes. Optimal policies display non-monotonic horizon effects whereby regime shifts tend to induce negative hedge demands in the short-run, while predictability induces positive hedge demands in the long-run. The economic costs of ignoring regime switching and predictability are high even in the light of regime and parameter uncertainty.},
  citeulike-article-id = {13922449},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2165029},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2239386code495543.pdf?abstractid=2165029 and mirid=1},
  day                  = {22},
  groups               = {DAA, ML_AssetPricing},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 11:08:54},
  timestamp            = {2020-02-25 23:12},
}

@Article{Weigand-2019,
  author         = {Weigand, Alois},
  date           = {2019-02-26},
  journaltitle   = {Financial Markets and Portfolio Management},
  title          = {Machine learning in empirical asset pricing},
  doi            = {10.1007/s11408-019-00326-3},
  issn           = {1934-4554},
  urldate        = {2019-03-03},
  abstract       = {The tremendous speedup in computing in recent years, the low data storage costs of today, the availability of  data as well as the broad range of free open-source software, have created a renaissance in the application of machine learning techniques in science. However, this new wave of research is not limited to computer science or software engineering anymore. Among others, machine learning tools are now used in financial problem settings as well. Therefore, this paper mentions a specific definition of machine learning in an asset pricing context and elaborates on the usefulness of machine learning in this context. Most importantly, the literature review gives the reader a theoretical overview of the most recent academic studies in empirical asset pricing that employ machine learning techniques. Overall, the paper concludes that machine learning can offer benefits for future research. However, researchers should be critical about these methodologies as machine learning has its pitfalls and is relatively new to asset pricing.},
  day            = {26},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:12},
}

@MastersThesis{Widegren-2017,
  author               = {Widegren, Philip},
  date                 = {2017},
  institution          = {KTH Royal institute of technology},
  title                = {Deep learning-based forecasting of financial assets},
  url                  = {http://kth.diva-portal.org/smash/record.jsf?pid=diva2%3A1107427&dswid=2705},
  abstract             = {Deep learning and neural networks has recently become a powerful tool to solve complex problem due to improvements in training algorithms. Examples of successful application can be found in speech recognition and machine translation. There exist relative few finance articles were deep learning have been applied, but existing articles indicate that deep learning can be successfully applied to problems in finance.

This thesis studies forecasting of financial price movements using two types of neural networks, namely; feedforward and recurrent networks. For the feedforward neural networks we considered non-deep networks with more neurons and deep networks with fewer neurons. In addition to the comparison between feedforward and recurrent networks, a comparison between deep and non-deep networks will be made. The recurrent architecture consists of a recurrent layer mapping into a feedforward layer followed by an output layer. The networks are trained with two different feature setups, one less complex and one more complex.

The findings for non-deep vs. deep feedforward neural networks imply that there does not exist any general pattern whether deep or non-deep networks are preferable. The findings for recurrent neural networks vs. feedforward neural networks imply that recurrent neural networks do not necessarily outperform feedforward neural networks even though financial data in general are time-dependent. In some cases, adding batch normalization can improve the accuracy for the feedforward neural networks. This can be preferable instead of using more complex models, such as a recurrent neural networks. Moreover, there are significant differences in accuracies between using the two different feature setups. The highest accuracy for all networks are 52.82\%, which is significantly better than the simple benchmark.},
  citeulike-article-id = {14447830},
  groups               = {FrcstQWIM_ML, ML_AssetPricing},
  posted-at            = {2017-10-09 13:06:30},
  timestamp            = {2020-02-25 23:12},
}

@Article{Pyo-Lee-2018,
  author         = {Pyo, Sujin and Lee, Jaewook},
  date           = {2018-10},
  journaltitle   = {Pacific-Basin Finance Journal},
  title          = {Exploiting the low-risk anomaly using machine learning to enhance the Black-Litterman framework: Evidence from South Korea},
  doi            = {10.1016/j.pacfin.2018.06.002},
  issn           = {0927-538X},
  pages          = {1--12},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S0927538X18301239}},
  urldate        = {2019-09-01},
  volume         = {51},
  abstract       = {Abstract Many studies have revealed that global financial markets are experiencing low-risk anomalies. In the Korean market, for example, even the portfolios of high-risk stocks recorded a loss of about 70\% between 2000 and 2016. In this study, we construct a low-risk portfolio that responds to low-risk anomalies in the Korean market using the Black-Litterman framework. We use three machine-learning predictive and traditional time-series models to predict the volatility of assets listed in the Korean Stock Price Index 200 (KOSPI 200) and select the best-performing one. Then, we use the model to classify assets into high- and low-risk groups and create a Black-Litterman portfolio that reflects the investor's view where low-risk stocks outperform high-risk stocks. The experiment shows that reflecting the low-risk view in the market equilibrium portfolio improves profitability and that this view dominates the market portfolio.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, ML_InvestSelect},
  timestamp      = {2020-02-25 23:13},
}

@Article{Raffinot-2016b,
  author               = {Raffinot, Thomas},
  date                 = {2016-05},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Investing Through Economic Cycles with Ensemble Machine Learning Algorithms},
  url                  = {https://ssrn.com/abstract=2785583},
  abstract             = {Ensemble machine learning algorithms, referred to as random forest (Breiman (2001)) and as boosting (Schapire (1990)), are applied to quickly and accurately detect economic turning points in the United States and in the euro area. The two key features of those algorithms are their abilities to entertain a large number of predictors and to perform estimation and variable selection simultaneously. The real-time ability to nowcast economic turning points is gauged. To assess the value of the models, profit maximization measures are employed in addition to more standard criteria. When comparing predictive accuracy and profit measures, the model confidence set procedure proposed by Hansen et al. (2011) is applied to avoid data snooping. The investment strategies based on the models achieve impressive risk-adjusted returns: macroeconomists can get rich nowcasting economic turning points.},
  citeulike-article-id = {14146764},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2785583},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2785583code2270025.pdf?abstractid=2785583 and mirid=1},
  day                  = {28},
  groups               = {Machine learning and investment strategies, ML_InvestSelect},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:33:00},
  timestamp            = {2020-02-25 23:13},
}

@Article{Raffinot-2018,
  author               = {Raffinot, Thomas},
  date                 = {2017-12-22},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Hierarchical Clustering-Based Asset Allocation},
  doi                  = {10.3905/jpm.2018.44.2.089},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {89--99},
  volume               = {44},
  abstract             = {This article proposes a hierarchical clustering-based asset allocation method, which uses graph theory and machine learning techniques. Hierarchical clustering refers to the formation of a recursive clustering, suggested by the data, not defined a priori. Several hierarchical clustering methods are presented and tested. Once the assets are hierarchically clustered, the authors compute a simple and efficient capital allocation within and across clusters of assets, so that many correlated assets receive the same total allocation as a single uncorrelated one. The out-of-sample performances of hierarchical clustering-based portfolios and more traditional risk-based portfolios are evaluated across three disparate datasets, which differ in term of the number of assets and the assets' composition. To avoid data snooping, the authors assess the comparison of profit measures using the bootstrap-based model confidence set procedure. Their empirical results indicate that hierarchical clustering-based portfolios are robust and truly diversified and achieve statistically better risk-adjusted performances than commonly used portfolio optimization techniques.},
  citeulike-article-id = {14510373},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2018.44.2.089},
  day                  = {22},
  groups               = {Network_Invest, ML_Network_QWIM, PortfOptim_Network, Invest_Network, ML_InvestSelect},
  posted-at            = {2017-12-30 13:27:26},
  timestamp            = {2020-02-25 23:13},
}

@Article{Raffinot-2018a,
  author         = {Raffinot, Thomas},
  date           = {2018-08-23},
  journaltitle   = {SSRN Electronic Journal},
  title          = {The Hierarchical Equal Risk Contribution Portfolio},
  url            = {https://ssrn.com/abstract=3237540},
  abstract       = {Building upon the fundamental notion of hierarchy, the "Hierarchical Risk Parity" (HRP) and the "Hierarchical Clustering based Asset Allocation" (HCAA), the Hierarchical Equal Risk Contribution Portfolio (HERC) aims at diversifying capital allocation and risk allocation. HERC merges and enhances the machine learning approach of HCAA and the Top-Down recursive bisection of HRP. In more detail, the modified Top-Down recursive division is based on the shape of dendrogram, follows an Equal Risk Contribution allocation and is extended to downside risk measures such as conditional value at risk (CVaR) and Conditional Drawdown at Risk (CDaR). The out-of-sample performances of hierarchical clustering based portfolios are evaluated across two empirical datasets, which differ in terms of number of assets and composition of the universe (multi-assets and individual stocks). Empirical results highlight that HERC Portfolios based on downside risk measures achieve statistically better risk-adjusted performances, especially those based on the CDaR.},
  day            = {23},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Risk, ML_Network_QWIM, ML_InvestSelect},
  timestamp      = {2020-02-25 23:13},
}

@Article{Rapach-et-al-2019,
  author         = {David E. Rapach and Jack K. Strauss and Jun Tu and Guofu Zhou},
  date           = {2019-08-01},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Industry return predictability: A machine learning approach},
  doi            = {10.3905/jfds.2019.1.3.009},
  issn           = {2640-3951},
  url            = {https://jfds.pm-research.com/content/1/3/9},
  urldate        = {2019-08-30},
  abstract       = {In this article, the authors use machine learning tools to analyze industry return predictability based on the information in lagged industry returns. Controlling for post-selection inference and multiple testing, they find significant in-sample evidence of industry return predictability. Lagged returns for the financial sector and commodity- and material-producing industries exhibit widespread predictive ability, consistent with the gradual diffusion of information across economically linked industries. Out-of-sample industry return forecasts that incorporate the information in lagged industry returns are economically valuable: Controlling for systematic risk using leading multifactor models from the literature, an industry-rotation portfolio that goes long (short) industries with the highest (lowest) forecasted returns delivers an annualized alpha of over 8\%. The industry-rotation portfolio also generates substantial gains during economic downturns, including the Great Recession.},
  day            = {1},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect},
  timestamp      = {2020-02-25 23:13},
}

@Article{Rasekhschaffe-Jones-2019,
  author         = {Rasekhschaffe, Keywan and Jones, Robert},
  date           = {2019-02-08},
  journaltitle   = {Financial Analysts Journal},
  title          = {Machine Learning for Stock Selection},
  url            = {https://ssrn.com/abstract=3330946},
  urldate        = {2019-03-07},
  abstract       = {Machine learning is an increasingly important and controversial topic in quantitative finance. A lively debate persists as to whether machine learning techniques can be practical investment tools. Although machine learning algorithms can uncover subtle, contextual and non-linear relationships, overfitting poses a major challenge when trying to extract signals from noisy historical data. In this article, we describe some of the basic concepts surrounding machine leaning and provide a simple example of how investors can use machine learning techniques to forecast the cross-section of stock returns while limiting the risk of overfitting.},
  day            = {8},
  f1000-projects = {QuantInvest},
  groups         = {ML_Overfitting, FrcstQWIM_Equity, ML_InvestSelect, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:13},
}

@InCollection{Sbruzzi-et-al-2018,
  author         = {Sbruzzi, Elton F. and Leles, Michel C. R. and Nascimento, Cairo L.},
  booktitle      = {2018 Annual IEEE International Systems Conference (SysCon)},
  date           = {2018-04-23},
  title          = {Introducing learning automata to financial portfolio components selection},
  doi            = {10.1109/{SYSCON}.2018.8369522},
  isbn           = {978-1-5386-3664-0},
  pages          = {1--6},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8369522/},
  urldate        = {2019-05-27},
  abstract       = {In this paper, we introduce a novel method to select the components of a portfolio of securities. This method is based on a reinforcement learning technique known as learning automata. Several heuristic solutions for the portfolio weights selection problem have been introduced in literature. The point is that these applications assumes that portfolio components are given. The difference of our work is that we propose some heuristic in order to select the portfolio components instead of the weights. In terms of heuristic, we propose learning automata because its ability to solve complex systems such as a the optimal portfolio components. We test the use of learning automata in terms of financial indicators optimization. Our findings show that our proposed method improves the portfolio optimization performance in terms of accuracy and computational effort.},
  day            = {23},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect},
  timestamp      = {2020-02-25 23:13},
}

@Article{Song-et-al-2017a,
  author               = {Song, Qiang and Liu, Anqi and Yang, Steve Y.},
  date                 = {2017-11},
  journaltitle         = {Neurocomputing},
  title                = {Stock portfolio selection using learning-to-rank algorithms with news sentiment},
  doi                  = {10.1016/j.neucom.2017.02.097},
  issn                 = {0925-2312},
  pages                = {20--28},
  volume               = {264},
  abstract             = {In this study, we apply learning-to-rank algorithms to design trading strategies using relative performance of a group of stocks based on investors' sentiment toward these stocks. We show that learning-to-rank algorithms are effective in producing reliable rankings of the best and the worst performing stocks based on investors' sentiment. More specifically, we use the sentiment shock and trend indicators introduced in the previous studies, and we design stock selection rules of holding long positions of the top 25\% stocks and short positions of the bottom 25\% stocks according to rankings produced by learning-to-rank algorithms. We then apply two learning-to-rank algorithms, ListNet and RankNet, in stock selection processes and test long-only and long-short portfolio selection strategies using 10 years of market and news sentiment data. Through backtesting of these strategies from 2006 to 2014, we demonstrate that our portfolio strategies produce risk-adjusted returns superior to the S\&P 500 index return, the hedge fund industry average performance - HFRIEMN, and some sentiment-based approaches without learning-to-rank algorithm during the same period.},
  citeulike-article-id = {14500439},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neucom.2017.02.097},
  groups               = {Machine learning and investment strategies, ML_InvestSelect, ML_Text_QWIM},
  posted-at            = {2017-12-11 07:44:24},
  timestamp            = {2020-02-25 23:13},
}

@Article{Verheyden-et-al-2016,
  author               = {Verheyden, Tim and De Moor, Lieven and Vanpee, Rosanne},
  date                 = {2016-01},
  journaltitle         = {Investment Analysts Journal},
  title                = {Mutual fund performance: a market efficiency perspective},
  doi                  = {10.1080/10293523.2015.1125058},
  number               = {1},
  pages                = {1--15},
  volume               = {45},
  abstract             = {ABSTRACTThis study reconciles existing literature on stock market efficiency and mutual fund performance by developing a framework to test whether fund managers are able to exploit market inefficiencies. We find a positive relationship between alpha and weak-form market efficiency. Most funds are unable to outperform the market systematically, although a few are able to exploit relatively inefficient markets. Top performing funds are characterised by a better management of downside risk in times of market distress, whilst simultaneously exploiting learning effects when markets return to equilibrium. By conditioning fund performance on the state of the underlying market, we propose a conditional alpha ratio, which helps to better understand fund performance and can improve the fund selection process for investors.},
  citeulike-article-id = {14333856},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/10293523.2015.1125058},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/10293523.2015.1125058},
  day                  = {2},
  groups               = {ML_InvestSelect},
  posted-at            = {2017-04-08 04:08:21},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 23:13},
}

@Article{Wu-et-al-2018c,
  author         = {Wu, Wenbo and Chen, Jiaqi and Yang, Zhibin (Ben) and Tindall, Michael L.},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {A Cross-Sectional Machine Learning Approach for Hedge Fund Return Prediction and Fund Selection},
  doi            = {10.2139/ssrn.3238466},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3238466},
  abstract       = {A cross-sectional machine learning approach is proposed to allow hedge fund managers to select superior funds within major hedge fund style categories into the investment portfolios based on the predicted future returns. Various machine learning methods including the least absolute shrinkage and selection operator, random forest, gradient boosting, and deep neural network are applied. Utilizing the predictive power of advanced machine learning methods, a set of predictive features are derived from historical returns. The return-based information set effectively captures the underlying association between the historical information of hedge funds and their future returns; and is shown to be an effective addition to macroeconomic factors in predicting hedge fund returns. The empirical results demonstrate the remarkable advantages of the machine learning approach in the out-of-sample performance compared to the benchmarks. A close investigation into the causes for such improvement reveals that the notable gain in returns come from two sources: a paradigm shift from time series regression to cross-sectional regression and the employment of the advanced modeling techniques. Among the advanced machine learning methods, the deep neural network is overall most effective. The kurtosis of returns is shown to be most important among the return-based features and is even more predictive than some macroeconomic factors for the machine learning methods. A bootstrap analysis validates that the superior performance is not due to "pure luck". Furthermore, a series of robustness tests illustrate that the proposed machine learning approach is persistent to some extent and is robust in both bull and bear markets.},
  f1000-projects = {QuantInvest},
  groups         = {Fund_Select, ML_FeatureSelection, Hedge_Funds, Benchmark_AI, ML_InvestSelect, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:13},
}

@Article{Xiong-et-al-2018,
  author         = {Xiong, Zhuoran and Liu, Xiao-Yang and Zhong, Shan and Yang, Hongyang and Walid, Anwar},
  date           = {2018-11-19},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Practical Deep Reinforcement Learning Approach for Stock Trading},
  url            = {https://arxiv.org/abs/1811.07522},
  urldate        = {2019-03-07},
  abstract       = {Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.},
  day            = {19},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:13},
}

@Article{Xu-2018a,
  author         = {Xu, Lei},
  date           = {2018-12},
  journaltitle   = {Applied informatics},
  title          = {Machine learning and causal analyses for modeling financial and economic data},
  doi            = {10.1186/s40535-018-0058-5},
  issn           = {2196-0089},
  number         = {1},
  pages          = {11},
  urldate        = {2019-10-09},
  volume         = {5},
  abstract       = {Instead of aiming at a systematic survey, we consider further developments on several typical linear models and their mixture extensions for prediction modeling, portfolio management and market analyses. The focus is put on outlining the studies by the author research group, featured by (a) extensions of AR, ARCH and GARCH models into finite mixture or mixture-of-experts; (b) improvements of Sharpe ratio by maximizing the expected return and the upside volatility while minimizing the downside risk, with the help of a priori aided diversification; (c) developments of arbitrage pricing theory (APT) into temporal factor analysis (TFA)-based temporal APT, macroeconomics-modulated temporal APT and a general formulation for market modeling, together with applications to temporal prediction and dynamic portfolio management; (d) Bayesian Ying-Yang (BYY) harmony learning is adopted to implement these developments, featured with automatic model selection. After a brief introduction on BYY harmony learning, gradient-based algorithms and EM-like algorithms are provided for learning alternative mixture-of-experts-based AR, ARCH and GARCH models; and (e) path analysis for linear causal analyses is briefly reviewed, a recent development on -diagram is refined for cofounder discovery, and a causal potential theory is proposed. Also, further discussions are made on structural equation modeling and its relations to modulated TFA-APT and nGCH-driven M-TFA-O.},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect},
  timestamp      = {2020-02-25 23:13},
}

@Article{Yu-et-al-2019,
  author         = {Yu, Pengqian and Lee, Joon Sern and Kulyatin, Ilya and Shi, Zekun and Dasgupta, Sakyasingha},
  date           = {2019-01-25},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Model-based Deep Reinforcement Learning for Dynamic Portfolio Optimization},
  url            = {https://arxiv.org/abs/1901.08740},
  urldate        = {2019-02-28},
  abstract       = {Dynamic portfolio optimization is the process of sequentially allocating wealth to a collection of assets in some consecutive trading periods, based on investors' return-risk profile. Automating this process with machine learning remains a challenging problem. Here, we design a deep reinforcement learning (RL) architecture with an autonomous trading agent such that, investment decisions and actions are made periodically, based on a global objective, with autonomy. In particular, without relying on a purely model-free RL agent, we train our trading agent using a novel RL architecture consisting of an infused prediction module (IPM), a generative adversarial data augmentation module (DAM) and a behavior cloning module (BCM). Our model-based approach works with both on-policy or off-policy RL algorithms. We further design the back-testing and execution engine which interact with the RL agent in real time. Using historical { real} financial market data, we simulate trading with practical constraints, and demonstrate that our proposed model is robust, profitable and risk-sensitive, as compared to baseline trading strategies and model-free RL agents from prior work.},
  day            = {25},
  f1000-projects = {QuantInvest},
  groups         = {PortfOptim_Dynamic, ML_NumOptimiz, ML_InvestSelect, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:13},
}

@Article{Zhu-et-al-2014,
  author               = {Zhu, Shushang and Fan, Minjie and Li, Duan},
  date                 = {2014-11},
  journaltitle         = {Journal of Economic Dynamics and Control},
  title                = {Portfolio management with robustness in both prediction and decision: A mixture model based learning approach},
  doi                  = {10.1016/j.jedc.2014.08.015},
  issn                 = {0165-1889},
  pages                = {1--25},
  volume               = {48},
  abstract             = {We develop in this paper a novel portfolio selection framework with a feature of double robustness in both return distribution modeling and portfolio optimization. While predicting the future return distributions always represents the most compelling challenge in investment, any underlying distribution can be always well approximated by utilizing a mixture distribution, if we are able to ensure that the component list of a mixture distribution includes all possible distributions corresponding to the scenario analysis of potential market modes.

Adopting a mixture distribution enables us to

(1) reduce the problem of distribution prediction to a parameter estimation problem in which the mixture weights of a mixture distribution are estimated under a Bayesian learning scheme and the corresponding credible regions of the mixture weights are obtained as well

(2) harmonize information from different channels, such as historical data, market implied information and investors subjective views.

We further formulate a robust mean-CVaR portfolio selection problem to deal with the inherent uncertainty in predicting the future return distributions. By employing the duality theory, we show that the robust portfolio selection problem via learning with a mixture model can be reformulated as a linear program or a second-order cone program, which can be effectively solved in polynomial time.

We present the results of simulation analyses and primary empirical tests to illustrate a significance of the proposed approach and demonstrate its pros and cons.},
  citeulike-article-id = {13936730},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jedc.2014.08.015},
  groups               = {Scenario generation, ML_InvestSelect},
  owner                = {cristi},
  posted-at            = {2016-03-12 10:38:54},
  timestamp            = {2020-02-25 23:13},
}

@Article{Alcazar-et-al-2019,
  author         = {Alcazar, Javier and Leyton-Ortega, Vicente and Perdomo-Ortiz, Alejandro},
  date           = {2019-08-28},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Classical versus Quantum Models in Machine Learning: Insights from a Finance Application},
  url            = {https://arxiv.org/abs/1908.10778},
  urldate        = {2019-09-06},
  abstract       = {Although several models have been proposed towards assisting machine learning (ML) tasks with quantum computers, a direct comparison of the expressive power and efficiency of classical versus quantum models for datasets originating from real-world applications is one of the key milestones towards a quantum ready era. Here, we take a first step towards addressing this challenge by performing a comparison of the widely used classical ML models known as restricted Boltzmann machines (RBMs), against a recently proposed quantum model, now known as quantum circuit Born machines (QCBMs). Both models address the same hard tasks in unsupervised generative modeling, with QCBMs exploiting the probabilistic nature of quantum mechanics and a candidate for near-term quantum computers, as experimentally demonstrated in three different quantum hardware architectures to date. To address the question of the performance of the quantum model on real-world classical data sets, we construct scenarios from a probabilistic version out of the well-known portfolio optimization problem in finance, by using time-series pricing data from asset subsets of the S\&P500 stock market index. It is remarkable to find that, under the same number of resources in terms of parameters for both classical and quantum models, the quantum models seem to have superior performance on typical instances when compared with the canonical training of the RBMs. Our simulations are grounded on a hardware efficient realization of the QCBMs on ion-trap quantum computers, by using their native gate sets, and therefore readily implementable in near-term quantum devices.},
  day            = {28},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:14},
}

@Article{Andrei-et-al-2018,
  author         = {Andrei, Daniel and Hasler, Michael and Jeanneret, Alexandre},
  date           = {2018-11-16},
  journaltitle   = {The Review of Financial Studies},
  title          = {Asset Pricing with Persistence Risk},
  doi            = {10.1093/rfs/hhy121},
  issn           = {0893-9454},
  urldate        = {2019-03-05},
  abstract       = {Persistence risk is an endogenous source of risk that arises when a rational agent learns about the length of business cycles. Persistence risk is positive during recessions and negative during expansions. This asymmetry, which solely results from learning about persistence, causes expected returns, return volatility, and the price of risk to rise during recessions. Persistence risk predicts future excess returns, particularly at 3- to 7-year horizons. Its predictability is strongest around business-cycle peaks and troughs. We confirm the model predictions in the data and provide evidence that persistence risk is priced in financial markets.Received October 13, 2017; editorial decision September 19, 2018 by Editor Stijn Van Nieuwerburgh. Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
  day            = {16},
  f1000-projects = {QuantInvest},
  groups         = {RiskRet_BusCycle, ML_AssetPricing},
  timestamp      = {2020-02-25 23:14},
}

@Article{Aubry-et-al-2019,
  author         = {Aubry, Mathieu and Kraeussl, Roman and Manso, Gustavo and Spaenjers, Christophe},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Machines and masterpieces: predicting prices in the art auction market},
  doi            = {10.2139/ssrn.3347175},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3347175},
  abstract       = {We study the accuracy and usefulness of automated (i.e., machine-generated) valuations for illiquid and heterogeneous assets. We assemble a database of 1.1 million paintings that were auctioned between 2008 and 2015. We use a popular machine-learning technique networks develop a price prediction algorithm based on both non-visual and visual artwork characteristics. Our out-of-sample valuations predict auction prices dramatically better than valuations based on a standard hedonic pricing model. Moreover, they help explaining price levels and sale probabilities even after conditioning on auctioneers pre-sale estimates. Machine learning is particularly helpful for assets that are associated with higher levels of ex-ante price uncertainty. Finally, we show that it can help overcome experts systematic biases in expectations formation.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:14},
}

@Article{BaroneAdesi-et-al-2019,
  author         = {Barone-Adesi, Giovanni and Mira, Antonietta and Pisati, Matteo},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {The keys of predictability: A comprehensive study},
  doi            = {10.2139/ssrn.3356736},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3356736},
  abstract       = {The problem of market predictability can be decomposed into two parts: predictive models and predictors. At first, we show how the joint employment of model selection and machine learning models can dramatically increase our capability to forecast the equity premium out-of-sample. Secondly, we introduce batteries of powerful predictors which brings the monthly SandP500 R-square to a high level of 24\%. Finally, we prove how predictability is a generalized characteristic of U.S. equity markets. For each of the three parts, we consider potential and challenges posed by the new approaches in the asset pricing field.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:14},
}

@Article{Bloch-2015,
  author               = {Bloch, Daniel A.},
  date                 = {2015-12},
  journaltitle         = {SSRN Electronic Journal},
  title                = {A Practical Guide to Quantitative Portfolio Trading},
  url                  = {https://ssrn.com/abstract=2543802},
  abstract             = {We discuss risk, preference and valuation in classical economics, which led academics to develop a theory of market prices, resulting in the general equilibrium theories. However, in practice, the decision process does not follow that theory since the qualitative aspect coming from human decision making process is missing. Further, a large number of studies in empirical finance showed that financial assets exhibit trends or cycles, resulting in persistent inefficiencies in the market, that can be exploited. The uneven assimilation of information emphasised the multifractal nature of the capital markets, recognising complexity. New theories to explain financial markets developed, among which is a multitude of interacting agents forming a complex system characterised by a high level of uncertainty. Recently, with the increased availability of data, econophysics emerged as a mix of physical sciences and economics to get the best of both world, in view of analysing more deeply assets' predictability. For instance, data mining and machine learning methodologies provide a range of general techniques for classification, prediction, and optimisation of structured and unstructured data. Using these techniques, one can describe financial markets through degrees of freedom which may be both qualitative and quantitative in nature. In this book we detail how the growing use of quantitative methods changed finance and investment theory. The most significant benefit being the power of automation, enforcing a systematic investment approach and a structured and unified framework. We present in a chronological order the necessary steps to identify trading signals, build quantitative strategies, assess expected returns, measure and score strategies, and allocate portfolios.},
  citeulike-article-id = {13926639},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2543802},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2692604code802495.pdf?abstractid=2543802 and mirid=1},
  day                  = {31},
  groups               = {ML_Classif_QWIM, ML_BestPractices, ML_Interpretability, ML_AssetPricing, ML_InvestSelect, DeepLearning_QWIM},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2543802},
  owner                = {zkgst0c},
  posted-at            = {2016-02-06 05:26:32},
  timestamp            = {2020-02-25 23:14},
}

@Article{Borisenko-2019,
  author         = {Borisenko, Dmitry},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Dissecting momentum: we need to go deeper},
  doi            = {10.2139/ssrn.3424793},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3424793},
  abstract       = {Cross-sectional predictability of returns by past prices, or momentum, is a lasting market anomaly. Previous research reports numerous ways to measure momentum and establishes a multitude of factors predicting its performance. The emerging machine learning asset pricing literature further identifies price-based firm characteristics as major predictors of returns. I investigate predictive power of a broad set of price-based variables over various time horizons in a deep learning framework and document rich non-linear structure in impact of these variables on expected returns in the US equity market. The magnitude and sign of the impact exhibit substantial time variation and are modulated by interaction effects among the variables. The degree of non-linearity in expected returns varies over time and is highest in distressed markets. Incorporating insights from the literature on time-varying, market state-dependent momentum risks and momentum crashes helps to improve out-of-sample performance of neural network portfolios, especially with respect to the downside risk -- investment strategies built on predictions of the deep learning model actively exploit the non-linearities and interaction effects, generating high and statistically significant returns with a robust risk profile and their performance virtually uncorrelated with the established risk factors including momentum. Lastly, I make a case for adoption of automated hyperparameter optimization techniques as an important component of disciplined research in financial machine learning.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:14},
}

@Article{Byrd-2019,
  author         = {Byrd, David},
  date           = {2019-09-25},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Explaining Agent-Based Financial Market Simulation},
  url            = {https://arxiv.org/abs/1909.11650},
  urldate        = {2019-10-02},
  abstract       = {This paper is intended to explain, in simple terms, some of the mechanisms and agents common to multiagent financial market simulations. We first discuss the necessity to include an exogenous price time series ("the fundamental value") for each asset and three methods for generating that series. We then illustrate one process by which a Bayesian agent may receive limited observations of the fundamental series and estimate its current and future values. Finally, we present two such agents widely examined in the literature, the Zero Intelligence agent and the Heuristic Belief Learning agent, which implement different approaches to order placement.},
  day            = {25},
  f1000-projects = {QuantInvest},
  groups         = {ML_Explain, ML_AssetPricing},
  timestamp      = {2020-02-25 23:14},
}

@Article{Cason-Samek-2015,
  author               = {Cason, TimothyN and Samek, Anya},
  date                 = {2015},
  journaltitle         = {Journal of the Economic Science Association},
  title                = {Learning through passive participation in asset market bubbles},
  doi                  = {10.1007/s40881-015-0013-3},
  number               = {2},
  pages                = {170--181},
  volume               = {1},
  abstract             = {We report a laboratory experiment that investigates the impact of passive participation on bubble formation in asset markets with inexperienced and experienced traders. Some treatments employ pre-market training in which each participant is 'matched' with a trader from a different prior market and observes all trading details but does not directly participate in trading. We find that passive participation, similar to direct experience, significantly reduces mispricing in subsequent markets. This finding suggests that observation of prices is a key mechanism through which experience mitigates bubbles. We also vary whether transaction prices are displayed in a column of text or in a graphical display, and find that among inexperienced and once-experienced traders, markets with the tabular display result in bubbles that are greater in amplitude relative to markets with the graphical display.},
  citeulike-article-id = {14015157},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s40881-015-0013-3},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s40881-015-0013-3},
  groups               = {ML_AssetPricing},
  owner                = {cristi},
  posted-at            = {2016-04-18 18:13:40},
  publisher            = {Springer US},
  timestamp            = {2020-02-25 23:14},
}

@Article{Chakraborty-Awasthi-2018,
  author         = {Chakravorty, Gaurav and Awasthi, Ankit},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Deep learning for global tactical asset allocation},
  doi            = {10.2139/ssrn.3242432},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3242432},
  abstract       = {We show how one can use deep neural networks with macro-economic data in conjunction with price-volume data in a walk-forward setting to do tactical asset allocation. Low cost publicly traded ETFs corresponding to major asset classes (equities, fixed income, real estate) and geographies (US, Ex-US Developed, Emerging) are used as proxies for asset classes and for back-testing performance. We take dropout as a Bayesian approximation to obtain prediction uncertainty and show it often deviates significantly from other measures of uncertainty such as volatility. We propose two very different ways of portfolio construction - one based on expected returns and uncertainty and the other which obtains allocations as part of the neural network and optimizes a custom utility function such as portfolio sharpe. We also find that adding a layer of error correction helps reduce drawdown significantly during the 2008 financial crisis. Finally, we compare results to risk parity and show that the above deep learning strategies trained in totally walk-forward manner have comparable performance.},
  f1000-projects = {QuantInvest},
  groups         = {TAA, DeepLearning_QWIM, ML_AssetPricing, ML_InvestSelect},
  timestamp      = {2020-02-25 23:14},
}

@Article{Chen-et-al-2019a,
  author         = {Chen, Luyang and Pelger, Markus and Zhu, Jason},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Deep learning in asset pricing},
  doi            = {10.2139/ssrn.3350138},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3350138},
  abstract       = {We estimate a general non-linear asset pricing model with deep neural networks applied to all U.S. equity data combined with a substantial set of macroeconomic and firm-specific information. Our crucial innovation is the use of the no-arbitrage condition as part of the neural network algorithm. We estimate the stochastic discount factor (SDF or pricing kernel) that explains all asset prices from the conditional moment constraints implied by no-arbitrage. For this purpose, we combine three different deep neural network structures in a novel way: A feedforward network to capture non-linearities, a recurrent Long-Short-Term-Memory network to find a small set of economic state processes, and a generative adversarial network to identify the portfolio strategies with the most unexplained pricing information. Our model allows us to understand what are the key factors that drive asset prices, identify mis-pricing of stocks and generate the mean-variance efficient portfolio. Empirically, our approach outperforms out-of- sample all other benchmark approaches: Our optimal portfolio has an annual Sharpe Ratio of 2.1, we explain 8\% of the variation in individual stock returns and explain over 90\% of average returns for all anomaly sorted portfolios.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:14},
}

@Article{CollinDufresne-et-al-2017,
  author         = {Collin-Dufresne, Pierre and Johannes, Michael and Lochstoer, Lars A.},
  date           = {2017-02},
  journaltitle   = {Review of Financial Studies},
  title          = {Asset pricing when time is different},
  doi            = {10.1093/rfs/hhw084},
  issn           = {0893-9454},
  number         = {2},
  pages          = {505--535},
  volume         = {30},
  abstract       = {Recent evidence suggests that younger people update beliefs more in response to aggregate shocks than older people. We embed this generational learning bias in an equilibrium model where agents have recursive preferences and are uncertain about exogenous aggregate dynamics. The departure from rational expectations is statistically modest, but generates high average risk premiums varying at `generational' frequencies, a positive relation between past returns and agents' future return forecasts, and substantial and persistent over- and under-valuation. Consistent with the model, the price-dividend ratio is empirically more sensitive to macroeconomic shocks when the fraction of young in the population is higher.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:14},
}

@Article{Ban-et-al-2016,
  author               = {Ban, Gah-Yi and El Karoui, Noureddine and Lim, Andrew E. B.},
  date                 = {2016-11},
  journaltitle         = {Management Science},
  title                = {Machine Learning and Portfolio Optimization},
  doi                  = {10.1287/mnsc.2016.2644},
  issn                 = {0025-1909},
  abstract             = {The portfolio optimization model has limited impact in practice because of estimation issues when applied to real data. To address this, we adapt two machine learning methods, regularization and cross-validation, for portfolio optimization. First, we introduce performance-based regularization (PBR), where the idea is to constrain the sample variances of the estimated portfolio risk and return, which steers the solution toward one associated with less estimation error in the performance. We consider PBR for both mean-variance and mean-conditional value-at-risk (CVaR) problems. For the mean-variance problem, PBR introduces a quartic polynomial constraint, for which we make two convex approximations: one based on rank-1 approximation and another based on a convex quadratic approximation. The rank-1 approximation PBR adds a bias to the optimal allocation, and the convex quadratic approximation PBR shrinks the sample covariance matrix. For the mean-CVaR problem, the PBR model is a combinatorial optimization problem, but we prove its convex relaxation, a quadratically constrained quadratic program, is essentially tight. We show that the PBR models can be cast as robust optimization problems with novel uncertainty sets and establish asymptotic optimality of both sample average approximation (SAA) and PBR solutions and the corresponding efficient frontiers. To calibrate the right-hand sides of the PBR constraints, we develop new, performance-based k-fold cross-validation algorithms. Using these algorithms, we carry out an extensive empirical investigation of PBR against SAA, as well as L1 and L2 regularizations and the equally weighted portfolio. We find that PBR dominates all other benchmarks for two out of three Fama-French data sets.},
  citeulike-article-id = {14430611},
  citeulike-linkout-0  = {http://dx.doi.org/10.1287/mnsc.2016.2644},
  day                  = {21},
  groups               = {Machine learning and investment strategies, ML_PerfMetrics, ML_Test_OOS, ML_Test_CrossVal, ML_Validation, Optimiz_Regulariz, ML_NumOptimiz, ML_InvestSelect},
  posted-at            = {2017-09-14 21:03:53},
  timestamp            = {2020-02-25 23:15},
}

@Article{Bao-et-al-2015a,
  author               = {Bao, C. and Zhu, Z. and Langrene, N. and Lee, G.},
  date                 = {2015-02},
  journaltitle         = {IAENG Transactions on Engineering Sciences},
  title                = {Multi-period dynamic portfolio optimization through least squares learning},
  doi                  = {10.1142/9789814667364\_0003},
  pages                = {29--42},
  abstract             = {This paper describes an algorithm to solve a dynamic portfolio selection problem. The portfolio selection problem is modelled as multiple switching problem, and a simulation-based numerical method is implemented for solving the dynamic portfolio optimization problem. A recursive numerical approach based on the Least Squares Monte Carlo method is used to calculate the conditional value functions of investors for a sequence of discrete decision dates. The methodology is data driven, is not restricted to specific asset models. Importantly, intermediate transaction costs associated with portfolio rebalancing is considered in the dynamic optimisation process. Investors' risk preferences and risk management constraints are also taken into account in the current implementation. A case study is presented for a global equity portfolio invested in five equity markets, and foreign exchange risks are also included. The case study provides a numerical example of using the methodology for 8-dimensions.},
  citeulike-article-id = {14159998},
  citeulike-linkout-0  = {http://dx.doi.org/10.1142/97898146673640003},
  citeulike-linkout-1  = {http://www.worldscientific.com/doi/abs/10.1142/97898146673640003},
  day                  = {6},
  groups               = {PortfOptim_Dynamic, ML_NumOptimiz, ML_InvestSelect},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 10:41:39},
  publisher            = {WORLD SCIENTIFIC},
  timestamp            = {2020-02-25 23:15},
}

@MastersThesis{Bohn-2017,
  author               = {Bohn, Tanner A.},
  date                 = {2017},
  institution          = {University of Western Ontario},
  title                = {Improving Long Term Stock Market Prediction with Text Analysis},
  url                  = {https://ir.lib.uwo.ca/etd/4497/},
  abstract             = {The task of forecasting stock performance is well studied with clear monetary motivations for those wishing to invest. A large amount of research in the area of stock performance prediction has already been done, and multiple existing results have shown that data derived from textual sources related to the stock market can be successfully used towards forecasting. These existing approaches have mostly focused on short term forecasting, used relatively simple sentiment analysis techniques, or had little data available. In this thesis, we prepare over ten years worth of stock data and propose a solution which combines features from textual yearly and quarterly filings with fundamental factors for long term stock performance forecasting. Additionally, we develop a method of text feature extraction and apply feature selection aided by a novel evaluation function. We work with investment company Highstreet Inc. and create a set of models with our technique allowing us to compare the performance to their own models. Our results show that feature selection is able to greatly improve the validation and test performance when compared to baseline models. We also show that for 2015, our method produces models which perform comparably to Highstreet's hand-made models while requiring no expert knowledge beyond data preparation, making the model an attractive aid for constructing investment portfolios. Highstreet has decided to continue to work with us on this research, and our machine learning models can potentially be used in actual portfolio selection in the near future.},
  citeulike-article-id = {14504953},
  groups               = {ML_Forecast_QWIM, ML_PerfMetrics, ML_FeatureSelection, ML_InvestSelect, ML_Text_QWIM},
  posted-at            = {2017-12-18 23:30:26},
  timestamp            = {2020-02-25 23:15},
}

@Article{Bolster-Platt-2017,
  author         = {Bolster, Paul and Platt, Harlan},
  date           = {2017-12-31},
  journaltitle   = {The Journal of Trading},
  title          = {The Challenges of Managing a Student-Managed Fund},
  doi            = {10.3905/jot.2018.13.1.035},
  issn           = {1559-3967},
  number         = {1},
  pages          = {35--38},
  volume         = {13},
  abstract       = {This paper summarizes our experiences as faculty advisors to the 360 Huntington Fund at Northeastern University. We strongly encourage other universities to provide their students with similar experiential investment opportunities. The paper details the role of students relative to the faculty in the fund, the selection of investment choices, and various difficulties encountered notably the need to replace high quality student managers as they graduate. Our objectives in creating the fund, beyond experiential learning, was to give students an advantage in their job search activity. This has been an unqualified success with our students getting more and better jobs as a result.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect},
  timestamp      = {2020-02-25 23:15},
}

@Article{Bruder-et-al-2016,
  author               = {Bruder, Benjamin and Gaussel, Nicolas and Richard, Jean-Charles and Roncalli, Thierry},
  date                 = {2016-04},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Regularization of Portfolio Allocation},
  url                  = {https://ssrn.com/abstract=2767358},
  abstract             = {The mean-variance optimization (MVO) theory of Markowitz (1952) for portfolio selection is one of the most important methods used in quantitative finance. This portfolio allocation needs two input parameters, the vector of expected returns and the covariance matrix of asset returns. This process leads to estimation errors, which may have a large impact on portfolio weights. In this paper we review different methods which aim to stabilize the mean-variance allocation. In particular, we consider recent results from machine learning theory to obtain more robust allocation.},
  citeulike-article-id = {14020431},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2767358},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2767358code903940.pdf?abstractid=2767358 and mirid=1},
  day                  = {21},
  groups               = {ML_Regularization, ML_InvestSelect},
  owner                = {cristi},
  posted-at            = {2016-04-23 19:02:33},
  timestamp            = {2020-02-25 23:15},
}

@Article{Byrd-et-al-2019,
  author       = {David Byrd and Sourabh Bajaj and Tucker Balch},
  date         = {2019},
  journaltitle = {The Journal of Financial Data Science},
  title        = {Fund Asset Inference Using Machine Learning Methods: What's in That Portfolio?},
  url          = {https://jfds.pm-research.com/content/1/3/98},
  abstract     = {Given only the historic net asset value of a large-cap mutual fund, which members of some universe of stocks are held by the fund? Discovering an exact solution is combinatorially intractable because there are, for example, C(500, 30) or 10 to power 48 possible portfolios of 30 stocks drawn from the S\&P 500. The authors extend an existing linear clones approach and introduce a new sequential oscillating selection method to produce a computationally efficient inference. Such techniques could inform efforts to detect fund window dressing of disclosure statements or to adjust market positions in advance of major fund disclosure dates. The authors test the approach by tasking the algorithm with inferring the constituents of exchange-traded funds for which the components can be later examined. Depending on the details of the specific problem, the algorithm runs on consumer hardware in 8 to 15 seconds and identifies target portfolio constituents with an accuracy of 88.2\% to 98.6\%.},
  groups       = {ML_InvestSelect},
  timestamp    = {2020-02-25 23:15},
}

@Article{Chen-et-al-2019h,
  author         = {Chen, Jingnan and Dai, Gengling and Zhang, Ning},
  date           = {2019-03-06},
  journaltitle   = {Annals of operations research},
  title          = {An application of sparse-group lasso regularization to equity portfolio optimization and sector selection},
  doi            = {10.1007/s10479-019-03189-z},
  issn           = {0254-5330},
  pages          = {1--20},
  urldate        = {2019-09-10},
  abstract       = {In this paper, we propose a modified mean-variance portfolio selection model that incorporates the sparse-group lasso (abbreviated as SGLasso) regularization in machine learning. This new model essentially has three merits: first, it allows investors to incorporate their preference over equity sectors when constructing portfolios; second, it helps investors select sectors based on assets past performances as it encourages sparsity among sectors; third, it has stabilizing and sparsifying effect on the entire portfolio. We connect our model to a robust portfolio selection problem, and investigate effects of the SGLasso regularization on the optimal strategy both theoretically and empirically. We develop an efficient algorithm to find the optimal portfolio and prove its global convergence. We demonstrate the efficiency of the algorithm through simulated experiments under large datasets and evaluate the out-of-sample performance of our model via empirical tests across different datasets.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {ML_NumOptimiz, ML_InvestSelect},
  timestamp      = {2020-02-25 23:15},
}

@Article{Dehghanpour-Esfahanipour-2018,
  author         = {Dehghanpour, Siamak and Esfahanipour, Akbar},
  date           = {2018-02-27},
  journaltitle   = {Journal of Information and Telecommunication},
  title          = {Dynamic portfolio insurance strategy: a robust machine learning approach},
  doi            = {10.1080/24751839.2018.1431447},
  issn           = {2475-1839},
  pages          = {1--19},
  abstract       = {In this paper, we propose a robust genetic programming (RGP) model for a dynamic strategy of stock portfolio insurance. With portfolio insurance strategy, we divide the money in a risky asset and a risk-free asset. Our applied strategy is based on a constant proportion portfolio insurance strategy. For determining the amount for investing in the risky asset, a critical parameter is a constant risk multiplier that is calculated in our proposed model using RGP to reflect market dynamics. Our model includes four main steps: (1) Selecting the best stocks for constructing a portfolio using a density-based clustering strategy. (2) Enhancing the robustness of our proposed model with an application of the Adaptive Neuro-Fuzzy Inference Systems (ANFIS) for forecasting the future prices of the selected stocks. The findings show that using ANFIS, instead of a regular multi-layer artificial neural network improves the prediction accuracy and our model robustness. (3) Implementing the RGP model for calculating the risk multiplier. Risk variables are used to generate equation trees for calculating the risk multiplier. (4) Determining the optimal portfolio weights of the assets using the well-known Markowitz portfolio optimization model. Experimental results show that our proposed strategy outperforms our previous model.},
  day            = {27},
  f1000-projects = {QuantInvest},
  groups         = {ML_Network_QWIM, FrcstQWIM_ML, ML_AssetPricing, DPPI_VPPI},
  timestamp      = {2020-02-25 23:15},
}

@Article{Dixon-Halperin-2019,
  author         = {Dixon, Matthew Francis and Halperin, Igor},
  date           = {2019-09-14},
  journaltitle   = {SSRN Electronic Journal},
  title          = {The Four Horsemen of Machine Learning in Finance},
  url            = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3453564},
  urldate        = {2019-09-14},
  abstract       = {Machine Learning has been used in the finance services industry for over 40 years, yet it is only in recent years that it has become more pervasive across investment management and trading. Machine learning provides a more general framework for financial modeling than its linear parametric predecessors, generalizing archetypal modeling approaches, such as factor modeling, derivative pricing, portfolio construction, optimal hedging with model-free, data-driven approaches which are more robust to model risk and capture outliers. Yet despite their demonstrated potential, barriers to adoption have emerged - most of them artifacts of the sociology of this inter-disciplinary field. Based on discussions with several industry experts and the authors' multi-decadal experience using machine learning and traditional quantitative finance at investment banks, asset management and securities trading firms, this position article identifies the major red flags and sets out guidelines and solutions to avoid them. Examples using supervised learning and reinforcement in investment management and trading are provided to illustrate best practices.},
  day            = {14},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:15},
}

@Article{Easley-et-al-2019,
  author         = {Easley, David and de Prado, Marcos Lopez and O'Hara, Maureen and Zhang, Zhibai},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Microstructure in the machine age},
  doi            = {10.2139/ssrn.3345183},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3345183},
  abstract       = {We demonstrate how a machine learning algorithm can be applied to predict and explain modern market microstructure phenomena. We investigate the efficacy of various microstructure measures and show that they continue to provide insights into price dynamics in current complex markets. Some microstructure features with apparent high explanatory power exhibit low predictive power, and vice versa. We also find that some microstructure-based measures are useful for out-of-sample prediction of various market statistics, leading to questions about the efficiency of markets. Our results are derived using 87 of the most liquid futures contracts across all asset classes.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:15},
}

@Article{Ehling-et-al-2016,
  author               = {Ehling, Paul and Graniero, Alessandro and Heyerdahl-Larsen, Christian},
  date                 = {2014-01},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Asset Prices and Portfolio Choice with Learning from Experience},
  url                  = {https://ssrn.com/abstract=2378330},
  abstract             = {We study asset prices and portfolio choice with overlapping generations where the young disregard history to learn from own experience. Disregarding history implies less precise estimates of consumption growth, which, in equilibrium, leads the young to increase their investment in risky assets after positive returns or act as trend chasers and to lose wealth to the old. Consistent with findings from survey data, the average belief about expected returns in the economy relates negatively to future realized returns and is smoother than objective expected returns. Having especially bad experiences early on in life, cause persistent disagreement and tilt portfolio weights.},
  citeulike-article-id = {13997454},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2378330},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2747938code327263.pdf?abstractid=2378330 and mirid=1},
  day                  = {14},
  groups               = {ML_AssetPricing},
  owner                = {cristi},
  posted-at            = {2016-04-05 05:04:10},
  timestamp            = {2020-02-25 23:15},
}

@PhdThesis{Farmer-2017,
  author               = {Farmer, Leland E.},
  date                 = {2017},
  institution          = {University of California, San Diego},
  title                = {Discrete Methods for the Estimation of Nonlinear Economic Models},
  url                  = {https://escholarship.org/uc/item/1w34k104},
  abstract             = {Economists increasingly use nonlinear methods to confront their theories with data. The switch from linear to nonlinear methods is driven, in part, by increased computing power, but also by a desire to understand economic phenomena that cannot easily be captured by linear models. My research is informed by questions at the intersection of macroeconomics and finance that cannot be addressed with standard methods.

Existing methods for estimating nonlinear dynamic models are either too computationally complex to be of practical use, or rely on local approximations which fail to adequately capture the nonlinear features of interest. My research develops a new methodology for accurately estimating nonlinear dynamic models which is computationally simple and easy to apply. In my dissertation, I apply this methodology to study a model of interest rate dynamics near the zero lower bound, an asset pricing model of rare disasters, and a model of learning about cash flows in the presence of structural change.},
  citeulike-article-id = {14501371},
  groups               = {ML_AssetPricing},
  posted-at            = {2017-12-12 01:12:12},
  timestamp            = {2020-02-25 23:15},
}

@Article{Farmer-et-al-2018,
  author         = {Farmer, Leland and Schmidt, Lawrence and Timmermann, Allan},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Pockets of Predictability},
  doi            = {10.2139/ssrn.3152386},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3152386},
  urldate        = {2019-04-21},
  abstract       = {Return predictability in the U.S. stock market is local in time as short periods with significant predictability ('pockets') are interspersed with long periods with little or no evidence of return predictability. We document this empirically using a flexible non-parametric approach and explore possible explanations of this fi nding, including time-varying risk-premia. We fi nd that short-lived predictability pockets are inconsistent with a broad class of affine asset pricing models. Conversely, pockets of return predictability are more in line with a model with investors' incomplete learning about a highly persistent growth component in the underlying cash flow process which undergoes occasional regime shifts.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:15},
}

@Article{Feng-et-al-2018,
  author         = {Feng, Guanhao and Polson, Nicholas G. and Xu, Jianeng},
  date           = {2018-05-03},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep Factor Alpha},
  url            = {https://arxiv.org/abs/1805.01104},
  abstract       = {Deep Factor Alpha provides a framework for extracting nonlinear factors information to explain the time-series cross-section properties of asset returns. Sorting securities based on firm characteristics is viewed as a nonlinear activation function which can be implemented within a deep learning architecture. Multi-layer deep learners are constructed to augment traditional long-short factor models. Searching firm characteristic space over deep architectures of nonlinear transformations is compatible with the economic goal of eliminating mispricing Alphas. Joint estimation of factors and betas is achieved with stochastic gradient descent. To illustrate our methodology, we design long-short latent factors in a train-validation-testing framework of US stock market asset returns from 1975 to 2017. We perform an out-of-sample study to analyze Fama-French factors, in both the cross-section and time-series, versus their deep learning counterparts. Finally, we conclude with directions for future research.},
  day            = {3},
  f1000-projects = {QuantInvest},
  groups         = {ML_Test_OOS, ML_Test_CrossVal, ML_Validation, ML_AssetPricing, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:15},
}

@Article{Feuerriegel-Prendinger-2016,
  author         = {Feuerriegel, Stefan and Prendinger, Helmut},
  date           = {2016-10},
  journaltitle   = {Decision Support Systems},
  title          = {News-based trading strategies},
  doi            = {10.1016/j.dss.2016.06.020},
  issn           = {0167-9236},
  pages          = {65--74},
  volume         = {90},
  abstract       = {The marvel of markets lies in the fact that dispersed information is instantaneously processed and used to adjust the price of goods, services and assets. Financial markets are particularly efficient when it comes to processing information; such information is typically embedded in textual news that is then interpreted by investors. Quite recently, researchers have started to automatically determine news sentiment in order to explain stock price movements. Interestingly, this so-called news sentiment works fairly well in explaining stock returns. In this paper, we design trading strategies that utilize textual news in order to obtain profits on the basis of novel information entering the market. We thus propose approaches for automated decision-making based on supervised and reinforcement learning. Altogether, we demonstrate how news-based data can be incorporated into an investment system.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:15},
}

@Article{Fulop-Yu-2017,
  author         = {Fulop, Andras and Yu, Jun},
  date           = {2017-10-23},
  journaltitle   = {Econometrics},
  title          = {Bayesian analysis of bubbles in asset prices},
  doi            = {10.3390/econometrics5040047},
  issn           = {2225-1146},
  number         = {4},
  pages          = {47},
  volume         = {5},
  abstract       = {We develop a new model where the dynamic structure of the asset price, after the fundamental value is removed, is subject to two different regimes. One regime reflects the normal period where the asset price divided by the dividend is assumed to follow a mean-reverting process around a stochastic long run mean. The second regime reflects the bubble period with explosive behavior. Stochastic switches between two regimes and non-constant probabilities of exit from the bubble regime are both allowed. A Bayesian learning approach is employed to jointly estimate the latent states and the model parameters in real time. An important feature of our Bayesian method is that we are able to deal with parameter uncertainty and at the same time, to learn about the states and the parameters sequentially, allowing for real time model analysis. This feature is particularly useful for market surveillance. Analysis using simulated data reveals that our method has good power properties for detecting bubbles. Empirical analysis using price-dividend ratios of SandP500 highlights the advantages of our method.},
  day            = {23},
  f1000-projects = {QuantInvest},
  groups         = {Bubble_Crash, Proba_Bayes, ML_AssetPricing},
  timestamp      = {2020-02-25 23:15},
}

@Article{Gan-et-al-2019,
  author         = {Gan, Lirong and Wang, Huamao and Yang, Zhaojun},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Machine learning solutions to challenges in finance: an application to the pricing of financial products},
  doi            = {10.2139/ssrn.3446042},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3446042},
  urldate        = {2019-10-02},
  abstract       = {The recent fast development of machine learning provides new tools to solve challenges in many areas. In finance, average options are popular financial products among corporations, institutional investors, and individual investors for risk management and investment because average options have the advantages of cheap prices and their payoffs are not very sensitive to the changes of the underlying asset prices at the maturity date, avoiding the manipulation of asset prices and option prices. The challenge is that pricing arithmetic average options requires traditional numerical methods with the drawbacks of expensive repetitive computations and simplified models with non-realistic assumptions. This paper proposes a machine-learning method to price arithmetic and geometric average options accurately and in particular quickly. We show the effectiveness of the new method by carrying out comprehensive numerical experiments. Finally, the method is verified by an empirical test. This empirical test actually shows that the machine learning method provides a new model-free method for asset pricing.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:15},
}

@Article{Gonon-et-al-2019,
  author         = {Gonon, Lukas and Muhle-Karbe, Johannes and Shi, Xiaofei},
  date           = {2019-05-13},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Asset Pricing with General Transaction Costs: Theory and Numerics},
  url            = {https://arxiv.org/abs/1905.05027},
  abstract       = {We study risk-sharing equilibria with general convex costs on the agents' trading rates. For an infinite-horizon model with linear state dynamics and exogenous volatilities, the equilibrium returns mean-revert around their frictionless counterparts -- the deviation has Ornstein-Uhlenbeck dynamics for quadratic costs whereas it follows a doubly-reflected Brownian motion if costs are proportional. More general models with arbitrary state dynamics and endogenous volatilities lead to multidimensional systems of nonlinear, fully-coupled forward-backward SDEs. These fall outside the scope of known wellposedness results, but can be solved numerically using the simulation-based deep-learning approach of han.al.17. In a calibration to time series of returns, bid-ask spreads, and trading volume, transaction costs substantially affect equilibrium asset prices. In contrast, the effects of different cost specifications are rather similar, justifying the use of quadratic costs as a proxy for other less tractable specifications.},
  day            = {13},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:15},
}

@Article{Gu-et-al-2019,
  author         = {Gu, Shihao and Kelly, Bryan T. and Xiu, Dacheng},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Autoencoder asset pricing models},
  doi            = {10.2139/ssrn.3335536},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3335536},
  urldate        = {2019-04-21},
  abstract       = {We propose a new latent factor conditional asset pricing model. Like Kelly, Pruitt, and Su (2019), our model allows for latent factors and factor exposures that depend on covariates such as asset characteristics. But, unlike the linearity assumption of KPS, we model factor exposures as a flexible nonlinear function of covariates. Our model retrofits the workhorse unsupervised dimension reduction device from the machine learning literature - autoencoder neural networks - to incorporate information from covariates along with returns themselves. This delivers estimates of nonlinear conditional exposures and the associated latent factors. Furthermore, our machine learning framework imposes the economic restriction of no-arbitrage. Our autoencoder asset pricing model delivers out-of-sample pricing errors that are far smaller (and generally insignificant) compared to other leading factor models.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:15},
}

@Article{Gu-et-al-2019b,
  author         = {Gu, Shihao and Kelly, Bryan T. and Xiu, Dacheng},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Empirical asset pricing via machine learning},
  doi            = {10.2139/ssrn.3159577},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3159577},
  abstract       = {We synthesize the field of machine learning with the canonical problem of empirical asset pricing: Measuring asset risk premia. In the familiar empirical setting of cross section and time series stock return prediction, we perform a comparative analysis of methods in the machine learning repertoire, including generalize linear models, dimension reduction, boosted regression trees, random forests, and neural networks. At the broadest level, we find that machine learning offers an improved description of asset price behavior relative to traditional methods. Our implementation establishes a new standard for accuracy in measuring risk premia summarized by unprecedented high out-of-sample return prediction R2. We identify the best performing methods (trees and neural nets) and trace their predictive gains to allowance of nonlinear predictor interactions that are missed by other methods. Lastly, we find that all methods agree on the same small set of dominant predictive signals that includes variations on momentum, liquidity, and volatility. Improved risk premia measurement through machine learning can simplify the investigation into economic mechanisms of asset pricing and justifies its growing role in innovative financial technologies.},
  f1000-projects = {QuantInvest},
  groups         = {ML_Test_OOS, ML_AssetPricing},
  timestamp      = {2020-02-25 23:15},
}

@Article{Guo-2019a,
  author         = {Guo, James Tengyu},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Anomaly Investing: Out-of-Sample Performance and Intertemporal Considerations},
  doi            = {10.2139/ssrn.3430641},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3430641},
  urldate        = {2019-08-24},
  abstract       = {I first show that the naive equal-weighted 1/N investing in the set of 34 stock market anomalies is a robust implementation for out-of-sample diversification. Two types of popular portfolio optimization methods, including Sharpe-Ratio-optimizing with weight constraints and Dimension-Reduction with machine learning techniques, do not achieve robustly higher out-of-sample performance. Further to explore the gains and risks in investing stock market anomalies, I take this equal-weighted anomaly portfolio to an intertemporal CAPM framework with stochastic volatility to understand the investment considerations of a specific anomaly investor. Based on my estimation, only the correlation-induced volatility news carries a significant risk premium, which highlights the economic importance of the comovement in anomaly asset prices.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:15},
}

@Article{Halperin-Feldshteyn-2018,
  author         = {Halperin, Igor and Feldshteyn, Ilya},
  date           = {2018-05-07},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Market Self-Learning of Signals, Impact and Optimal Trading: Invisible Hand Inference with Free Energy (Or, How We Learned to Stop Worrying and Love Bounded Rationality)},
  url            = {https://ssrn.com/abstract=3174498},
  abstract       = {We present a simple model of a non-equilibrium self-organizing market where asset prices are partially driven by investment decisions of a bounded-rational agent. The agent acts in a stochastic market environment driven by various exogenous "alpha" signals, agent's own actions (via market impact), and noise. Unlike traditional agent-based models, our agent aggregates all traders in the market, rather than being a representative agent. Therefore, it can be identified with a bounded-rational component of the market itself, providing a particular implementation of an Invisible Hand market mechanism. In such setting, market dynamics are modeled as a fictitious self-play of such bounded-rational market-agent in its adversarial stochastic environment. As rewards obtained by such self-playing market agent are not observed from market data, we formulate and solve a simple model of such market dynamics based on a neuroscience-inspired Bounded Rational Information Theoretic Inverse Reinforcement Learning (BRIT-IRL). This results in effective asset price dynamics with a non-linear mean reversion - which in our model is generated dynamically, rather than being postulated. We argue that our model can be used in a similar way to the Black-Litterman model. In particular, it represents, in a simple modeling framework, market views of common predictive signals, market impacts and implied optimal dynamic portfolio allocations, and can be used to assess values of private signals. Moreover, it allows one to quantify a "market-implied" optimal investment strategy, along with a measure of market rationality. Our approach is numerically light, and can be implemented using standard off-the-shelf software such as TensorFlow.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {ML_ReinfoLrng, ML_AssetPricing, ML_InvestSelect},
  timestamp      = {2020-02-25 23:15},
}

@Article{Houlihan-Creamer-2017,
  author         = {Houlihan, Patrick and Creamer, German G.},
  date           = {2017-08-31},
  journaltitle   = {The Journal of Investing},
  title          = {Risk premium of social media sentiment},
  doi            = {10.3905/joi.2017.26.3.021},
  issn           = {1068-0896},
  number         = {3},
  pages          = {21--28},
  volume         = {26},
  abstract       = {This research investigates the predictive capability of sentiment extrapolated from three dictionaries: financial, social media, and mood states. The findings show that 1) through the Fama-MacBeth regression method, social media-based sentiment measures can be used as risk factors in an asset pricing framework; 2) these sentiment measures have predictive capability when used as features in a machine learning framework, and 3) adjusting returns for market effects results in positive alpha.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:15},
}

@Article{DeFranco-et-al-2018,
  author         = {{De Franco}, Carmine and Nicolle, Johann and Pham, Huyen},
  date           = {2018-11-16},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Bayesian learning for the Markowitz portfolio selection problem},
  url            = {https://arxiv.org/abs/1811.06893},
  abstract       = {We study the Markowitz portfolio selection problem with unknown drift vector in the multidimensional framework. The prior belief on the uncertain expected rate of return is modeled by an arbitrary probability law, and a Bayesian approach from filtering theory is used to learn the posterior distribution about the drift given the observed market data of the assets. The Bayesian Markowitz problem is then embedded into an auxiliary standard control problem that we characterize by a dynamic programming method and prove the existence and uniqueness of a smooth solution to the related semi-linear partial differential equation (PDE). The optimal Markowitz portfolio strategy is explicitly computed in the case of a Gaussian prior distribution. Finally, we measure the quantitative impact of learning, updating the strategy from observed data, compared to non-learning, using a constant drift in an uncertain context, and analyze the sensitivity of the value of information w.r.t. various relevant parameters of our model.},
  day            = {16},
  f1000-projects = {QuantInvest},
  groups         = {PortfOptim_Bayes, Proba_Bayes, ML_InvestSelect, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:16},
}

@Article{Dolan-et-al-2017,
  author         = {Dolan, Robert C. and Stevens, Jerry L. and Zucker, Collin R.},
  date           = {2017-12-31},
  journaltitle   = {The Journal of Trading},
  title          = {The Next Generation ETF Student-Managed Investment Program},
  doi            = {10.3905/jot.2018.13.1.007},
  issn           = {1559-3967},
  number         = {1},
  pages          = {7--16},
  volume         = {13},
  abstract       = {Exchange-traded funds (ETFs) are well suited for trading in student-managed investment funds (SMIFs). Unlike other forms of security selection, ETF trading provides efficient trading of portfolios by asset classes, subclasses, investment style, countries, regions, and sectors. The learning experience from trading ETFs based on global macroeconomic themes enhances the learning experience of economics students by requiring application of macroeconomics, industrial organization, international economics, and econometrics. This article presents the structure, tools, and results of an ETF trading program implemented by the economics and finance departments at the University of Richmond. Although this SMIF example uses ETF funds as a learning medium for undergraduates, the investment process with ETFs is also well suited to small individual investors.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@Article{Fastrich-2013,
  author               = {Fastrich, Bjoern},
  date                 = {2013},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Updating Views by Learning from the Others: Dynamically Combining Asset Allocation Strategies},
  doi                  = {10.2139/ssrn.2309696},
  issn                 = {1556-5068},
  abstract             = {The well-known difficulties in obtaining satisfactory results with Markowitz' intuitive portfolio theory have lead to an innumerable amount of proposed advancements by researchers and practitioners. As different as these approaches are, they typically appear to exhibit a satisfactory out-of-sample performance; however, at the same time, studies show that the equally weighted portfolio still cannot be dominated by them. The starting point of our study is therefore not an(other) entirely new idea, which is based on a new strategy we claim performs well, but instead the acknowledgement that the strategies proposed in earlier studies have specific advantages, which, though not consistently apparent, might prevail in specific and possible rare situations of dynamic markets. We therefore propose a strategy that "learns from"a population of already existing strategies and dynamically combines their respective characteristics, resulting in a strategy that is expected to perform best in light of the expected/predicted market situation. We show that our approach is successful by carrying out an empirical backtest study applied in a multi-asset setting for investor clienteles with mean-variance, mean-conditional value-at-risk, and maximum Omega utility functions. The improvements of our flexible approach, which include a higher mean return and lower volatility, stay (statistically) significant, even when we take into account transaction costs and improve the competing strategies by employing robust input parameter estimates.},
  citeulike-article-id = {14320232},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2309696},
  groups               = {ML_InvestSelect},
  posted-at            = {2017-03-26 16:31:15},
  timestamp            = {2020-02-25 23:16},
}

@Article{Feng-He-2019,
  author         = {Feng, Guanhao and He, Jingyu},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Factor investing: hierarchical ensemble learning},
  doi            = {10.2139/ssrn.3326617},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3326617},
  urldate        = {2019-03-07},
  abstract       = {We present a Bayesian hierarchical framework for both cross-sectional and time-series return prediction. Our approach builds on a market-timing predictive system that jointly allows for time-varying coefficients driven by fundamental characteristics. With a Bayesian formulation for ensemble learning, we examine the joint predictability as well as portfolio efficiency via predictive distribution. In the empirical analysis of asset-sector allocation, our hierarchical ensemble learning portfolio achieves 500\% cumulative returns in the period 1998-2017, and outperforms most workhorse benchmarks as well as the passive investing index. Our Bayesian inference for model selection identifies useful macro predictors (long-term yield, inflation, and stock market variance) and asset characteristics (dividend yield, accrual, and gross profit). Using the selected model for predicting sector evolution, an equally weighted long-short portfolio on winners over losers achieves a 46\% Sharpe ratio with a significant Jensen alpha. Finally, we explore an underexploited connection between classical Bayesian forecasting and modern ensemble learning.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_ShortTerm, FrcstQWIM_MedLngTerm, FcstQWIM_Equity, ML_ForcstTimeSrs, ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@InCollection{GadrePatwardhan-et-al-2018,
  author         = {Gadre-Patwardhan, Swapnaja and Katdare, Vivek and Joshi, Manish},
  booktitle      = {Smart computing and informatics},
  date           = {2018},
  title          = {Study of dimensionality reduction techniques for effective investment portfolio data management},
  doi            = {10.1007/978-981-10-5544-7\_67},
  editor         = {Satapathy, Suresh Chandra and Bhateja, Vikrant and Das, Swagatam},
  isbn           = {978-981-10-5543-0},
  location       = {Singapore},
  pages          = {679--689},
  publisher      = {Springer Singapore},
  series         = {Smart innovation, systems and technologies},
  volume         = {77},
  abstract       = {The aim of dimensionality reduction is to depict meaningful low-dimensional data of high-dimensional data set. Several new nonlinear methods have been proposed for last many years. But the question of their assessment is still open for the study. Dimensionality reduction is the vital problem in supervised and unsupervised learning. For high-dimensional data, computation becomes heavy if no pre-processing is done before supplying it to any of the classifiers. Because of the constraints like memory and speed, it is not suitable for certain practical applications. As per the method of attribute selection process, attribute sets are provided as an input to the classifier. The attributes that incorrectly classified are supposed to be irrelevant and are removed by obtaining the subset of selected attributes. Thus, accuracy of the classifier is improved, and time is also reduced. Attribute evaluators such as cfsSubset evaluator, information gain ranking filter, chi-squared ranking filter and gain ration feature evaluator are used for the classifiers viz. decision table, decision stump, J48, random forest. Individual investor investment portfolio data is used for the present study. Twenty-six attributes are obtained from the questionnaire. By applying dimensionality reduction techniques, five major attributes are obtained using information gain ranking filter, chi-squared ranking filter, gain ratio feature evaluation and seven attributes using cfsSubset evaluator. Around 70.7692\% accuracy is obtained using three attribute evaluators for all five classification algorithms, whereas cfsSubset evaluator along with random forest classifier gives 81.5385\% accuracy. It has been observed that cfsSubset evaluator with partition membership as a pre-processing technique and random forest as classification algorithm performs reasonably better in terms of accuracy.},
  f1000-projects = {QuantInvest},
  groups         = {Dimens_Reduc, ML_FeatureSelection, ML_InvestSelect},
  issn           = {2190-3018},
  timestamp      = {2020-02-25 23:16},
}

@Article{Gan-2013,
  author               = {Gan, Guojun},
  date                 = {2013-11},
  journaltitle         = {Insurance: Mathematics and Economics},
  title                = {Application of data clustering and machine learning in variable annuity valuation},
  doi                  = {10.1016/j.insmatheco.2013.09.021},
  issn                 = {0167-6687},
  number               = {3},
  pages                = {795--801},
  volume               = {53},
  abstract             = {We study the pricing of a large portfolio of VA policies. A clustering method is used to select representative policies. A machine learning method is used to estimate the guarantee value. The proposed method performs well in terms of accuracy and speed. The valuation of variable annuity guarantees has been studied extensively in the past four decades. However, almost all the studies focus on the valuation of guarantees embedded in a single variable annuity contract. How to efficiently price the guarantees for a large portfolio of variable annuity contracts has not received enough attention. This paper fills the gap by introducing a novel method based on data clustering and machine learning to price the guarantees for a large portfolio of variable annuity contracts. Our test results show that this method performs very well in terms of accuracy and speed.},
  citeulike-article-id = {13934351},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.insmatheco.2013.09.021},
  groups               = {Networks and investment management, Machine learning and investment strategies, Annuities, ML_InvestSelect},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:15:25},
  timestamp            = {2020-02-25 23:16},
}

@InBook{Guglietta-2019,
  author         = {Guglietta, Joel},
  booktitle      = {Big data and machine learning in quantitative investment},
  date           = {2019},
  title          = {Support Vector Machine-Based Global Tactical Asset Allocation},
  doi            = {10.1002/9781119522225.ch11},
  isbn           = {9781119522195},
  location       = {Chichester, UK},
  pages          = {211--224},
  publisher      = {John Wiley \& Sons, Ltd},
  abstract       = {This chapter shows how machine learning, more specifically support vector machine/regression (SVM/R) can help building global tactical asset allocation (GTAA) portfolio. It presents a quick literature review on GTAA, explaining the different families of asset allocation. The chapter goes through a historical perspective of tactical asset allocation in the last 50 years, introducing the seminal concepts behind it. It explains the definition of support vector machine (SVM) and support vector relevance (SVR). The chapter presents the machine learning model used for tactical asset allocation and discusses the results. SVM is essentially an algorithm used to solve a classification problem such as deciding which stocks to buy and to sell. Because of its machine learning characteristics, SVR-based GTAA portfolio can adapt to different economic environments and provide investors with a robust solution improving the main goal of asset allocation: getting the best expected return-to-risk profile.},
  day            = {30},
  f1000-projects = {QuantInvest},
  groups         = {TAA, ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@Article{Hajek-2017,
  author               = {Hajek, Petr},
  date                 = {2017},
  journaltitle         = {Neural Computing and Applications},
  title                = {Combining bag-of-words and sentiment features of annual reports to predict abnormal stock returns},
  doi                  = {10.1007/s00521-017-3194-2},
  pages                = {1--16},
  abstract             = {Automated textual analysis of firm-related documents has become an important decision support tool for stock market investors. Previous studies tended to adopt either dictionary-based or machine learning approach. Nevertheless, little is known about their concurrent use. Here we use the combination of financial indicators, readability, sentiment categories, and bag-of-words (BoW) to increase prediction accuracy. This paper aims to extract both sentiment and BoW information from the annual reports of US firms. The sentiment analysis is based on two commonly used dictionaries, namely a general dictionary Diction 7.0 and a finance-specific dictionary proposed by Loughran and McDonald (J Finance 66:35-65, 2011). The BoW are selected according to their tf-idf. We combine these features with financial indicators to predict abnormal stock returns using a multilayer perceptron neural network with dropout regularization and rectified linear units. We show that this method performs similarly as naive Bayes and outperforms other machine learning algorithms (support vector machine, C4.5 decision tree, and k-nearest neighbour classifier) in predicting positive/negative abnormal stock returns in terms of ROC. We also show that the quality of the prediction significantly increased when using the correlation-based feature selection of BoW. This prediction performance is robust to industry categorization and event window.},
  citeulike-article-id = {14506908},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00521-017-3194-2},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s00521-017-3194-2},
  groups               = {Characteristics and return prediction, ML_InvestSelect, ML_Text_QWIM},
  posted-at            = {2017-12-22 00:13:25},
  publisher            = {Springer London},
  timestamp            = {2020-02-25 23:16},
}

@Article{Harvey-et-al-2018a,
  author         = {Harvey, Campbell R and Rattray, Sandy and Sinclair, Andrew and van Hemert, Otto},
  date           = {2018-07-31},
  journaltitle   = {Practical Applications},
  title          = {Practical applications of man vs. machine: comparing discretionary and systematic hedge fund performance},
  doi            = {10.3905/pa.6.1.269},
  issn           = {2329-0196},
  number         = {1},
  pages          = {13--5},
  volume         = {6},
  abstract       = {Practical Applications Summary Quantitative investing, which deploys machine learning and other algorithms, now more or less dominates financial markets. In this environment, it9s useful to step back and compare the performance and risk exposures of discretionary and systematic hedge fund managers. Many allocators to hedge funds, large and small alike, avoid allocating to systematic funds, either partially or entirely, believing them to be difficult to understand, to offer less transparency, and to deliver worse performance due to the use of data from the past. These reasons seem to be consistent with aversion distrust of systems. In their article Man vs. Machine: Comparing Discretionary and Systematic Hedge Fund Performance, published in the Summer 2017 issue of The Journal of Portfolio Management , Campbell R. Harvey, Sandy Rattray, Andrew Sinclair, and Otto van Hemert compare the past performance of systematic funds with their discretionary counterparts. They show that, after adjusting for volatility and factor exposures, the lack of confidence in systematic funds is not justified.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Hedge_Funds, ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@Article{Hsu-et-al-2018,
  author         = {Hsu, Po-Hsuan and Han, Qiheng and Wu, Wensheng and Cao, Zhiguang},
  date           = {2018-12},
  journaltitle   = {Journal of Banking \& Finance},
  title          = {Asset allocation strategies, data snooping, and the 1 / N rule},
  doi            = {10.1016/j.jbankfin.2018.09.021},
  issn           = {0378-4266},
  pages          = {257--269},
  volume         = {97},
  abstract       = {Using a series of advanced tests from White's (2000) Check to correct for data-snooping bias, we assess the out-of-sample performance of various portfolio strategies relative to the naive 1/N rule. When we analyze 16 basic portfolio strategies, 126 learning strategies, and nearly 2,000 extended strategies, we find that some strategies outperform the 1/N rule in conventional tests that do not account for data-snooping bias. However, after we use the new tests that control for such bias, we find that none or very few of these strategies outperform the 1/N rule. Thus, our finding underscores the necessity to control for data-snooping bias when making asset allocation decisions.},
  f1000-projects = {QuantInvest},
  groups         = {ML_Test_OOS, ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@Article{Huber-2018,
  author         = {Huber, Claus},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Machine learning for visual risk analysis and hedge fund selection},
  doi            = {10.2139/ssrn.3289979},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3289979},
  abstract       = {One of the main principles to build portfolios of financial assets is to achieve stable long-term performance and avoid large drawdowns. This article describes how a method of Machine Learning, Kohonen Self-Organising Maps (SOM), can be applied to visualise risk and to build robust portfolios of hedge fund managers. Essentially, it documents a feasibility study that was conducted to gauge whether Machine Learning can add any value to the investment process of an investor in hedge funds.We suggest a simple method to exploit the SOM feature of identifying similarities in high-dimensional data: managers are selected from the 4 most remote parts of the SOM, i.e., the units in the lower left, lower right, upper left and upper right corners. Hedge Fund portfolios based on this method achieve more favourable risk/return ratios and lower drawdowns than benchmarks. In discussions with clients it has turned out that the way SOMs work as well as the method to pick managers from remote areas of the SOM can be intuitively explained and understood, which increases acceptance by practitioners.},
  f1000-projects = {QuantInvest},
  groups         = {Fund_Select, ML_FeatureSelection, Hedge_Funds, Benchmark_AI, ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@Article{Hughen-et-al-2017,
  author         = {Hughen, Christopher and Strauss, Jack and Tremblay, J. P.},
  date           = {2017-12-31},
  journaltitle   = {The Journal of Trading},
  title          = {Adding Value in Student-Managed Funds:Benchmark and Sector Selection},
  doi            = {10.3905/jot.2018.13.1.027},
  issn           = {1559-3967},
  number         = {1},
  pages          = {27--34},
  urldate        = {2019-04-28},
  volume         = {13},
  abstract       = {Student-managed portfolios offer a practical learning environment but often miss opportunities for outperformance. The authors provide several recommendations for structuring fund trades to enhance the pedagogical experience for the students in addition to generating alpha. A strategy that targets midcap stocks offers favorable risk-return characteristics and focuses on a market capitalization category that receives relatively less attention from professional money managers. Furthermore, a formal sector allocation strategy provides an additional source of portfolio outperformance when using a metric that is robust to differences in companies across sectors. The authors document the high relative return dispersion among sectors in midcap stocks and show that enterprise value/EBITDA is a consistently effective ratio in identifying both undervalued and overvalued stocks.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@Article{Kim-2019,
  author         = {Kim, Saejoon},
  date           = {2019-01-25},
  journaltitle   = {Quantitative Finance},
  title          = {Enhancing the momentum strategy through deep regression},
  doi            = {10.1080/14697688.2018.1563707},
  issn           = {1469-7688},
  pages          = {1--13},
  abstract       = {Momentum is a pervasive and persistent phenomenon in financial economics that has been found to generate abnormal returns not explainable by the traditional asset pricing models. This paper investigates some variations of the existing momentum strategies to increase profit and gain other desirable properties such as low kurtosis, small negative skewness and small maximum drawdown. We investigate these by using regression that is based on the latest techniques from deep learning such as stacked autoencoders and denoising autoencoders. Empirical results indicate that our regression-based variations can generate increased returns, and improved higher-order moments and maximum drawdown characteristics. Furthermore, our results reveal such improved performance can only be attained through the use of the latest deep learning technologies.},
  day            = {25},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:16},
}

@InBook{Kollo-2019,
  author         = {Kollo, Michael},
  booktitle      = {Big data and machine learning in quantitative investment},
  date           = {2019},
  title          = {Do algorithms dream about artificial alphas?},
  doi            = {10.1002/9781119522225.ch1},
  isbn           = {9781119522195},
  location       = {Chichester, UK},
  pages          = {1--12},
  publisher      = {John Wiley \& Sons, Ltd},
  urldate        = {2019-01-26},
  abstract       = {The reinvention was in a shift in the object of interest, from individual stocks to a series of network relationships, and their ebb and flow through time. It was subtle, as it was severe, and is probably still not fully understood. Reinvention with machine learning poses a similar opportunity for us to reinvent the way we think about the financial markets. In the case of a handwritten number, for example, the pixels of the picture are converted to numeric representations, and the patterns in the pixels are sought using a deep learning algorithm. In the case of a fundamental investment process, the 'language' of asset pricing is one filled with reference to the business conditions of firms, their financial statements, earnings, assets, and generally business prospects. A set of observations and examples of how machine learning could help us learn more about financial markets is provided.},
  day            = {30},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:16},
}

@Article{Kolm-Ritter-2019a,
  author         = {Kolm, Petter N. and Ritter, Gordon},
  date           = {2019-09-06},
  journaltitle   = {The Journal of Machine Learning in Finance},
  title          = {Modern Perspectives on Reinforcement Learning in Finance},
  number         = {1},
  url            = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3449401},
  urldate        = {2019-09-07},
  volume         = {1},
  abstract       = {We give an overview and outlook of the field of reinforcement learning as it applies to solving financial applications of intertemporal choice. In finance, common problems of this kind include pricing and hedging of contingent claims, investment and portfolio allocation, buying and selling a portfolio of securities subject to transaction costs, market making, asset liability management and optimization of tax consequences, to name a few. Reinforcement learning allows us to solve these dynamic optimization problems in an almost model-free way, relaxing the assumptions often needed for classical approaches.A main contribution of this article is the elucidation of the link between these dynamic optimization problem and reinforcement learning, concretely addressing how to formulate expected intertemporal utility maximization problems using modern machine learning techniques.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@Article{Kozak-2019,
  author         = {Kozak, Serhiy},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Kernel trick for the cross section},
  doi            = {10.2139/ssrn.3307895},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3307895},
  abstract       = {Characteristics-based asset pricing implicitly assumes that factor betas or risk prices are linear functions of pre-specified characteristics. Present-value identities, such as Campbell-Shiller or clean-surplus accounting, however, clearly predict that expected returns are highly non-linear functions of all characteristics. While basic non-linearities can be easily accommodated by adding non-linear functions to the set of characteristics, the problem quickly becomes infeasible once interactions of characteristics are considered. I propose a method which uses economically-driven regularization to construct a stochastic discount factor (SDF) when the set of characteristics is extended to an arbitrary - potentially infinitely-dimensional - set of non-linear functions of original characteristics. The method borrows ideas from a machine learning technique known as the trick to circumvent the curse of dimensionality. I find that allowing for interactions and non-linearities of characteristics leads to substantially more efficient SDFs; out-of-sample Sharpe ratios for the implied MVE portfolio double.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:16},
}

@Article{Kyriakou-et-al-2019,
  author         = {Kyriakou, Ioannis and Mousavi, Parastoo and Nielsen, Jens Perch and Scholz, Michael},
  date           = {2019-07-24},
  journaltitle   = {Annals of operations research},
  title          = {Forecasting benchmarks of long-term stock returns via machine learning},
  doi            = {10.1007/s10479-019-03338-4},
  issn           = {0254-5330},
  abstract       = {Recent advances in pension product development seem to favour alternatives to the risk free asset often used in the financial theory as a performance standard for measuring the value generated by an investment or a reference point for determining the value of a financial instrument. To this end, in this paper, we apply the simplest machine learning technique, namely, a fully nonparametric smoother with the covariates and the smoothing parameter chosen by cross-validation to forecast stock returns in excess of different benchmarks, including the short-term interest rate, long-term interest rate, earnings-by-price ratio, and the inflation. We find that, net-of-inflation, the combined earnings-by-price and long-short rate spread form our best-performing two-dimensional set of predictors for future annual stock returns. This is a crucial conclusion for actuarial applications that aim to provide real-income forecasts for pensioners.},
  day            = {24},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:16},
}

@Article{Law-ShaweTaylor-2017,
  author               = {Law, T. and Shawe-Taylor, J.},
  date                 = {2017-03},
  journaltitle         = {Quantitative Finance},
  title                = {Practical Bayesian support vector regression for financial time series prediction and market condition change detection},
  doi                  = {10.1080/14697688.2016.1267868},
  pages                = {1--14},
  abstract             = {Support vector regression (SVR) has long been proven to be a successful tool to predict financial time series. The core idea of this study is to outline an automated framework for achieving a faster and easier parameter selection process, and at the same time, generating useful prediction uncertainty estimates in order to effectively tackle flexible real-world financial time series prediction problems. A Bayesian approach to SVR is discussed, and implemented. It is found that the direct implementation of the probabilistic framework of Gao et al. returns unsatisfactory results in our experiments. A novel enhancement is proposed by adding a new kernel scaling parameter to overcome the difficulties encountered. In addition, the multi-armed bandit Bayesian optimization technique is applied to automate the parameter selection process. Our framework is then tested on financial time series of various asset classes (i.e. equity index, credit default swaps spread, bond yields, and commodity futures) to ensure its flexibility. It is shown that the generalization performance of this parameter selection process can reach or sometimes surpass the computationally expensive cross-validation procedure. An adaptive calibration process is also described to allow practical use of the prediction uncertainty estimates to assess the quality of predictions. It is shown that the machine-learning approach discussed in this study can be developed as a very useful pricing tool, and potentially a market condition change detector. A further extension is possible by taking the prediction uncertainties into consideration when building a financial portfolio.},
  citeulike-article-id = {14309373},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2016.1267868},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1267868},
  day                  = {9},
  groups               = {Proba_Bayes, ML_Test_CrossVal, ML_Validation, ML_AssetPricing},
  posted-at            = {2017-03-12 01:25:21},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 23:16},
}

@Article{Liang-et-al-2019a,
  author         = {Liang, Jian and Xu, Zhe and Li, Peter},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Deep Learning-Based Least Square Forward-Backward Stochastic Differential Equation Solver for High-Dimensional Derivative Pricing},
  doi            = {10.2139/ssrn.3381794},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3381794},
  urldate        = {2019-08-11},
  abstract       = {We propose a new forward-backward stochastic differential equation solver for high-dimensional derivatives pricing problems by combining deep learning solver with least square regression technique widely used in the least square Monte Carlo method for the valuation of American options. Our numerical experiments demonstrate the efficiency and accuracy of our least square backward deep neural network solver and its capability to provide accurate prices for complex early exercise derivatives such as callable yield notes. Our method can serve as a generic numerical solver for pricing derivatives across various asset groups, in particular, as an efficient means for pricing high-dimensional derivatives with early exercises features.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:16},
}

@Article{Liew-Mayster-2017a,
  author               = {Liew, Jim K. and Mayster, Boris},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Forecasting ETFs with Machine Learning Algorithms},
  doi                  = {10.2139/ssrn.2899520},
  issn                 = {1556-5068},
  abstract             = {In this work, we apply cutting edge machine learning algorithms to one of the oldest challenges in finance: Predicting returns. For the sake of simplicity, we focus on predicting the direction (e.g. either up or down) of several liquid ETFs and do not attempt to predict the magnitude of price changes. The ETFs we use serve as asset class proxies. We employ approximately five years of historical daily data obtained through Yahoo Finance from January 2011 to January 2016. Utilizing our supervised learning classification algorithms, readily available from Python's Scikit-Learn, we employ three powerful techniques:

(1) Deep Neural Networks,

(2) Random Forests, and

(3) Support Vector Machines (linear and radial basis function).

We document the performance of our three algorithms across our four information sets. We segment our information sets into

(A) past returns,

(B) past volume,

(C) dummies for days/months, and a combination of all three.

We introduce our "gain criterion"to aid in our comparison of classifiers' performance. First, we find that these algorithms work well over the one-month to three-month horizons. Short-horizon predictability, over days, is extremely difficult, thus our results support the short-term random walk hypothesis. Second, we document the importance of cross-sectional and intertemporal volume as a powerful information set. Third, we show that many features are needed for predictability as each feature provides very small contributions. We conclude, therefore, that ETFs can be predicted with machine learning algorithms but practitioners should incorporate prior knowledge of markets and intuition on asset class behavior.},
  citeulike-article-id = {14332537},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2899520},
  groups               = {ML_AssetPricing},
  posted-at            = {2017-04-05 21:29:15},
  timestamp            = {2020-02-25 23:16},
}

@Article{Lihn-2018,
  author         = {Lihn, Stephen H. T.},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Jubilee Tectonic Model: Forecasting Long-Term Growth and Mean Reversion in the U.S. Stock Market},
  doi            = {10.2139/ssrn.3156574},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3156574},
  abstract       = {We propose a long-term forecast model based on linear growth and mean reversion characteristics in the U.S. stock market. It can forecast future returns of the stock market, Treasury yield, and gold price. The name comes from its optimal trend-following window of 45 years. The name comes from the hypothesis that there are fault lines in the historical CAPE, which can be calibrated and corrected through statistical learning. The tectonically adjusted log-CAPE can be fully decomposed by a four-factor regression: two from mean reversion, two from inflation. This regression explains the mystery of lofty CAPE. These five factors form the forecast model for the 10 and 20-year future equity returns with high R-square above 80\%. Major fault lines in CAPE are identified in each equity forecast model. We also analyze the causality in which the equity cycle leads both the Treasury yield and real gold price by 74 and 30 months. Therefore, their near-term direction can be predicted by extrapolating the regression. For longer horizon, we apply the factor model to forecast Treasury 20-year future yield and gold 20-year real return. A parsimonious triangular wave model is constructed to explain the periodicity of mean reversion in the past century. We conclude that the channel deviation is an universal mean-reversion variable among the three major asset classes.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_MedLngTerm, FcstQWIM_Equity, ML_AssetPricing},
  timestamp      = {2020-02-25 23:16},
}

@Article{Liu-et-al-2019b,
  author         = {Liu, Shuaiqiang and Borovykh, Anastasia and Grzelak, Lech A. and Oosterlee, Cornelis W.},
  date           = {2019-04-23},
  journaltitle   = {arXiv Electronic Journal},
  title          = {A neural network-based framework for financial model calibration},
  url            = {https://arxiv.org/abs/1904.10523},
  abstract       = {A data-driven approach called CaNN (Calibration Neural Network) is proposed to calibrate financial asset price models using an Artificial Neural Network (ANN). Determining optimal values of the model parameters is formulated as training hidden neurons within a machine learning framework, based on available financial option prices. The framework consists of two parts: a forward pass in which we train the weights of the ANN off-line, valuing options under many different asset model parameter settings; and a backward pass, in which we evaluate the trained ANN-solver on-line, aiming to find the weights of the neurons in the input layer. The rapid on-line learning of implied volatility by ANNs, in combination with the use of an adapted parallel global optimization method, tackles the computation bottleneck and provides a fast and reliable technique for calibrating model parameters while avoiding, as much as possible, getting stuck in local minima. Numerical experiments confirm that this machine-learning framework can be employed to calibrate parameters of high-dimensional stochastic volatility models efficiently and accurately.},
  day            = {23},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:16},
}

@Article{Lonn-Schotman-2018,
  author         = {Lonn, Rasmus and Schotman, Peter C.},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Empirical asset pricing with many assets and short time series},
  doi            = {10.2139/ssrn.3278229},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3278229},
  urldate        = {2019-09-07},
  abstract       = {We construct tracking portfolios consisting of a large number of assets for macroeconomic factors using the L2-boosting algorithm. We use these tracking portfolios as instruments to estimate factor risk prices. The same learning algorithm also provides the weights of a mean-variance efficient portfolio. With this additional input we compute the Hansen-Jagannathan distance to compare how alternative models fit the cross-section. We apply the method to 900 portfolio return series in the Kenneth French data library. While macro factors fail to explain most cross-sectional variation, we find that both consumption and inflation risk are priced.},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing},
  timestamp      = {2020-02-25 23:16},
}

@Article{Ma-2018,
  author         = {Ma, Tao},
  date           = {2018-09-27},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Multi-task Learning for Financial Forecasting},
  url            = {https://arxiv.org/abs/1809.10336},
  urldate        = {2019-03-07},
  abstract       = {Financial forecasting is challenging and attractive in machine learning. There are many classic solutions, as well as many deep learning based methods, proposed to deal with it yielding encouraging performance. Stock time series forecasting is the most representative problem in financial forecasting. Due to the strong connections among stocks, the information valuable for forecasting is not only included in individual stocks, but also included in the stocks related to them. However, most previous works focus on one single stock, which easily ignore the valuable information in others. To leverage more information, in this paper, we propose a jointly forecasting approach to process multiple time series of related stocks simultaneously, using multi-task learning framework. Compared to the previous works, we use multiple networks to forecast multiple related stocks, using the shared and private information of them simultaneously through multi-task learning. Moreover, we propose an attention method learning an optimized weighted combination of shared and private information based on the idea of Capital Asset Pricing Model (CAPM) to help forecast. Experimental results on various data show improved forecasting performance over baseline methods.},
  day            = {27},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs, ML_AssetPricing},
  timestamp      = {2020-02-25 23:16},
}

@Article{Nagel-2013,
  author               = {Nagel, Stefan},
  date                 = {2013},
  journaltitle         = {Annual Review of Financial Economics},
  title                = {Empirical Cross-Sectional Asset Pricing},
  doi                  = {10.1146/annurev-financial-110112-121009},
  number               = {1},
  pages                = {167--199},
  volume               = {5},
  abstract             = {I review recent research efforts in the area of empirical cross-sectional asset pricing. I start by summarizing the evidence on cross-sectional return predictability and the failure of standard (consumption) capital asset pricing models (CAPMs) and their conditional versions to explain these predictability patterns. Part of the recent literature focuses on ad hoc factor models, which summarize the cross section of expected returns in parsimonious form, or on production-based approaches, which suggest links between firm characteristics and expected returns. Without imposing restrictions on investor preferences and beliefs, neither one of these two approaches can answer the question of why investors price assets the way they do. Within the rational expectations paradigm, recent research that imposes such restrictions has focused on the intertemporal CAPM (ICAPM), long-run risks models, as well as frictions and liquidity risk. Approaches based on investor sentiment have focused on the development of empirical proxies for sentiment and for the limits to arbitrage that allow sentiment to affect prices. Empirical work that considers learning and adaptation of investors has worked with out-of-sample tests of cross-sectional predictability.},
  citeulike-article-id = {13988521},
  citeulike-linkout-0  = {http://www.annualreviews.org/doi/abs/10.1146/annurev-financial-110112-121009},
  citeulike-linkout-1  = {http://dx.doi.org/10.1146/annurev-financial-110112-121009},
  groups               = {ML_AssetPricing, ML_Text_QWIM},
  owner                = {cristi},
  posted-at            = {2016-03-26 15:00:13},
  timestamp            = {2020-02-25 23:16},
}

@Article{NoelKoide-2016,
  author               = {Noel-Koide, Kevin},
  date                 = {2016-09},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Application of Machine Learning to Systematic Allocation Strategies},
  url                  = {https://ssrn.com/abstract=2837664},
  abstract             = {We investigate the use of machine learning techniques into building statistically stable systematic allocation strategies. Traditionally, allocation processes usually rely on variations of Markowitz framework such as Mean Variance allocation, Maximum Diversity, Risk Parity, Conditional Value at Risk, ie convex frontier optimization. Although those methods show some efficiency to allocate assets through the convex efficient frontier, they usually rely deeply on the estimation and the usage of the covariance matrix. Being no stationary and having multiple range memory (ie FIGARCH), the statistical estimation of covariance may lead to biases and errors and in the end, bias conclusions. Very extensive literature in econo-metrics, econo-physics, quantitative allocation cover this problem in order to remedy to the statistical estimation of covariance and his bias and issues.Here, our emphasis is not a new estimator of the covariance matrix, or a variant of Risk Parity but an application of Machine Learning techniques to infer no-linear relationships and long range memory between the assets.It has the advantage to remove the linear projection of the assets onto the covariance framework and then capture no-linear relationships between at various time periods.Recent advances in Neural Network, Deep Learning and Machine Learning allows a more efficient modeling of the no-linear statistical relationships between data (ie price, dividends,....). Among them, we can mention Restricted Boltzman Machines, Variationnal Auto-encoders and variations of Recurrent Neural Network, Highway Long Short Term Memory Network as well as Factorization Machines for projection on local sub-spaces.Thus, we investigate some of the techniques to develop practical systematic allocation strategies by reducing risks and estimations biases and show the results.},
  citeulike-article-id = {14136316},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2837664},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2837664code2498383.pdf?abstractid=2837664 and mirid=1},
  day                  = {12},
  groups               = {ML_AssetPricing},
  owner                = {cristi},
  posted-at            = {2016-09-13 15:30:32},
  timestamp            = {2020-02-25 23:16},
}

@Article{Jain-Jain-2019,
  author         = {Jain, Prayut and Jain, Shashi},
  date           = {2019-05-26},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Can Machine Learning Based Portfolios Outperform Traditional Risk-Based Portfolios? The Need to Account for Covariance Misspecification},
  url            = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3394427},
  urldate        = {2019-09-06},
  abstract       = {The Hierarchical risk parity (HRP) approach of portfolio allocation, introduced by [Lopez de Prado, 2016], applies graph theory and machine learning to build a diversified portfolio. Like the traditional risk based allocation methods, HRP is also a function of the estimate of the covariance matrix, however, it doesn require its invertibility. In this paper we first study the impact of covariance misspecification on the performance of the different allocation methods. Next we study under appropriate covariance forecast model whether the machine learning based HRP out-performs the traditional risk based portfolios. For our analysis we use the test for superior predictive ability, on out-of-sample portfolio performance, to determine whether the observed excess performance is significant or occurred by chance. We find that when the covariance estimates are crude, inverse volatility weighted portfolios are more robust, followed by the machine learning based portfolios. Minimum variance and maximum diversification are most sensitive to covariance misspecification. HRP follows the middle ground, it is less sensitive to covariance misspecification when compared with minimum variance or maximum diversification portfolio, while it is not as robust as the inverse volatility weighed portfolio. We also study the impact of different rebalancing horizon and how the portfolios compare against a market-capitalization weighted portfolio.},
  day            = {26},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect, Invest_Risk},
  timestamp      = {2020-02-25 23:16},
}

@Article{James-et-al-2019a,
  author         = {Alexander James and Yaser S. Abu-Mostafa and Xiao Qiao},
  date           = {2019-08-01},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Machine learning for recession prediction and dynamic asset allocation},
  doi            = {10.3905/jfds.2019.1.007},
  issn           = {2640-3951},
  url            = {https://jfds.pm-research.com/content/1/3/41},
  urldate        = {2019-09-22},
  abstract       = {The authors introduce a novel application of support vector machines (SVM), an important machine learning algorithm, to determine the beginning and end of recessions in real time. Nowcasting, forecasting a condition in the present time because the full information will not be available until later, is key for recessions, which are only determined months after the fact. The authors show that SVM has excellent predictive performance for this task, capturing all six recessions from 1973 to 2018 and providing the signal with minimal delay. The authors take advantage of the timeliness of SVM signals to test dynamic asset allocation between stocks and bonds. A dynamic risk budgeting approach using SVM outputs appears superior to an equal-risk contribution portfolio, improving the average returns by 85 bps per annum without increased tail risk.},
  day            = {1},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@Article{Jiang-et-al-2017a,
  author               = {Jiang, Zhengyao and Xu, Dixing and Liang, Jinjun},
  date                 = {2017},
  journaltitle         = {arXiv Electronic Journal},
  title                = {A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem},
  eprint               = {1706.10059},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1706.10059},
  abstract             = {Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25\% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days.},
  citeulike-article-id = {14449203},
  citeulike-linkout-0  = {http://arxiv.org/abs/1706.10059},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1706.10059},
  day                  = {16},
  groups               = {Machine learning and investment strategies, ML_ReinfoLrng, ML_InvestSelect, DeepLearning_QWIM},
  posted-at            = {2017-10-12 00:17:25},
  timestamp            = {2020-02-25 23:16},
  year                 = {2017},
}

@Article{Karathanasopoulos-et-al-2017,
  author               = {Karathanasopoulos, Andreas and Mitra, Sovan and Skindilias, Konstantinos and Lo, Chia C.},
  date                 = {2017},
  journaltitle         = {Journal of Forecasting},
  title                = {Modelling and Trading the English and German Stock Markets with Novelty Optimization Techniques},
  doi                  = {10.1002/for.2445},
  abstract             = {The motivation for this paper was the introduction of novel short-term models to trade the FTSE 100 and DAX 30 exchange-traded funds (ETF) indices. There are major contributions in this paper which include the introduction of an input selection criterion when utilizing an expansive universe of inputs, a hybrid combination of partial swarm optimizer (PSO) with radial basis function (RBF) neural networks, the application of a PSO algorithm to a traditional autoregressive moving model (ARMA), the application of a PSO algorithm to a higher-order neural network and, finally, the introduction of a multi-objective algorithm to optimize statistical and trading performance when trading an index. All the machine learning-based methodologies and the conventional models are adapted and optimized to model the index. A PSO algorithm is used to optimize the weights in a traditional RBF neural network, in a higher-order neural network (HONN) and the AR and MA terms of an ARMA model. In terms of checking the statistical and empirical accuracy of the novel models, we benchmark them with a traditional HONN, with an ARMA, with a moving average convergence/divergence model (MACD) and with a naive strategy. More specifically, the trading and statistical performance of all models is investigated in a forecast simulation of the FTSE 100 and DAX 30 ETF time series over the period January 2004 to December 2015 using the last 3 years for out-of-sample testing. Finally, the empirical and statistical results indicate that the PSO-RBF model outperforms all other examined models in terms of trading accuracy and profitability, even with mixed inputs and with only autoregressive inputs.},
  citeulike-article-id = {14270567},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2445},
  groups               = {ML_Forecast_QWIM, FrcstQWIM_Hybrid, ML_NumOptimiz, ML_ForcstTimeSrs, ML_InvestSelect},
  posted-at            = {2017-02-02 19:13:30},
  timestamp            = {2020-02-25 23:16},
}

@Article{KomSamo-Vervuurt-2016,
  author               = {Kom Samo, Yves-Laurent and Vervuurt, Alexander},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Stochastic Portfolio Theory: A Machine Learning Perspective},
  url                  = {https://ssrn.com/abstract=2777631},
  abstract             = {In this paper we propose a novel application of Gaussian processes (GPs) to financial asset allocation. Our approach is deeply rooted in Stochastic Portfolio Theory (SPT), a stochastic analysis framework introduced by Robert E. Fernholz that aims at flexibly analysing the performance of certain investment strategies in stock markets relative to benchmark indices. In particular, SPT has exhibited some investment strategies based on company sizes that, under realistic assumptions, outperform benchmark indices with probability 1 over certain time horizons. Galvanised by this result, we consider the inverse problem that consists of learning (from historical data) an optimal investment strategy based on any given set of trading characteristics, and using a user-specified optimality criterion that may go beyond outperforming a benchmark index. Although this inverse problem is of the utmost interest to investment management practitioners, it can hardly be tackled using the SPT framework. We show that our machine learning approach learns investment strategies that considerably outperform existing SPT strategies in the US stock market.},
  citeulike-article-id = {14332531},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2777631},
  groups               = {Machine learning and investment strategies, ML_InvestSelect},
  posted-at            = {2017-04-05 21:07:53},
  timestamp            = {2020-02-25 23:16},
}

@Article{Liang-et-al-2018d,
  author         = {Liang, Zhipeng and Chen, Hao and Zhu, Junhao and Jiang, Kangkang and Li, Yanran},
  date           = {2018-08-29},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Adversarial Deep Reinforcement Learning in Portfolio Management},
  url            = {https://arxiv.org/abs/1808.09940},
  abstract       = {In this paper, we implement three state-of-art continuous reinforcement learning algorithms, Deep Deterministic Policy Gradient (DDPG), Proximal Policy Optimization (PPO) and Policy Gradient (PG)in portfolio management. All of them are widely-used in game playing and robot control. What's more, PPO has appealing theoretical propeties which is hopefully potential in portfolio management. We present the performances of them under different settings, including different learning rates, objective functions, feature combinations, in order to provide insights for parameters tuning, features selection and data preparation. We also conduct intensive experiments in China Stock market and show that PG is more desirable in financial market than DDPG and PPO, although both of them are more advanced. What's more, we propose a so called Adversarial Training method and show that it can greatly improve the training efficiency and significantly promote average daily return and sharpe ratio in back test. Based on this new modification, our experiments results show that our agent based on Policy Gradient can outperform UCRP.},
  day            = {29},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:16},
}

@Article{Li-et-al-2018i,
  author         = {Li, Xiaodong and Xie, Haoran and Lau, Raymond Y. K. and Wong, Tak-Lam and Wang, Fu-Lee},
  date           = {2018},
  journaltitle   = {IEEE access : practical innovations, open solutions},
  title          = {Stock prediction via sentimental transfer learning},
  doi            = {10.1109/{ACCE\SS}.2018.2881689},
  issn           = {2169-3536},
  pages          = {73110--73118},
  url            = {https://ieeexplore.ieee.org/document/8537894/},
  urldate        = {2019-10-05},
  volume         = {6},
  abstract       = {Stock prediction is always an attractive problem. With the expansion of information sources, news-driven stock prediction based on sentiments of social media, such as sentiment polarities in financial news, becomes more and more popular. However, the distributions of news articles among different stocks are skewed, which makes stocks with few news have few training samples for their prediction models, and thus leads to low prediction accuracy in the stock predictions. To address this problem, we propose sentimental transfer learning, which transfers sentimental information learned from news-rich stocks (source) to the news-poor ones (target), and prediction performances of the later ones are, therefore, improved. In this approach, the financial news articles of both the source and target stocks are first mapped into the same feature space that is constructed by sentiment dimensions. Second, we develop three different transfer principles in order to explore different transfer scenarios: 1) the source and target stocks historical price time series are highly correlated; 2) the source and target stocks are in the same sector and the former is the most news-rich one in the sector; and 3) the source stock has the highest prediction performance in validation data set. Third, a majority voting mechanism is designed based on the principles. The voting mechanism is to select the most proper source stock from the candidate stocks that are generated by different principles. Stock predictions are finally made based on the prediction models trained on the selected stocks. Experiments are conducted based on the data of Hong Kong Stock Exchange stocks from 2003 to 2008. The empirical results show that sentiment transfer learning can improve the prediction performance of the target stocks, and the performances are better and more stable with the source stocks selected by the voting mechanism.},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:16},
}

@Article{Li-et-al-2019i,
  author         = {Li, Xinyi and Li, Yinchuan and Zhan, Yuancheng and Liu, Xiao-Yang},
  date           = {2019-06-21},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Optimistic Bull or Pessimistic Bear: Adaptive Deep Reinforcement Learning for Stock Portfolio Allocation},
  url            = {https://arxiv.org/abs/1907.01503},
  urldate        = {2019-08-28},
  abstract       = {Portfolio allocation is crucial for investment companies. However, getting the best strategy in a complex and dynamic stock market is challenging. In this paper, we propose a novel Adaptive Deep Deterministic Reinforcement Learning scheme (Adaptive DDPG) for the portfolio allocation task, which incorporates optimistic or pessimistic deep reinforcement learning that is reflected in the influence from prediction errors. Dow Jones 30 component stocks are selected as our trading stocks and their daily prices are used as the training and testing data. We train the Adaptive DDPG agent and obtain a trading strategy. The Adaptive DDPG's performance is compared with the vanilla DDPG, Dow Jones Industrial Average index and the traditional min-variance and mean-variance portfolio allocation strategies. Adaptive DDPG outperforms the baselines in terms of the investment return and the Sharpe ratio.},
  day            = {21},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:16},
}

@Article{Ludwig-Piovoso-2005,
  author         = {Ludwig, Robert S. and Piovoso, Michael J.},
  date           = {2005-07},
  journaltitle   = {Intelligent Systems in Accounting, Finance and Management},
  title          = {A comparison of machine-learning classifiers for selecting money managers},
  doi            = {10.1002/isaf.262},
  issn           = {1055-{615X}},
  number         = {3},
  pages          = {151--164},
  urldate        = {2019-09-10},
  volume         = {13},
  f1000-projects = {QuantInvest},
  groups         = {ML_InvestSelect},
  timestamp      = {2020-02-25 23:16},
}

@Article{Lwin-et-al-2014,
  author               = {Lwin, Khin and Qu, Rong and Kendall, Graham},
  date                 = {2014-11},
  journaltitle         = {Applied Soft Computing},
  title                = {A learning-guided multi-objective evolutionary algorithm for constrained portfolio optimization},
  doi                  = {10.1016/j.asoc.2014.08.026},
  issn                 = {1568-4946},
  pages                = {757--772},
  volume               = {24},
  abstract             = {A learning-guided multi-objective evolutionary algorithm for constrained portfolio optimization problem is proposed. Four practical constraints, cardinality, quantity, pre-assignment and round lot, are considered. Performance wise, the proposed algorithm is not only capable to deliver high-quality portfolios enriched by additional constraints but also able to efficiently solve a reasonable size of asset up to 1318. It significantly outperforms the existing state-of-the-art algorithms. Portfolio optimization involves the optimal assignment of limited capital to different available financial assets to achieve a reasonable trade-off between profit and risk objectives. In this paper, we studied the extended Markowitz's mean-variance portfolio optimization model. We considered the cardinality, quantity, pre-assignment and round lot constraints in the extended model. These four real-world constraints limit the number of assets in a portfolio, restrict the minimum and maximum proportions of assets held in the portfolio, require some specific assets to be included in the portfolio and require to invest the assets in units of a certain size respectively. An efficient learning-guided hybrid multi-objective evolutionary algorithm is proposed to solve the constrained portfolio optimization problem in the extended mean-variance framework. A learning-guided solution generation strategy is incorporated into the multi-objective optimization process to promote the efficient convergence by guiding the evolutionary search towards the promising regions of the search space. The proposed algorithm is compared against four existing state-of-the-art multi-objective evolutionary algorithms, namely Non-dominated Sorting Genetic Algorithm (NSGA-II), Strength Pareto Evolutionary Algorithm (SPEA-2), Pareto Envelope-based Selection Algorithm (PESA-II) and Pareto Archived Evolution Strategy (PAES). Computational results are reported for publicly available OR-library datasets from seven market indices involving up to 1318 assets. Experimental results on the constrained portfolio optimization problem demonstrate that the proposed algorithm significantly outperforms the four well-known multi-objective evolutionary algorithms with respect to the quality of obtained efficient frontier in the conducted experiments.},
  citeulike-article-id = {14160055},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.asoc.2014.08.026},
  groups               = {Machine learning and investment strategies, ML_NumOptimiz, ML_InvestSelect},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 12:22:54},
  timestamp            = {2020-02-25 23:16},
}

@Article{Lwin-Ku-2013,
  author               = {Lwin, Khin and Qu, Rong},
  date                 = {2013},
  journaltitle         = {Applied Intelligence},
  title                = {A hybrid algorithm for constrained portfolio selection problems},
  doi                  = {10.1007/s10489-012-0411-7},
  number               = {2},
  pages                = {251--266},
  volume               = {39},
  abstract             = {Since Markowitz's seminal work on the mean-variance model in modern portfolio theory, many studies have been conducted on computational techniques and recently meta-heuristics for portfolio selection problems. In this work, we propose and investigate a new hybrid algorithm integrating the population based incremental learning and differential evolution algorithms for the portfolio selection problem. We consider the extended mean-variance model with practical trading constraints including the cardinality, floor and ceiling constraints. The proposed hybrid algorithm adopts a partially guided mutation and an elitist strategy to promote the quality of solution. The performance of the proposed hybrid algorithm has been evaluated on the extended benchmark datasets in the OR Library. The computational results demonstrate that the proposed hybrid algorithm is not only effective but also efficient in solving the mean-variance model with real world constraints.},
  citeulike-article-id = {14160054},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10489-012-0411-7},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10489-012-0411-7},
  groups               = {ML_InvestSelect},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 12:21:32},
  publisher            = {Springer US},
  timestamp            = {2020-02-25 23:16},
}

@Article{Malandri-et-al-2018,
  author         = {Malandri, Lorenzo and Xing, Frank Z. and Orsenigo, Carlotta and Vercellis, Carlo and Cambria, Erik},
  date           = {2018-11-26},
  journaltitle   = {Cognitive Computation},
  title          = {Public mood-driven asset allocation: the importance of financial sentiment in portfolio management},
  doi            = {10.1007/s12559-018-9609-2},
  issn           = {1866-9956},
  abstract       = {The study of the impact of investor sentiment on stock returns has gained increasing momentum in the past few years. It has been widely accepted that public mood is correlated with financial markets. However, only a few studies discussed how the public mood would affect one of the fundamental problems of computational finance: portfolio management. In this study, we use public financial sentiment and historical prices collected from the New York Stock Exchange (NYSE) to train multiple machine learning models for automatic wealth allocation across a set of assets. Unlike previous studies which set as target variable the asset prices in the portfolio, the variable to predict here is represented by the best asset allocation strategy ex post. Experiments performed on five portfolios show that long short-term memory networks are superior to multi-layer perceptron and random forests producing, in the period under analysis, an average increase in the revenue across the portfolios ranging between 5\% (without financial mood) and 19\% (with financial mood) compared to the equal-weighted portfolio. Results show that our all-in-one and end-to-end approach for automatic portfolio selection outperforms the equal-weighted portfolio. Moreover, when using long short-term memory networks, the employment of sentiment data in addition to lagged data leads to greater returns for all the five portfolios under evaluation. Finally, we find that among the employed machine learning algorithms, long short-term memory networks are better suited for learning the impact of public mood on financial time series.},
  day            = {26},
  f1000-projects = {QuantInvest},
  groups         = {ML_AssetPricing, ML_InvestSelect, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:16},
}

@InCollection{Moeini-et-al-2016,
  author               = {Moeini, Mahdi and Wendt, Oliver and Krumrey, Linus},
  booktitle            = {Intelligent Information and Database Systems},
  date                 = {2016},
  title                = {Portfolio Optimization by Means of a chi-Armed Bandit Algorithm},
  doi                  = {10.1007/978-3-662-49390-8\_60},
  editor               = {Nguyen, Ngoc Thanh and Trawinski, Bogdan and Fujita, Hamido and Hong, Tzung-Pei},
  pages                = {620--629},
  publisher            = {Springer Berlin Heidelberg},
  series               = {Lecture Notes in Computer Science},
  volume               = {9622},
  abstract             = {In this paper, we are interested in studying and solving the portfolio selection problem by means of a machine learning method. Particularly, we use a chi-armed bandit algorithm called Hierarchical Optimistic Optimization (HOO). HOO is an optimization approach that can be used for finding optima of box constrained nonlinear and nonconvex functions. Under some restrictions, such as locally Lipschitz condition, HOO can provide global solutions. Our idea consists in using HOO for solving some NP-hard variants of the portfolio selection problem. We test this approach on some data sets and report the results. In order to verify the quality of the solutions, we compare them with the best known solutions, provided by a derivative-free approach, called DIRECT. The preliminary numerical experiments give promising results.},
  citeulike-article-id = {14320246},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-662-49390-860},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-662-49390-860},
  groups               = {ML_NumOptimiz, ML_InvestSelect},
  posted-at            = {2017-03-26 16:46:18},
  timestamp            = {2020-02-25 23:16},
}

@Article{Obeidat-et-al-2018,
  author         = {Obeidat, Samer and Shapiro, Daniel and Lemay, Mathieu and MacPherson, Mary Kate and Bolic, Miodrag},
  date           = {2018-06-30},
  journaltitle   = {International Journal On Advances in Intelligent Systems},
  title          = {Adaptive Portfolio Asset Allocation Optimization with Deep Learning},
  url            = {http://www.thinkmind.org/index.php?view=article\&articleid=intsys\_v11\_n12\_2018\_3},
  urldate        = {2019-09-19},
  day            = {30},
  f1000-projects = {QuantInvest},
  groups         = {ML_NumOptimiz, ML_InvestSelect, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:16},
}

@MastersThesis{Olden-2016,
  author               = {Olden, Magnus},
  date                 = {2016},
  institution          = {University of Oslo},
  title                = {Predicting Stocks with Machine Learning: Stacked Classifiers and other Learners Applied to the Oslo Stock Exchange},
  url                  = {https://www.duo.uio.no/handle/10852/51275},
  abstract             = {This study aims to determine whether it is possible to make a profitable stock trading scheme using machine learning on the Oslo Stock Exchange (OSE). It compares binary classification learning algorithms and their performance. It investigates whether Stacked Ensemble Learning Algorithms, utilizing other learning algorithms predictions as additional features, outperforms other machine learning techniques. The experiments attempt to predict the daily movement of 22 stocks from OSE with 37 machine learning techniques, using selected data spanning over four years. The results shows that the top performing algorithms outperform Oslo Benchmark Index (OBX). However, several issues regarding the test period and stock prediction in general stops us from drawing an indisputable conclusion whether a long term profitable scheme is likely. The experiments yielded no evidence indicating that stacked ensemble learning outperforms other machine learning techniques.},
  citeulike-article-id = {14503588},
  groups               = {ML_Classif_QWIM, ML_PerfMetrics, ML_InvestSelect},
  posted-at            = {2017-12-15 22:21:22},
  timestamp            = {2020-02-25 23:16},
}

@Article{Aminghafari-Poggi-2007,
  author         = {Aminghafari, Mina and Poggi, Jean-Michel},
  date           = {2007-09},
  journaltitle   = {Int J Wavelets Multiresolut Inf Process},
  title          = {Forecasting Time Series Using Wavelets},
  doi            = {10.1142/S0219691307002002},
  issn           = {0219-6913},
  number         = {05},
  pages          = {709--724},
  volume         = {05},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs},
  timestamp      = {2020-02-25 23:18},
}

@TechReport{Arnott-et-al-2017d,
  author               = {Arnott, Rob and Beck, Noah and Kalesnik, Vitali},
  date                 = {2017},
  institution          = {Research Affiliates},
  title                = {Forecasting Factor and Smart Beta Returns (Hint: History Is Worse than Useless)},
  url                  = {https://www.researchaffiliates.com/en_us/publications/articles/595-forecasting-factor-and-smart-beta-returns.html},
  abstract             = {Using past performance to forecast future performance is likely to disappoint.

We find that a factor's most recent five-year performance is negatively correlated with its subsequent five-year performance.

By significantly extending the period of past performance used to forecast future performance, we can improve predictive ability, but the forecasts are still negatively correlated with subsequent performance: the forecast is still essentially useless!

Using relative valuations, we forecast the five-year expected alphas for a broad universe of smart beta strategies as a tool for managing expectations about current portfolios and constructing new portfolios positioned for future outperformance. These forecasts will be updated regularly and available on our website.

In a series of articles we published in 2016,1 we show that relative valuations predict subsequent returns for both factors and smart beta strategies in exactly the same way price matters in stock selection and asset allocation. To many, one surprising revelation in that series is that a number of "smart beta"strategies are expensive today relative to their historical valuations. The fact they are expensive has two uncomfortable implications. The first is that the past success of a smart beta strategy-often only a simulated past performance-is partly a consequence of "revaluation alpha"arising because many of these strategies enjoy a tailwind as they become more expensive. We, as investors, extrapolate that part of the historical alpha at our peril. The second implication is that any mean reversion toward the smart beta strategy's historical normal relative valuation could transform lofty historical alpha into negative future alpha. As with asset allocation and stock selection, relative valuations can predict the long-term future returns of strategies and factors-not precisely, nor with any meaningful short-term timing efficacy, but well enough to add material value. These findings are robust to variations in valuation metrics, geographies, and time periods used for estimation.},
  citeulike-article-id = {14322595},
  citeulike-linkout-0  = {https://www.researchaffiliates.com/enus/publications/articles/595-forecasting-factor-and-smart-beta-returns.html},
  groups               = {FrcstQWIM_TimeSrs, FrcstQWIM_ShortTerm, FrcstQWIM_MedLngTerm},
  posted-at            = {2017-03-29 15:43:04},
  timestamp            = {2020-02-25 23:18},
}

@Article{Ayed-et-al-2016,
  author               = {Bel Hadj Ayed, Ahmed and Loeper, Gregoire and Abergel, Frederic},
  date                 = {2016-07},
  journaltitle         = {Quantitative Finance},
  title                = {Forecasting trends with asset prices},
  doi                  = {10.1080/14697688.2016.1206959},
  pages                = {1--14},
  abstract             = {The question of interest in this paper is the estimation of the trend of a financial asset, and the impact of its misspecification on investment strategies. The setting we consider is that of a stochastic asset price model where the trend follows an unobservable Ornstein?Uhlenbeck process. Motivated by the use of Kalman filtering as a forecasting tool, we address the problem of parameter estimation, and measure the effect of parameter misspecification. Numerical examples illustrate the difficulty of trend forecasting in financial time series.},
  citeulike-article-id = {14150621},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2016.1206959},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1206959},
  day                  = {20},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-10-03 01:14:02},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 23:18},
}

@Article{Bao-et-al-2017,
  author               = {Bao, Wei and Yue, Jun and Rao, Yulei},
  date                 = {2017-07},
  journaltitle         = {PLOS ONE},
  title                = {A deep learning framework for financial time series using stacked autoencoders and long-short term memory},
  doi                  = {10.1371/journal.pone.0180944},
  number               = {7},
  pages                = {e0180944+},
  volume               = {12},
  abstract             = {The application of deep learning approaches to finance has received a great deal of attention from both investors and researchers. This study presents a novel deep learning framework where wavelet transforms (WT), stacked autoencoders (SAEs) and long-short term memory (LSTM) are combined for stock price forecasting. The SAEs for hierarchically extracted deep features is introduced into stock price forecasting for the first time. The deep learning framework comprises three stages. First, the stock price time series is decomposed by WT to eliminate noise. Second, SAEs is applied to generate deep high-level features for predicting the stock price. Third, high-level denoising features are fed into LSTM to forecast the next day's closing price. Six market indices and their corresponding index futures are chosen to examine the performance of the proposed model. Results show that the proposed model outperforms other similar models in both predictive accuracy and profitability performance.},
  citeulike-article-id = {14417331},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0180944},
  day                  = {14},
  groups               = {FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_ForcstTimeSrs, DeepLearning_QWIM},
  posted-at            = {2017-12-11 00:46:33},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-25 23:18},
}

@Article{Barrow-Crone-2016,
  author               = {Barrow, Devon K. and Crone, Sven F.},
  date                 = {2016-10},
  journaltitle         = {International Journal of Forecasting},
  title                = {Cross-validation aggregation for combining autoregressive neural network forecasts},
  doi                  = {10.1016/j.ijforecast.2015.12.011},
  issn                 = {0169-2070},
  number               = {4},
  pages                = {1120--1137},
  volume               = {32},
  abstract             = {This paper evaluates kk-fold and Monte Carlo cross-validation and aggregation (crogging) for combining neural network autoregressive forecasts. We introduce Monte Carlo crogging which combines bootstrapping and cross-validation (CV) in a single approach through repeated random splitting of the original time series into mutually exclusive datasets for training. As the training/validation split is independent of the number of folds, the algorithm offers more flexibility in the size, and number of training samples compared to kk-fold cross-validation. The study also provides for crogging and bagging: (1) the first systematic evaluation across time series length and combination size, (2) a bias and variance decomposition of the forecast errors to understand improvement gains, and (3) a comparison to established benchmarks of model averaging and selection. Crogging can easily be extended to other autoregressive models. Results on real and simulated series demonstrate significant improvements in forecasting accuracy especially for short time series and long forecast horizons.},
  citeulike-article-id = {14071841},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2015.12.011},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-06-18 18:56:15},
  timestamp            = {2020-02-25 23:18},
}

@Article{Bergmeir-et-al-2012,
  author               = {Bergmeir, C. and Triguero, I. and Molina, D. and Aznarte, J. L. and Benitez, J. M.},
  date                 = {2012-11},
  journaltitle         = {IEEE Transactions on Neural Networks and Learning Systems},
  title                = {Time Series Modeling and Forecasting Using Memetic Algorithms for Regime-Switching Models},
  doi                  = {10.1109/tnnls.2012.2216898},
  issn                 = {2162-237X},
  number               = {11},
  pages                = {1841--1847},
  volume               = {23},
  abstract             = {In this brief, we present a novel model fitting procedure for the neuro-coefficient smooth transition autoregressive model (NCSTAR), as presented by Medeiros and Veiga. The model is endowed with a statistically founded iterative building procedure and can be interpreted in terms of fuzzy rule-based systems. The interpretability of the generated models and a mathematically sound building procedure are two very important properties of forecasting models. The model fitting procedure employed by the original NCSTAR is a combination of initial parameter estimation by a grid search procedure with a traditional local search algorithm. We propose a different fitting procedure, using a memetic algorithm, in order to obtain more accurate models. An empirical evaluation of the method is performed, applying it to various real-world time series originating from three forecasting competitions. The results indicate that we can significantly enhance the accuracy of the models, making them competitive to models commonly used in the field.},
  citeulike-article-id = {13995926},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tnnls.2012.2216898},
  citeulike-linkout-1  = {http://ieeexplore.ieee.org/xpls/absall.jsp?arnumber=6323088},
  groups               = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  institution          = {Department of Computer Science and Artificial Intelligence, CITIC-UGR, University of Granada, Granada, Spain},
  owner                = {cristi},
  posted-at            = {2016-04-04 02:52:51},
  publisher            = {IEEE},
  timestamp            = {2020-02-25 23:18},
}

@Article{Bergmeir-et-al-2018,
  author         = {Bergmeir, Christoph and Hyndman, Rob J. and Koo, Bonsoo},
  date           = {2018-04},
  journaltitle   = {Computational Statistics \& Data Analysis},
  title          = {A note on the validity of cross-validation for evaluating autoregressive time series prediction},
  doi            = {10.1016/j.csda.2017.11.003},
  issn           = {0167-9473},
  pages          = {70--83},
  volume         = {120},
  abstract       = {One of the most widely used standard procedures for model evaluation in classification and regression is K fold cross-validation (CV). However, when it comes to time series forecasting, because of the inherent serial correlation and potential non-stationarity of the data, its application is not straightforward and often replaced by practitioners in favour of an out-of-sample (OOS) evaluation. It is shown that for purely autoregressive models, the use of standard K fold CV is possible provided the models considered have uncorrelated errors. Such a setup occurs, for example, when the models nest a more appropriate model. This is very common when Machine Learning methods are used for prediction, and where CV can control for overfitting the data. Theoretical insights supporting these arguments are presented, along with a simulation study and a real-world example. It is shown empirically that K fold CV performs favourably compared to both OOS evaluation and other time-series-specific techniques such as non-dependent cross-validation.},
  f1000-projects = {QuantInvest},
  groups         = {ML_Classif_QWIM, FrcstQWIM_TimeSrs, NonStatry_FinTimeSrs, ML_Test_CrossVal, ML_Validation, ML_Overfitting, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:18},
}

@Article{Bianchi-Tamoni-2016,
  author               = {Bianchi, Daniele and Tamoni, Andrea},
  date                 = {2016-11},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The Dynamics of Expected Returns: Evidence from Multi-Scale Time Series Modeling},
  url                  = {https://ssrn.com/abstract=2684728},
  abstract             = {Conventional wisdom posits that all the relevant investors' information lies at the highest possible frequency of observation, so that long-run expected returns can be mechanically inferred by a forward aggregation of short-run estimates. We reverse such logic and propose a novel framework to model and extract the dynamics of latent short-term expected returns by coherently combining the lower-frequency information embedded in multiple predictors. We show that the information cascade from low- to high-frequency levels allows to identify long-lasting effects on expected returns that cannot be captured by standard persistent ARMA processes. The empirical analysis demonstrates that the ability of the model to capture simultaneously medium- to long-term fluctuations in the dynamics of expected returns, has first order implications for forecasting and investment decisions.},
  citeulike-article-id = {14063988},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2684728},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2774387code1209422.pdf?abstractid=2684728 and mirid=1},
  day                  = {3},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-06-09 18:34:40},
  timestamp            = {2020-02-25 23:18},
}

@Article{Billio-et-al-2013a,
  author               = {Billio, Monica and Casarin, Roberto and Ravazzolo, Francesco and van Dijk, Herman K.},
  date                 = {2013-12},
  journaltitle         = {Journal of Econometrics},
  title                = {Time-varying combinations of predictive densities using nonlinear filtering},
  doi                  = {10.1016/j.jeconom.2013.04.009},
  issn                 = {0304-4076},
  number               = {2},
  pages                = {213--232},
  volume               = {177},
  abstract             = {We propose a Bayesian combination approach for multivariate predictive densities which relies upon a distributional state space representation of the combination weights. Several specifications of multivariate time-varying weights are introduced with a particular focus on weight dynamics driven by the past performance of the predictive densities and the use of learning mechanisms. In the proposed approach the model set can be incomplete, meaning that all models can be individually misspecified. A Sequential Monte Carlo method is proposed to approximate the filtering and predictive densities. The combination approach is assessed using statistical and utility-based performance measures for evaluating density forecasts of simulated data, US macroeconomic time series and surveys of stock market prices. Simulation results indicate that, for a set of linear autoregressive models, the combination strategy is successful in selecting, with probability close to one, the true model when the model set is complete and it is able to detect parameter instability when the model set includes the true model that has generated subsamples of data. Also, substantial uncertainty appears in the weights when predictors are similar; residual uncertainty reduces when the model set is complete; and learning reduces this uncertainty. For the macro series we find that incompleteness of the models is relatively large in the 1970's, the beginning of the 1980's and during the recent financial crisis, and lower during the Great Moderation; the predicted probabilities of recession accurately compare with the NBER business cycle dating; model weights have substantial uncertainty attached. With respect to returns of the SandP 500 series, we find that an investment strategy using a combination of predictions from professional forecasters and from a white noise model puts more weight on the white noise model in the beginning of the 1990's and switches to giving more weight to the professional forecasts over time. Information on the complete predictive distribution and not just on some moments turns out to be very important, above all during turbulent times such as the recent financial crisis. More generally, the proposed distributional state space representation offers great flexibility in combining densities.},
  citeulike-article-id = {13989281},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jeconom.2013.04.009},
  groups               = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-03-28 03:48:20},
  timestamp            = {2020-02-25 23:18},
}

@InCollection{Binner-et-al-2004,
  author               = {Binner, Jane M. and Elger, Thomas and Nilsson, Birger and Tepper, Jonathan A.},
  booktitle            = {Volume 19, Applications of Artificial Intelligence in Finance and Economics},
  date                 = {2004},
  title                = {Tools for non-linear time series forecasting in economics - an empirical comparison of regime switching vector autoregressive models and recurrent neural networks},
  doi                  = {10.1016/s0731-9053(04)19003-8},
  isbn                 = {0-7623-1150-9},
  location             = {Bingley},
  pages                = {71--91},
  publisher            = {Emerald (MCB UP )},
  volume               = {19},
  abstract             = {The purpose of this study is to contrast the forecasting performance of two non-linear models, a regime-switching vector autoregressive model (RS-VAR) and a recurrent neural network (RNN), to that of a linear benchmark VAR model. Our specific forecasting experiment is U.K. inflation and we utilize monthly data from 1969 to 2003. The RS-VAR and the RNN perform approximately on par over both monthly and annual forecast horizons. Both non-linear models perform significantly better than the VAR model.},
  citeulike-article-id = {13995905},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/s0731-9053(04)19003-8},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-04-04 02:20:24},
  timestamp            = {2020-02-25 23:18},
}

@Article{Bisaglia-Gerolimetto-2008,
  author               = {Bisaglia, Luisa and Gerolimetto, Margherita},
  date                 = {2008-03},
  journaltitle         = {Economics Letters},
  title                = {Forecasting long memory time series when occasional breaks occur},
  doi                  = {10.1016/j.econlet.2007.05.001},
  issn                 = {0165-1765},
  number               = {3},
  pages                = {253--258},
  volume               = {98},
  abstract             = {In this paper, in order to investigate if a long memory model will provide good forecasts even if the real DGP is affected by level shifts (as suggested by Diebold and Inoue 2001: Long memory and regime switching, Journal of Econometrics) we compare via simulations the forecasting performance of long memory and occasional breaks processes.},
  citeulike-article-id = {13995912},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.econlet.2007.05.001},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-04-04 02:30:59},
  timestamp            = {2020-02-25 23:18},
}

@Article{Bisaglia-Grigoletto-2018,
  author         = {Bisaglia, Luisa and Grigoletto, Matteo},
  date           = {2018-12-18},
  journaltitle   = {arXiv Electronic Journal},
  title          = {A new time-varying model for forecasting long-memory series},
  url            = {https://arxiv.org/abs/1812.07295},
  abstract       = {In this work we propose a new class of long-memory models with tim-varying fractional parameter. In particular, the dynamics of the long-memory coefficient, dd, is specified through a stochastic recurrence equation driven by the score of the predictive likelihood, as suggested by Creal et al (2013) and Harvey (2013). We demonstrate the validity of the proposed model by a Monte Carlo experiment and an application to two real time series.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs},
  timestamp      = {2020-02-25 23:18},
}

@Article{Borovykh-et-al-2017,
  author               = {Borovykh, Anastasia and Bohte, Sander and Oosterlee, Cornelis W.},
  date                 = {2017-10},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Conditional Time Series Forecasting with Convolutional Neural Networks},
  eprint               = {1703.04691},
  url                  = {https://arxiv.org/abs/1703.04691},
  abstract             = {We present a method for conditional time series forecasting based on the recent deep convolutional WaveNet architecture. The proposed network contains stacks of dilated convolutions that allow it to access a broad range of history when forecasting; multiple convolutional filters are applied in parallel to separate time series and allow for the fast processing of data and the exploitation of the correlation structure between the multivariate time series. The performance of the deep convolutional neural network is analyzed on various multivariate time series and compared to that of the well-known autoregressive model and a long-short term memory network. We show that our network is able to effectively learn dependencies between the series without the need for long historical time series and can outperform the baseline neural forecasting models.},
  citeulike-article-id = {14500288},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.04691},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.04691},
  day                  = {16},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-12-11 00:53:52},
  timestamp            = {2020-02-25 23:18},
}

@Article{Burns-et-al-2018,
  author         = {Burns, David M. and Whyne, Cari M.},
  date           = {2018-03-21},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Seglearn: A Python Package for Learning Sequences and Time Series},
  url            = {https://arxiv.org/abs/1803.08118v2},
  abstract       = {Seglearn is an open-source python package for machine learning time series or sequences using a sliding window segmentation approach. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. This package is compatible with scikit-learn and is listed under scikit-learn Related Projects. The package depends on numpy, scipy, and scikit-learn. Seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage.},
  day            = {21},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:18},
}

@InCollection{Castellani-dosSantos-2014,
  author               = {Castellani, Marco and dos Santos, EmanuelA},
  booktitle            = {Innovations in Intelligent Machines-4},
  date                 = {2014},
  title                = {Prediction of Long-Term Government Bond Yields Using Statistical and Artificial Intelligence Methods},
  doi                  = {10.1007/978-3-319-01866-9\_11},
  editor               = {Faucher, Colette and Jain, Lakhmi C.},
  pages                = {341--367},
  publisher            = {Springer International Publishing},
  series               = {Studies in Computational Intelligence},
  volume               = {514},
  abstract             = {This chapter investigates the use of different artificial intelligence and classical techniques for forecasting the monthly yield of the US 10-year Treasury bonds from a set of four economic indicators. The task is particularly challenging due to the sparseness of the data samples and the complex interactions amongst the variables. At the same time, it is of high significance because of the important and paradigmatic role played by the US market in the world economy. Four data-driven artificial intelligence approaches are considered: a manually built fuzzy logic model, a machine learned fuzzy logic model, a self-organising map model, and a multi-layer perceptron model. Their prediction accuracy is compared with that of two classical approaches: a statistical ARIMA model and an econometric error correction model. The algorithms are evaluated on a complete series of end-month US 10-year Treasury bonds yields and economic indicators from 1986:1 to 2004:12. In terms of prediction accuracy and reliability, the best results are obtained by the three parametric regression algorithms, namely the econometric, the statistical, and the multi-layer perceptron model. Due to the sparseness of the learning data samples, the manual and the automatic fuzzy logic approaches fail to follow with adequate precision the range of variations of the US 10-year Treasury bonds. For similar reasons, the self-organising map model performs unsatisfactorily. Analysis of the results indicates that the econometric model has a slight edge over the statistical and the multi-layer perceptron models. This suggests that pure data-driven induction may not fully capture the complicated mechanisms ruling the changes in interest rates. Overall, the prediction accuracy of the best models is only marginally better than the prediction accuracy of a basic one-step lag predictor. This result highlights the difficulty of the modelling task and, in general, the difficulty of building reliable predictors for financial markets.},
  citeulike-article-id = {14503477},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-01866-911},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-01866-911},
  groups               = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  posted-at            = {2017-12-15 16:28:25},
  timestamp            = {2020-02-25 23:18},
}

@InCollection{Cerqueira-et-al-2017,
  author         = {Cerqueira, Vitor and Torgo, Luis and Pinto, Fabio and Soares, Carlos},
  booktitle      = {Machine learning and knowledge discovery in databases},
  date           = {2017},
  title          = {Arbitrated ensemble for time series forecasting},
  doi            = {10.1007/978-3-319-71246-8\_29},
  editor         = {Ceci, Michelangelo and Hollmen, Jaakko and Todorovski, Ljupco and Vens, Celine and Dzeroski, Saso},
  isbn           = {978-3-319-71245-1},
  pages          = {478--494},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  volume         = {10535},
  abstract       = {This paper proposes an ensemble method for time series forecasting tasks. Combining different forecasting models is a common approach to tackle these problems. State-of-the-art methods track the loss of the available models and adapt their weights accordingly. Metalearning strategies such as stacking are also used in these tasks. We propose a metalearning approach for adaptively combining forecasting models that specializes them across the time series. Our assumption is that different forecasting models have different areas of expertise and a varying relative performance. Moreover, many time series show recurring structures due to factors such as seasonality. Therefore, the ability of a method to deal with changes in relative performance of models as well as recurrent changes in the data distribution can be very useful in dynamic environments. Our approach is based on an ensemble of heterogeneous forecasters, arbitrated by a metalearning model. This strategy is designed to cope with the different dynamics of time series and quickly adapt the ensemble to regime changes. We validate our proposal using time series from several real world domains. Empirical results show the competitiveness of the method in comparison to state-of-the-art approaches for combining forecasters.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, Scenario_ExpertView, ML_ForcstTimeSrs},
  issn           = {0302-9743},
  timestamp      = {2020-02-25 23:18},
}

@Article{Chang-et-al-2018b,
  author         = {Chang, Yen-Yu and Sun, Fan-Yun and Wu, Yueh-Hua and Lin, Shou-De},
  date           = {2018-09-06},
  journaltitle   = {arXiv Electronic Journal},
  title          = {A Memory-Network Based Solution for Multivariate Time-Series Forecasting},
  url            = {https://arxiv.org/abs/1809.02105},
  urldate        = {2018-09-10},
  abstract       = {Multivariate time series forecasting is extensively studied throughout the years with ubiquitous applications in areas such as finance, traffic, environment, etc. Still, concerns have been raised on traditional methods for incapable of modeling complex patterns or dependencies lying in real word data. To address such concerns, various deep learning models, mainly Recurrent Neural Network (RNN) based methods, are proposed. Nevertheless, capturing extremely long-term patterns while effectively incorporating information from other variables remains a challenge for time-series forecasting. Furthermore, lack-of-explainability remains one serious drawback for deep neural network models. Inspired by Memory Network proposed for solving the question-answering task, we propose a deep learning based model named Memory Time-series network (MTNet) for time series forecasting. MTNet consists of a large memory component, three separate encoders, and an autoregressive component to train jointly. Additionally, the attention mechanism designed enable MTNet to be highly interpretable. We can easily tell which part of the historic data is referenced the most.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:18},
}

@Article{Cheng-et-al-2015,
  author               = {Cheng, Changqing and {Sa Ngasoongsong}, Akkarapol and Beyca, Omer and Le, Trung and Yang, Hui and Kong, Zhenyu J. and Bukkapatnam, Satish T. S.},
  date                 = {2015-10},
  journaltitle         = {IIE Transactions},
  title                = {Time series forecasting for nonlinear and non-stationary processes: a review and comparative study},
  doi                  = {10.1080/0740817x.2014.999180},
  number               = {10},
  pages                = {1053--1071},
  volume               = {47},
  abstract             = {Forecasting the evolution of complex systems is noted as one of the 10 grand challenges of modern science. Time series data from complex systems capture the dynamic behaviors and causalities of the underlying processes and provide a tractable means to predict and monitor system state evolution. However, the nonlinear and non-stationary dynamics of the underlying processes pose a major challenge for accurate forecasting.

For most real-world systems, the vector field of state dynamics is a nonlinear function of the state variables; i.e., the relationship connecting intrinsic state variables with their autoregressive terms and exogenous variables is nonlinear. Time series emerging from such complex systems exhibit aperiodic (chaotic) patterns even under steady state. Also, since real-world systems often evolve under transient conditions, the signals obtained therefrom tend to exhibit myriad forms of non-stationarity. Nonetheless, methods reported in the literature focus mostly on forecasting linear and stationary processes.

This article presents a review of these advancements in nonlinear and non-stationary time series forecasting models and a comparison of their performances in certain real-world manufacturing and health informatics applications. Conventional approaches do not adequately capture the system evolution (from the standpoint of forecasting accuracy, computational effort, and sensitivity to quantity and quality of a priori information) in these applications.},
  citeulike-article-id = {13922887},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/0740817x.2014.999180},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/0740817X.2014.999180},
  day                  = {3},
  groups               = {FrcstQWIM_TimeSrs, NonStatry_FinTimeSrs, Data_NonStationary},
  owner                = {zkgst0c},
  posted-at            = {2016-01-31 17:53:15},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-25 23:18},
}

@Article{Chicheportiche-Chakraborti-2017,
  author               = {Chicheportiche, R?my and Chakraborti, Anirban},
  date                 = {2017-05},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {A model-free characterization of recurrences in stationary time series},
  doi                  = {10.1016/j.physa.2017.01.073},
  issn                 = {0378-4371},
  pages                = {312--318},
  volume               = {474},
  abstract             = {Copula is a natural model-free framework to study non-linear dependencies like recurrences. Non-linear dependencies do impact both the statistics and dynamics of recurrence times. Scaling arguments for the unconditional distribution may not be applicable. Fitting and/or simulating the intertemporal distribution of recurrence intervals is very much system specific. Study of recurrences in earthquakes, climate, financial time-series, etc. is crucial to better forecast disasters and limit their consequences. Most of the previous phenomenological studies of recurrences have involved only a long-ranged autocorrelation function, and ignored the multi-scaling properties induced by potential higher order dependencies. We argue that copulas is a natural model-free framework to study non-linear dependencies in time series and related concepts like recurrences. Consequently, we arrive at the facts that (i) non-linear dependences do impact both the statistics and dynamics of recurrence times, and (ii) the scaling arguments for the unconditional distribution may not be applicable. Hence, fitting and/or simulating the intertemporal distribution of recurrence intervals is very much system specific, and cannot actually benefit from universal features, in contrast to the previous claims. This has important implications in epilepsy prognosis and financial risk management applications.},
  citeulike-article-id = {14388429},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2017.01.073},
  groups               = {FrcstQWIM_TimeSrs, Data_NonStationary},
  posted-at            = {2017-07-05 16:20:51},
  timestamp            = {2020-02-25 23:18},
}

@Article{Chliamovitch-et-al-2015,
  author               = {Chliamovitch, G. and Dupuis, A. and Golub, A. and Chopard, B.},
  date                 = {2015-04},
  journaltitle         = {EPL (Europhysics Letters)},
  title                = {Improving predictability of time series using maximum entropy methods},
  doi                  = {10.1209/0295-5075/110/10003},
  issn                 = {0295-5075},
  number               = {1},
  pages                = {10003+},
  volume               = {110},
  abstract             = {We discuss how maximum entropy methods may be applied to the reconstruction of Markov processes underlying empirical time series and compare this approach to usual frequency sampling. It is shown that, at least in low dimension, there exists a subset of the space of stochastic matrices for which the MaxEnt method is more efficient than sampling, in the sense that shorter historical samples have to be considered to reach the same accuracy. Considering short samples is of particular interest when modelling smoothly non-stationary processes, for then it provides, under some conditions, a powerful forecasting tool. The method is illustrated for a discretized empirical series of exchange rates.},
  citeulike-article-id = {14398886},
  citeulike-linkout-0  = {http://dx.doi.org/10.1209/0295-5075/110/10003},
  day                  = {01},
  groups               = {Predictability_FinInfo, FrcstQWIM_TimeSrs},
  posted-at            = {2017-07-24 19:01:36},
  timestamp            = {2020-02-25 23:18},
}

@Article{Clements-et-al-2004,
  author               = {Clements, Michael P. and Franses, Philip H. and Swanson, Norman R.},
  date                 = {2004-04},
  journaltitle         = {International Journal of Forecasting},
  title                = {Forecasting economic and financial time-series with non-linear models},
  doi                  = {10.1016/j.ijforecast.2003.10.004},
  issn                 = {0169-2070},
  number               = {2},
  pages                = {169--183},
  volume               = {20},
  abstract             = {In this paper we discuss the current state-of-the-art in estimating, evaluating, and selecting among non-linear forecasting models for economic and financial time series. We review theoretical and empirical issues, including predictive density, interval and point evaluation and model selection, loss functions, data-mining, and aggregation. In addition, we argue that although the evidence in favor of constructing forecasts using non-linear models is rather sparse, there is reason to be optimistic. However, much remains to be done. Finally, we outline a variety of topics for future research, and discuss a number of areas which have received considerable attention in the recent literature, but where many questions remain.},
  citeulike-article-id = {13995869},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2003.10.004},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-04-04 01:03:07},
  timestamp            = {2020-02-25 23:18},
}

@Article{CorberanVallet-et-al-2011,
  author               = {Corberan-Vallet, Ana and Bermudez, Jose D. and Vercher, Enriqueta},
  date                 = {2011-04},
  journaltitle         = {International Journal of Forecasting},
  title                = {Forecasting correlated time series with exponential smoothing models},
  doi                  = {10.1016/j.ijforecast.2010.06.003},
  issn                 = {0169-2070},
  number               = {2},
  pages                = {252--265},
  volume               = {27},
  abstract             = {This paper presents the Bayesian analysis of a general multivariate exponential smoothing model that allows us to forecast time series jointly, subject to correlated random disturbances. The general multivariate model, which can be formulated as a seemingly unrelated regression model, includes the previously studied homogeneous multivariate Holt-Winters' model as a special case when all of the univariate series share a common structure.

MCMC simulation techniques are required in order to approach the non-analytically tractable posterior distribution of the model parameters. The predictive distribution is then estimated using Monte Carlo integration. A Bayesian model selection criterion is introduced into the forecasting scheme for selecting the most adequate multivariate model for describing the behaviour of the time series under study.

The forecasting performance of this procedure is tested using some real examples.},
  citeulike-article-id = {7918976},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2010.06.003},
  day                  = {26},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-02-15 06:48:57},
  timestamp            = {2020-02-25 23:18},
}

@Article{Amisano-Giacomini-2007,
  author         = {Amisano, Gianni and Giacomini, Raffaella},
  date           = {2007-04},
  journaltitle   = {Journal of Business \& Economic Statistics},
  title          = {Comparing density forecasts via weighted likelihood ratio tests},
  doi            = {10.1198/073500106000000332},
  issn           = {0735-0015},
  number         = {2},
  pages          = {177--190},
  volume         = {25},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_Test},
  timestamp      = {2020-02-25 23:19},
}

@Article{Bahrami-et-al-2016,
  author               = {Bahrami, Afsaneh and Shamsuddin, Abul and Uylangco, Katherine},
  date                 = {2016-09},
  journaltitle         = {Accounting \& Finance},
  title                = {Out-of-sample stock return predictability in emerging markets},
  doi                  = {10.1111/acfi.12234},
  pages                = {n/a},
  abstract             = {This article builds on the widely debated issue of stock return predictability by applying a broad range of predictor variables and comprehensively considering the in-sample and out-of-sample stock return predictability of ten advanced emerging markets. It compares forecasts from models with a single predictor variable, multiple predictor variables and a combination forecast approach. The results confirm the findings of Welch and Goyal (2008) for US data that only a limited number of individual predictor variables are able to deliver significant out-of-sample forecasts. However, a combination forecast approach provides statistically and economically significant out-of-sample forecast results.},
  citeulike-article-id = {14166641},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/acfi.12234},
  day                  = {1},
  groups               = {Predictability_FinInfo, FcstQWIM_Equity, FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-10-17 03:34:41},
  timestamp            = {2020-02-25 23:19},
}

@Article{Baker-2011,
  author               = {Baker, Alastair},
  date                 = {2011-05},
  journaltitle         = {Journal of Asset Management},
  title                = {Forecasting medium-term returns and testing their value in constructing a simple portfolio},
  doi                  = {10.1057/jam.2011.5},
  issn                 = {1470-8272},
  number               = {4},
  pages                = {235--247},
  volume               = {12},
  abstract             = {This article examines the process behind generating robust medium-term (10 year) forecasts and tests their use in portfolio construction. The article revisits Bogle's (1991a, b and 1995) Occam's razor approach to forecasting. It examines why it has performed well and discusses its most recent performance. The article uses the Bogle model to test the portfolio construction process used by different actors in the asset management industry.

Finally, this article examines alternative methods for forecasting the valuation component of medium-term returns. It discusses why considering the level of inflation or inflation volatility may provide an alternative to the 30-year average suggested by Bogle (1991b).},
  citeulike-article-id = {9741220},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2011.5},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/pal/jam/2011/00000012/00000004/art00002},
  day                  = {26},
  groups               = {FrcstQWIM_Test, FrcstQWIM_MedLngTerm},
  owner                = {cristi},
  posted-at            = {2016-03-06 05:23:45},
  publisher            = {Palgrave Macmillan},
  timestamp            = {2020-02-25 23:19},
}

@Article{Barhoumi-et-al-2013,
  author               = {Barhoumi, Karim and Darne, Olivier and Ferrara, Laurent},
  date                 = {2013-02},
  journaltitle         = {Oxford Bulletin of Economics and Statistics},
  title                = {Testing the Number of Factors: An Empirical Assessment for a Forecasting Purpose*},
  doi                  = {10.1111/obes.12010},
  issn                 = {0305-9049},
  number               = {1},
  pages                = {64--79},
  volume               = {75},
  abstract             = {GDP forecasts based on dynamic factor models, applied to a large data set, are now widely used by practitioners involved in nowcasting and short-term macroeconomic forecasting. One recurrent empirical question that arises when dealing with such models is the way to determine the optimal number of factors. At the same time, statistical tests have recently been put forward in the literature in order to optimally determine the number of significant factors. In this article, we propose to reconcile both fields of interest by selecting the number of factors, through a testing procedure, to include in the forecasting equation. Through an empirical exercise on French and German GDPs, we assess the impact of a battery of recent statistical tests for the number of factors for a forecasting purpose. By implementing a rolling experience, we also assess the stability of the results overtime.},
  citeulike-article-id = {14072065},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/obes.12010},
  groups               = {FrcstQWIM_Test, Stat_Test},
  owner                = {cristi},
  posted-at            = {2016-06-19 16:19:26},
  timestamp            = {2020-02-25 23:19},
}

@Article{Barroso-Mayo-2017,
  author               = {Barroso, Pedro and Maio, Paulo F.},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The Risk-Return Tradeoff Among Equity Factors},
  url                  = {https://ssrn.com/abstract=2909085},
  abstract             = {We examine the risk-return trade-off among equity factors. We obtain a positive in-sample risk-return trade-off for the profitability (RMW) and investment (CMA) factors of Fama and French (2015, 2016), while for the market and momentum factors there is a negative relation. The out-of-sample forecasting power (of factor volatility for factor returns) is economically significant for both RMW and CMA: By constructing a trading strategy that relies on such predictability, we obtain annual Sharpe ratios above one and utility gains above 5 percent per year. We also find weak evidence that the factor variances are negatively correlated with the aggregate equity premium.},
  citeulike-article-id = {14276764},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2909085},
  groups               = {FcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2017-02-10 20:54:41},
  timestamp            = {2020-02-25 23:19},
}

@Article{Berkowitz-2001,
  author       = {Berkowitz, J.},
  date         = {2001},
  journaltitle = {Journal of Business and Economic Statistics},
  title        = {Testing density forecasts, with applications to risk management},
  doi          = {10.1198/07350010152596718},
  pages        = {465--474},
  url          = {https://www.tandfonline.com/doi/abs/10.1198/07350010152596718},
  volume       = {19},
  abstract     = {The forecast evaluation literature has traditionally focused on methods of assessing point forecasts. However, in the context of many models of financial risk, interest centers on more than just a single point of the forecast distribution. For example, value-at-risk models that are currently in extremely wide use form interval forecasts. Many other important financial calculations also involve estimates not summarized by a point forecast. Although some techniques are currently available for assessing interval and density forecasts, existing methods tend to display low power in sample sizes typically available.

This article suggests a new approach to evaluating such forecasts. It requires evaluation of the entire forecast distribution, rather than a scalar or interval. The information content of forecast distributions combined with ex post realizations is enough to construct a powerful test even with sample sizes as small as 100.},
  groups       = {FrcstQWIM_Test},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 23:19},
}

@Article{Byrne-Fu-2017,
  author               = {Byrne, Joseph P. and Fu, Rong},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Stock Return Prediction with Fully Flexible Models and Coefficients},
  url                  = {https://ssrn.com/abstract=2881247},
  abstract             = {We evaluate stock return predictability using a fully flexible Bayesian framework, which explicitly allows for different degrees of time-variation in coefficients and in forecasting models. We believe that asset return predictability can evolve quickly or slowly, based upon market conditions, and we should account for this. Our approach has superior out-of-sample predictive performance compared to the historical mean, from a statistical and economic perspective. We also find that our model statistically dominates its nested combination methods, including equal weighted models, Bayesian model averaging (BMA) and Dynamic model averaging (DMA). By decomposing sources of prediction uncertainty into five parts, we uncover that our fully flexible approach more precisely identifies the time-variation in coefficients and the combination method we should apply, leading to mitigation of estimation risk and forecasting improvements. Finally, we relate predictability to the business cycle},
  citeulike-article-id = {14322585},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2881247},
  groups               = {Predictability_Return_Other, FcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2017-03-29 15:21:04},
  timestamp            = {2020-02-25 23:19},
}

@Article{Chan-et-al-2015a,
  author               = {Chan, Wai-Sum and Cheung, Siu H. and Chow, Wai K. and Zhang, Li-Xin},
  date                 = {2015-09},
  journaltitle         = {Journal of Forecasting},
  title                = {A Robust Test for Threshold-Type Nonlinearity in Multivariate Time Series Analysis},
  doi                  = {10.1002/for.2344},
  number               = {6},
  pages                = {441--454},
  volume               = {34},
  abstract             = {There is growing interest in exploring potential forecast gains from the nonlinear structure of multivariate threshold autoregressive (MTAR) models. A least squares-based statistical test has been proposed in the literature. However, previous studies on univariate time series analysis show that classical nonlinearity tests are often not robust to additive outliers. The outlier problem is expected to pose similar difficulties for multivariate nonlinearity tests. In this paper, we propose a new and robust MTAR-type nonlinearity test, and derive the asymptotic null distribution of the test statistic. A Monte Carlo experiment is carried out to compare the power of the proposed test with that of the least squares-based test under the influence of additive time series outliers. The results indicate that the proposed method is preferable to the classical test when observations are contaminated by outliers. Finally, we provide illustrative examples by applying the statistical tests to two real datasets.},
  citeulike-article-id = {13995887},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2344},
  day                  = {1},
  groups               = {FrcstQWIM_Test, Stat_Test, Data_NonLinear},
  owner                = {cristi},
  posted-at            = {2016-04-04 01:36:35},
  timestamp            = {2020-02-25 23:19},
}

@Article{ChanLau-2016,
  author               = {Chan-Lau, Jorge A.},
  date                 = {2016-05},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Lasso Regressions and Forecasting Models in Applied Stress Testing},
  url                  = {https://ssrn.com/abstract=2784217},
  abstract             = {Model selection and forecasting in stress tests can be facilitated using machine learning techniques. These techniques have proved robust in other fields for dealing with the curse of dimensionality, a situation often encountered in applied stress testing. Lasso regressions, in particular, are well suited for building forecasting models when the number of potential covariates is large, and the number of observations is small or roughly equal to the number of covariates. This paper presents a conceptual overview of lasso regressions, explains how they fit in applied stress tests, describes its advantages over other model selection methods, and illustrates their application by constructing forecasting models of sectoral probabilities of default in an advanced emerging market economy.},
  citeulike-article-id = {14064039},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2784217},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2784217code96623.pdf?abstractid=2784217 and mirid=1},
  day                  = {25},
  groups               = {FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-06-09 21:30:48},
  timestamp            = {2020-02-25 23:19},
}

@Article{Charles-et-al-2016,
  author               = {Charles, Amelie and Darne, Olivier and Kim, Jae H.},
  date                 = {2016-06},
  journaltitle         = {International Review of Financial Analysis},
  title                = {International stock return predictability: Evidence from new statistical tests},
  doi                  = {10.1016/j.irfa.2016.06.005},
  issn                 = {1057-5219},
  abstract             = {We examine return predictability of international stock markets. We use financial ratios, technical indicators and short-term rates. We apply two new alternative testing and estimation methods. We find that the financial ratios show weak predictive ability. The price pressure and interest rate are found to be strong predictors for stock return. We investigate whether stock returns of international markets are predictable from a range of fundamentals including key financial ratios (dividend-price ratio, dividend-yield, earnings-price ratio, dividend-payout ratio), technical indicators (price pressure, change in volume), and short-term interest rates. We adopt two new alternative testing and estimation methods: the improved augmented regression method and wild bootstrapping of predictive model based on a restricted VAR form. Both methods take explicit account of endogeneity of predictors, providing bias-reduced estimation and improved statistical inference in small samples. From monthly data of 16 Asia-Pacific (including U.S.) and 21 European stock markets from 2000 to 2014, we find that the financial ratios show weak predictive ability with small effect sizes and poor out-of-sample forecasting performances. In contrast, the price pressure and interest rate are found to be strong predictors for stock return with large effect sizes and satisfactory out-of-sample forecasting performance.},
  citeulike-article-id = {14261819},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.irfa.2016.06.005},
  groups               = {Predictability_Return_Other, Predictability_FinInfo, FrcstQWIM_Test},
  posted-at            = {2017-01-24 19:57:47},
  timestamp            = {2020-02-25 23:19},
}

@Article{Chincoli-Guidolin-2017,
  author               = {Chincoli, Francesco and Guidolin, Massimo},
  date                 = {2017},
  journaltitle         = {Journal of Asset Management},
  title                = {Linear and nonlinear predictability in investment style factors: multivariate evidence},
  doi                  = {10.1057/s41260-017-0048-5},
  pages                = {1--34},
  abstract             = {This paper studies the predictive performance of multivariate models at forecasting the (excess) returns of portfolios mimicking the Market, Size, Value, Momentum, and Low Volatility factors isolated in asset pricing research. We evaluate the accuracy of the point forecasts of a number of linear and regime-switching models in recursive, out-of-sample forecasting experiments. We assess the accuracy of the models using several measures of unbiasedness and predictive accuracy, and using Diebold and Mariano's approach to test whether differences in expected losses from all possible pairs of forecast models are statistically significant. We fail to find evidence that complex statistical models are uniformly more accurate than a naive constant expected return model for factor-mimicking portfolio (excess) returns. However, we show that it is possible to build simple portfolio strategies that profit from the higher out-of-sample predictive accuracy of forecasting models with Markov switching in conditional mean coefficients. These results appear to be independent of the forecasting horizon and robust to changes in the loss function that captures the investors' objectives.},
  citeulike-article-id = {14333434},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/s41260-017-0048-5},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/s41260-017-0048-5},
  groups               = {Factor_Test, Predictability_FinInfo, ChngPoints_TimeSrs, FrcstQWIM_Test},
  posted-at            = {2017-04-07 11:46:17},
  publisher            = {Palgrave Macmillan UK},
  timestamp            = {2020-02-25 23:19},
}

@Article{Chorro-et-al-2018,
  author         = {Chorro, Christophe and Guegan, Dominique and Ielpo, Florian and Lalaharison, Hanjarivo},
  date           = {2018-09},
  journaltitle   = {Journal of Empirical Finance},
  title          = {Testing for leverage effects in the returns of US equities},
  doi            = {10.1016/j.jempfin.2018.07.008},
  issn           = {0927-5398},
  pages          = {290--306},
  volume         = {48},
  abstract       = {Abstract This article questions the empirical usefulness of leverage effects to forecast the dynamics of equity returns. In sample, we consistently find a significant but limited contribution of leverage effects over the past 25 years of SandP 500 returns. From an out-of-sample forecasting perspective and using a variety of different models, we find no statistical or economical value in using leverage effects, provided that an asymmetric and fat-tailed conditional distribution is used. This conclusion holds both at the index level and for 70\% of the individual stocks constituents of the equity index.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_Test, Stat_Test, FcstQWIM_Equity},
  timestamp      = {2020-02-25 23:19},
}

@InCollection{Christoffersen-2010,
  author       = {Peter Christoffersen},
  booktitle    = {Encyclopedia of quantitative finance},
  date         = {2010},
  title        = {Backtesting},
  doi          = {10.1002/9780470061602.eqf15018},
  publisher    = {Wiley},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470061602.eqf15018},
  abstract     = {This chapter surveys methods for backtesting risk models using the ex ante riskmeasure forecasts from the model and the ex post realized portfolio profit or loss. The risk measure forecast can take the form of a VaR, an Expected Shortfall, or a distribution forecast. The backtesting methods surveyed in this chapter can be seen as a final diagnostic check on the aggregate risk model carried out by the risk management team that constructed the risk model, or they can be used by external model-evaluators such as bank supervisors. Common for the approaches suggested is that they only require information on the daily ex ante risk model forecast and the daily ex post corresponding profit and loss. In particular, knowledge about the assumptions behind the risk model and its construction is not required.},
  groups       = {FrcstQWIM_Test, ExAnte_ExPost},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2044825},
  owner        = {a444631},
  timestamp    = {2020-02-25 23:19},
}

@Article{Clark-et-al-2015,
  author               = {Clark, Todd E. and McCracken, Michael W.},
  date                 = {2015-05},
  journaltitle         = {Journal of Econometrics},
  title                = {Nested forecast model comparisons: A new approach to testing equal accuracy},
  doi                  = {10.1016/j.jeconom.2014.06.016},
  issn                 = {0304-4076},
  number               = {1},
  pages                = {160--177},
  volume               = {186},
  abstract             = {We develop methods for testing whether, in a finite sample, forecasts from nested models are equally accurate. Most prior work has focused on a null of equal accuracy in population basically, whether the additional coefficients of the larger model are zero. Our asymptotic approximation instead treats the coefficients as non-zero but small, such that, in a finite sample, forecasts from the small and large models are expected to be equally accurate. We derive the limiting distributions of tests of equal mean square error, and develop a bootstrap for inference. Simulations show that our procedures have good size and power properties.},
  citeulike-article-id = {14070596},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jeconom.2014.06.016},
  groups               = {FrcstQWIM_Test},
  owner                = {zkgst0c},
  posted-at            = {2016-06-16 23:24:55},
  timestamp            = {2020-02-25 23:19},
}

@Article{Clark-West-2007,
  author               = {Clark, Todd E. and West, Kenneth D.},
  date                 = {2007-05},
  journaltitle         = {Journal of Econometrics},
  title                = {Approximately normal tests for equal predictive accuracy in nested models},
  doi                  = {10.1016/j.jeconom.2006.05.023},
  issn                 = {0304-4076},
  number               = {1},
  pages                = {291--311},
  volume               = {138},
  abstract             = {Forecast evaluation often compares a parsimonious null model to a larger model that nests the null model. Under the null that the parsimonious model generates the data, the larger model introduces noise into its forecasts by estimating parameters whose population values are zero. We observe that the mean squared prediction error (MSPE) from the parsimonious model is therefore expected to be smaller than that of the larger model. We describe how to adjust MSPEs to account for this noise. We propose applying standard methods [West, K.D., 1996. Asymptotic inference about predictive ability. Econometrica 64, 1067-1084] to test whether the adjusted mean squared error difference is zero. We refer to nonstandard limiting distributions derived in Clark and McCracken [2001. Tests of equal forecast accuracy and encompassing for nested models. Journal of Econometrics 105, 85-110; 2005a. Evaluating direct multistep forecasts. Econometric Reviews 24, 369-404] to argue that use of standard normal critical values will yield actual sizes close to, but a little less than, nominal size. Simulation evidence supports our recommended procedure.},
  citeulike-article-id = {14167245},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jeconom.2006.05.023},
  groups               = {FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-10-18 08:28:48},
  timestamp            = {2020-02-25 23:19},
}

@Article{Almadi-et-al-2014,
  author               = {Almadi, Himanshu and Rapach, David E. and Suri, Anil},
  date                 = {2014-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Return Predictability and Dynamic Asset Allocation: How Often Should Investors Rebalance?},
  doi                  = {10.3905/jpm.2014.40.4.016},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {16--27},
  volume               = {40},
  abstract             = {To exploit return predictability via dynamic asset allocation, investors face the important practical issue of how often to rebalance their portfolios. More frequent rebalancing uses statistically and economically significant short-horizon return predictability to aggressively pursue the dynamic investment opportunities afforded by changes in expected returns. However, the degree of return predictability typically appears stronger at longer horizons, which, along with lower transaction costs, favors less frequent rebalancing.

The authors analyze the performance effects of rebalancing frequency in the context of dynamic portfolios constructed from monthly, quarterly, semi-annual, and annual return forecasts for U.S. stocks, bonds, and bills, where the dynamic portfolios rebalance at the same frequency as the forecast horizon. Along the transaction-cost/rebalancing frontier, monthly (annual) rebalancing provides the greatest outperformance when unit transaction costs are below (above) approximately 50 basis points, and dynamic portfolios based on annual rebalancing typically outperform the benchmarks for unit transaction costs well in excess of 400 basis points.},
  citeulike-article-id = {13972193},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2014.40.4.016},
  groups               = {DAA, Invest_Dynamic, Predictability_FinInfo, FcstQWIM_Bond, FcstQWIM_Equity},
  owner                = {cristi},
  posted-at            = {2016-03-08 09:09:19},
  timestamp            = {2020-02-25 23:19},
}

@InCollection{Alti-Titman-2018,
  author               = {Alti, Aydogan and Titman, Sheridan},
  booktitle            = {Asset Pricing: New Theories and Empirical Approaches},
  date                 = {2018},
  title                = {A Dynamic Model of Characteristic-based Return Predictability},
  url                  = {http://discovery.ucl.ac.uk/1461997/},
  abstract             = {We present a model in which characteristic-based investment strategies generate abnormal returns in finite samples but not in the long run. In the model, the "climate of disruptive innovation," which affects the arrival rate of new projects and the exit rate of existing businesses, is a source of systematic risk that influences the returns of portfolios sorted on value, profitability, and asset growth. If investors are overconfident about their abilities to evaluate the disruption climate, these characteristic-sorted portfolios exhibit persistent expected returns, which increase the likelihood of extreme return realizations in finite samples. Through simulations we analyze the model's implications for the precision of empirical estimates and assess the likelihood of historical evidence repeating in the future.},
  citeulike-article-id = {14501328},
  groups               = {Invest_Dynamic, Predictability_FinInfo},
  posted-at            = {2017-12-11 23:28:40},
  timestamp            = {2020-02-25 23:19},
}

@Article{Anatolyev-Barunik-2017a,
  author         = {Anatolyev, Stanislav and Barunik, Jozef},
  date           = {2017-11-15},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Forecasting dynamic return distributions based on ordered binary choice and cross-quantile predictability connection},
  url            = {https://arxiv.org/abs/1711.05681},
  abstract       = {We present a simple approach to forecasting conditional probability distributions of asset returns. We work with a parsimonious specification of ordered binary choice regression that imposes a connection on sign predictability across different quantiles. The model forecasts the future conditional probability distributions of returns quite precisely when using a past indicator and past volatility proxy as predictors. Direct benefits of the model are revealed in an empirical application to 29 most liquid U.S. stocks. The forecast probability distribution is translated to significant economic gains in a simple trading strategy. Our approach can also be useful in many other applications where conditional distribution forecasts are desired.},
  day            = {15},
  f1000-projects = {QuantInvest},
  groups         = {Predictability_FinInfo, FcstQWIM_Equity},
  timestamp      = {2020-02-25 23:19},
}

@Article{Ang-Bekaert-2007,
  author         = {Ang, Andrew and Bekaert, Geert},
  date           = {2007-05},
  journaltitle   = {Review of Financial Studies},
  title          = {Stock return predictability: is it there?},
  doi            = {10.1093/rfs/hhl021},
  issn           = {0893-9454},
  number         = {3},
  pages          = {651--707},
  volume         = {20},
  abstract       = {We examine the predictive power of the dividend yields for forecasting excess returns, cash flows, and interest rates. Dividend yields predict excess returns only at short horizons together with the short rate and do not have any long-horizon predictive power. At short horizons, the short rate strongly negatively predicts returns. These results are robust in international data and are not due to lack of power. A present value model that matches the data shows that discount rate and short rate movements play a large role in explaining the variation in dividend yields. Finally, we find that earnings yields significantly predict future cash flows. (JEL C12, C51, C52, E49, F30, G12)},
  f1000-projects = {QuantInvest},
  groups         = {Predictability_Return_Other, Predictability_FinInfo, FcstQWIM_Equity},
  timestamp      = {2020-02-25 23:19},
}

@Article{ArrietaIbarra-Lobato-2015,
  author               = {Arrieta-ibarra, Imanol and Lobato, Ignacio N.},
  date                 = {2015-09},
  journaltitle         = {Journal Of Time Series Analysis},
  title                = {Testing for Predictability in Financial Returns Using Statistical Learning Procedures},
  doi                  = {10.1111/jtsa.12120},
  number               = {5},
  pages                = {672--686},
  volume               = {36},
  abstract             = {This article examines the ability of recently developed statistical learning procedures, such as random forests or support vector machines, for forecasting the first two moments of stock market daily returns. These tools present the advantage of the flexibility of the considered nonlinear regression functions even in the presence of many potential predictors. We consider two cases: where the agent's information set only includes the past of the return series, and where this set includes past values of relevant economic series, such as interest rates, commodities prices or exchange rates. Even though these procedures seem to be of no much use for predicting returns, it appears that there is real potential for some of these procedures, especially support vector machines, to improve over the standard GARCH(1,1) model the out-of-sample forecasting ability for squared returns. The researcher has to be cautious on the number of predictors employed and on the specific implementation of the procedures since using many predictors and the default settings of standard computing packages leads to overfitted models and to larger standard errors.},
  citeulike-article-id = {13984181},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/jtsa.12120},
  day                  = {1},
  groups               = {Predictability_FinInfo, FrcstQWIM_ML, Stat_Test, ML_Overfitting, FcstQWIM_Equity, ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-03-21 01:23:42},
  timestamp            = {2020-02-25 23:19},
}

@Article{Auer-2016,
  author               = {Auer, Benjamin R.},
  date                 = {2016-06},
  journaltitle         = {Emerging Markets Review},
  title                = {On time-varying predictability of emerging stock market returns},
  doi                  = {10.1016/j.ememar.2016.02.005},
  issn                 = {1566-0141},
  pages                = {1--13},
  volume               = {27},
  abstract             = {The two recent studies of Cajueiro and Tabak (2004b) and Hull and McGroarty (2014) investigate the predictability of emerging stock market returns based on the Hurst coefficient - a simple but powerful measure of long-range dependence. Unfortunately, the insights gained in these studies are limited because they (i) present conflicting evidence on the time-varying nature of the estimated Hurst coefficients and (ii) incorrectly equate random walk behaviour with market efficiency. In this note, we revisit the issue of time-varying predictability for a rich sample of 21 emerging markets in the 27-year period from 1988 to 2015. Extending the two aforementioned studies by various alternative fractal estimators of the Hurst coefficient, trend regressions and several robustness checks, our analysis reveals significant downward trends in the local Hurst coefficients of almost all markets. Specifically, we document vanishing predictability over time, which indicates that profitable emerging market investment strategies based on past returns may not continue their good performance in the future. Furthermore, we explicitly point out why a random walk is neither a necessary nor a sufficient condition for rationally determined security prices, and thus signs of predictability (randomness) should not be interpreted as evidence for market inefficiency (efficiency).},
  citeulike-article-id = {14184899},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ememar.2016.02.005},
  groups               = {Predictability_Return_Other, Predictability_FinInfo},
  owner                = {cristi},
  posted-at            = {2016-11-15 21:27:11},
  timestamp            = {2020-02-25 23:19},
}

@Article{Aye-et-al-2017,
  author               = {Aye, Goodness C and Balcilar, Mehmet and Gupta, Rangan},
  date                 = {2017},
  journaltitle         = {Empirica},
  title                = {International stock return predictability: Is the role of U.S. time-varying?},
  doi                  = {10.1007/s10663-015-9313-3},
  number               = {1},
  pages                = {121--146},
  volume               = {44},
  citeulike-article-id = {14070604},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10663-015-9313-3},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10663-015-9313-3},
  groups               = {Predictability_FinInfo},
  owner                = {zkgst0c},
  posted-at            = {2016-06-16 23:34:02},
  publisher            = {Springer US},
  timestamp            = {2020-02-25 23:19},
}

@Article{Baker-Wurgler-2012,
  author               = {Baker, Malcolm and Wurgler, Jeffrey},
  date                 = {2012-03},
  journaltitle         = {Review of Asset Pricing Studies},
  title                = {Comovement and Predictability Relationships Between Bonds and the Cross-section of Stocks},
  doi                  = {10.1093/rapstu/ras002},
  issn                 = {2044-9887},
  abstract             = {Government bonds comove more strongly with bond-like stocks: stocks of large, mature, low-volatility, profitable, dividend-paying firms that are neither high growth nor distressed. Variables that are derived from the yield curve that are already known to predict returns on bonds also predict returns on bond-like stocks; investor sentiment, a predictor of the cross-section of stock returns, also predicts excess bond returns. These relationships remain in place even when bonds and stocks become decoupled- at the index level. They are driven by a combination of effects including correlations between real cash flows on bonds and bond-like stocks, correlations between their risk-based return premia, and periodic flights to quality.},
  citeulike-article-id = {12518718},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rapstu/ras002},
  citeulike-linkout-1  = {http://raps.oxfordjournals.org/content/early/2012/03/16/rapstu.ras002.abstract},
  citeulike-linkout-2  = {http://raps.oxfordjournals.org/content/early/2012/03/16/rapstu.ras002.full.pdf},
  day                  = {16},
  groups               = {Predictability_FinInfo, Real_Yield_Curve},
  keywords             = {interestrate, macroeconomicfactors},
  owner                = {zkgst0c},
  posted-at            = {2016-08-31 15:31:56},
  publisher            = {Oxford University Press},
  timestamp            = {2020-02-25 23:19},
}

@Article{Baku-2018,
  author         = {Baku, Elisa},
  date           = {2018-06},
  journaltitle   = {International Economics},
  title          = {Exchange rate predictability in emerging markets},
  doi            = {10.1016/j.inteco.2018.06.003},
  issn           = {2110-7017},
  abstract       = {Abstract This paper uses financial and macroeconomic variables to predict currency returns, by using a two-step procedure. The first step consists of a cointegration equation that explains the exchange rate level as a function of global and domestic financial factors. The second step estimates an error-correction equation, for modeling the expected returns. This approach is a factor model analysis, where a Lasso derived technique is used for variable selection. This paper will focus on the five most frequently traded Latin American currencies, Brazilian Real (BRL), Chilean Peso (CLP), Colombian Peso (COL), Mexican Peso (MXN) and Peruvian Sol (PEN), during the time horizon from December 2001 until February 2016. The first finding is that the Global Exchange Rate Factor offers information about the exchange rate movements. In addition, this paper shows that commodity, equity prices and domestic risk premium are important variables for explaining exchange rates. Moreover, it confirms the existing results for the carry and slope variables.},
  f1000-projects = {QuantInvest},
  groups         = {Predictability_Return_Other, Predictability_FinInfo},
  timestamp      = {2020-02-25 23:19},
}

@Article{Baltas-Karyampas-2016a,
  author               = {Baltas, Nick and Karyampas, Dimitris},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Forecasting the Equity Risk Premium: Predictability versus Profitability},
  url                  = {https://ssrn.com/abstract=2717706},
  abstract             = {Asset allocation is critically dependent on the ability to forecast the equity risk premium (ERP) out-of-sample. But, is econometric predictability synonymous to profitability? We evaluate the performance of recently introduced ERP forecasting models, which have been shown to generate econometrically superior ERP forecasts, and find that their profitability benefit is regime-dependent. They give rise to significant relative losses during market downturns, when it matters the most for asset allocators to retain assets and their client base intact. Conversely, any relative economic benefit that mainly occurs during market up- swings can be significantly shrunk, especially for high risk averse and leverage constrained investors.},
  citeulike-article-id = {14184893},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2717706},
  groups               = {Predictability_Return_Other, RiskPremia_Equity, RiskPremia_Forecast, RiskPremia_Regime, Predictability_FinInfo, FrcstQWIM_MedLngTerm, FcstQWIM_Equity},
  owner                = {cristi},
  posted-at            = {2016-11-15 21:19:42},
  timestamp            = {2020-02-25 23:19},
}

@TechReport{Beckmann-et-al-2017,
  author         = {Beckmann, J and Koop, G and Korobilis, D and Schussler, R},
  date           = {2017},
  institution    = {University of Essex},
  title          = {Exchange rate predictability and dynamic Bayesian learning},
  location       = {Colchester},
  url            = {http://repository.essex.ac.uk/20781/},
  abstract       = {This paper considers how an investor in foreign exchange markets might exploit predictive information in macroeconomic fundamentals by allowing for switching between multivariate time series regression models. These models are chosen to reflect a wide array of established empirical and theoretical stylized facts. In an application involving monthly exchange rates for seven countries, we find that an investor using our methods to dynamically allocate assets achieves significant gains relative to benchmark strategies. In particular, we find strong evidence for fast model switching, with most of the time only a small set of macroeconomic fundamentals being relevant for forecasting.},
  f1000-projects = {QuantInvest},
  groups         = {Predictability_Return_Other, Predictability_FinInfo, FrcstQWIM_ML, FrcstQWIM_Other, ML_ForcstTimeSrs},
  publisher      = {Essex Finance Centre Working Papers},
  timestamp      = {2020-02-25 23:19},
}

@Article{Bekiros-et-al-2017a,
  author               = {Bekiros, Stelios and Shahzad, Syed J. and Arreola-Hernandez, Jose and Ur Rehman, Mobeen},
  date                 = {2017-10},
  journaltitle         = {Economic Modelling},
  title                = {Directional predictability and time-varying spillovers between stock markets and economic cycles},
  doi                  = {10.1016/j.econmod.2017.10.003},
  issn                 = {0264-9993},
  abstract             = {We examine the nonlinear dependence structure and causal nexus between business cycles, stock market returns and asset return volatility for the US economy. We implement two novel methodologies, namely quantile-on-quantile analysis and cross-quantilogram to account for tail dependence and spillovers across quantile ranges. We find evidence of statistically significant spillover effects from extreme equity market returns and their corresponding volatility to specific stages of business cycles. The sensitivity of returns and volatility to business cycle shocks is only evident for extreme quantiles. These findings indicate the importance of modeling the nonlinearity and tail behaviour when analyzing the relationships between equity markets and business cycles. Financial and monetary policy regulators may use the dynamics of spillover predictability and influence between the equity market returns, their volatility and business cycles to exert some degree of control upon business cycle formation and development.},
  citeulike-article-id = {14516034},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.econmod.2017.10.003},
  groups               = {Predictability_Return_Other, Predictability_FinInfo, Data_NonLinear, RiskRet_BusCycle},
  posted-at            = {2018-01-12 23:01:05},
  timestamp            = {2020-02-25 23:19},
}

@InCollection{Bianchi-et-al-2017b,
  author               = {Bianchi, Robert J. and Drew, Michael E. and Pappas, Scott N.},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {The Predictability of Risk-Factor Returns},
  doi                  = {10.1016/b978-1-78548-201-4.50005-2},
  isbn                 = {9781785482014},
  pages                = {99--126},
  publisher            = {Elsevier},
  abstract             = {This chapter explores the predictability of risk-factor investment returns using a range of combination forecast models. We find evidence that single-variable forecasts can be combined to produce risk-factor return estimates that are economically and statistically significant. Forecast effectiveness, however, varies depending on the risk factor modeled and the weighting method employed to combine individual forecasts.},
  citeulike-article-id = {14499091},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50005-2},
  groups               = {Predictability_FinInfo},
  posted-at            = {2017-12-08 00:47:58},
  timestamp            = {2020-02-25 23:19},
}

@Article{Bianchi-Guidolin-2014,
  author               = {Bianchi, Daniele and Guidolin, Massimo},
  date                 = {2014},
  journaltitle         = {The Journal of Real Estate Finance and Economics},
  title                = {Can Linear Predictability Models Time Bull and Bear Real Estate Markets? Out-of-Sample Evidence from REIT Portfolios},
  doi                  = {10.1007/s11146-013-9411-6},
  number               = {1},
  pages                = {116--164},
  url                  = {https://link.springer.com/article/10.1007/s11146-013-9411-6},
  volume               = {49},
  abstract             = {A recent literature has shown that REIT returns contain strong evidence of bull and bear dynamic regimes that may be best captured using nonlinear econometric models of the Markov switching type. In fact, REIT returns would display regime shifts that are more abrupt and persistent than in the case of other asset classes. In this paper we ask whether and how simple linear predictability models of the vector autoregressive (VAR) type may be extended to capture the bull and bear patterns typical of many asset classes, including REITs. We find that nonlinearities are so deep that it is impossibile for a large family of VAR models to either produce similar portfolio weights or to yield realized, ex-post out-of-sample long-horizon portfolio performances that may compete with those typical of bull and bear models. A typical investor with intermediate risk aversion and a 5-year horizon ought to be ready to pay an annual fee of up to 5.7 percent to have access to forecasts of REIT returns that take their bull and bear dynamics into account instead of simpler, linear forecast.},
  booktitle            = {The Journal of Real Estate Finance and Economics},
  citeulike-article-id = {13988682},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11146-013-9411-6},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11146-013-9411-6},
  groups               = {Regime based investing, Predictability_FinInfo, Invest_RealEstate, FrcstQWIM_MedLngTerm, FrcstQWIM_Other},
  owner                = {cristi},
  posted-at            = {2016-03-26 23:51:37},
  publisher            = {Springer US},
  timestamp            = {2020-02-25 23:19},
}

@TechReport{BNYMellon-2015,
  author      = {{BNYMellon Investment Strategy}},
  date        = {2015},
  institution = {BNY Mellon},
  title       = {10-Year Capital Market Return Assumptions Calendar Year 2015},
  url         = {https://im.bnymellon.com/us/en/institutional/perspectives/all.jsp},
  groups      = {Predictability_FinInfo},
  owner       = {zkgst0c},
  timestamp   = {2020-02-25 23:19},
}

@Article{Bollerslev-et-al-2015,
  author               = {Bollerslev, Tim and Todorov, Viktor and Xu, Lai},
  date                 = {2015-10},
  journaltitle         = {Journal of Financial Economics},
  title                = {Tail risk premia and return predictability},
  doi                  = {10.1016/j.jfineco.2015.02.010},
  issn                 = {0304-405X},
  number               = {1},
  pages                = {113--134},
  volume               = {118},
  abstract             = {The variance risk premium, defined as the difference between the actual and risk-neutral expectations of the forward aggregate market variation, helps predict future market returns. Relying on a new essentially model-free estimation procedure, we show that much of this predictability may be attributed to time variation in the part of the variance risk premium associated with the special compensation demanded by investors for bearing jump tail risk, consistent with the idea that market fears play an important role in understanding the return predictability.},
  citeulike-article-id = {13948057},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jfineco.2015.02.010},
  groups               = {Predictability_FinInfo},
  owner                = {cristi},
  posted-at            = {2016-03-01 22:33:20},
  timestamp            = {2020-02-25 23:19},
}

@Article{Boudoukh-et-al-2008,
  author               = {Boudoukh, Jacob and Richardson, Matthew and Whitelaw, Robert F.},
  date                 = {2008-07},
  journaltitle         = {Review of Financial Studies},
  title                = {The Myth of Long-Horizon Predictability},
  doi                  = {10.1093/rfs/hhl042},
  issn                 = {1465-7368},
  number               = {4},
  pages                = {1577--1605},
  volume               = {21},
  abstract             = {The prevailing view in finance is that the evidence for long-horizon stock return predictability is significantly stronger than that for short horizons. We show that for persistent regressors, a characteristic of most of the predictive variables used in the literature, the estimators are almost perfectly correlated across horizons under the null hypothesis of no predictability. For the persistence levels of dividend yields, the analytical correlation is 99 percent between the 1- and 2-year horizon estimators and 94 percent between the 1- and 5-year horizons. Common sampling error across equations leads to ordinary least squares coefficient estimates and R2s that are roughly proportional to the horizon under the null hypothesis. This is the precise pattern found in the data. We perform joint tests across horizons for a variety of explanatory variables and provide an alternative view of the existing evidence.},
  citeulike-article-id = {5282603},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rfs/hhl042},
  citeulike-linkout-1  = {http://rfs.oxfordjournals.org/content/21/4/1577.abstract},
  citeulike-linkout-2  = {http://rfs.oxfordjournals.org/content/21/4/1577.full.pdf},
  day                  = {01},
  groups               = {Predictability_FinInfo},
  owner                = {zkgst0c},
  posted-at            = {2016-08-11 21:17:22},
  publisher            = {Oxford University Press},
  timestamp            = {2020-02-25 23:19},
}

@Article{Brandon-Wang-2013,
  author         = {Brandon, Rajna Gibson and Wang, Songtao},
  date           = {2013-02},
  journaltitle   = {Journal of Financial and Quantitative Analysis},
  title          = {Liquidity risk, return predictability, and hedge funds performance: an empirical study},
  doi            = {10.1017/S0022109012000634},
  issn           = {0022-1090},
  number         = {01},
  pages          = {219--244},
  volume         = {48},
  abstract       = {This article analyzes the effect of liquidity risk on the performance of equity hedge fund portfolios. Similarly to Avramov, Kosowski, Naik, and Teo ( 2007 ), (2011), we observe that, before accounting for the effect of liquidity risk, hedge fund portfolios that incorporate predictability in managerial skills generate superior performance. This outperformance disappears or weakens substantially for most emerging markets, event-driven, and long/short hedge fund portfolios once we account for liquidity risk. Moreover, we show that the equity market-neutral and long/short hedge fund portfolios also entail rents for their service as liquidity providers. These results hold under various robustness tests.},
  f1000-projects = {QuantInvest},
  groups         = {Predictability_FinInfo, Hedge_Funds},
  timestamp      = {2020-02-25 23:19},
}

@Article{Carl-2016,
  author               = {Carl, Ulrich},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Equity Factor Predictability},
  url                  = {https://ssrn.com/abstract=2915450},
  abstract             = {This article comprehensively reviews the predictability of six equity factors. These factors are the market excess return, size, value, momentum, low beta and quality. I find predictability for the low beta factor and moderate predictability for the size factor. The results for other factors are mixed. Moreover, predicted returns for the market, size, value and momentum factors are to a large extent driven by a common component. This common component is partly related to the business cycle: the market, size and value factors are anti-cyclical, while the momentum factor is pro-cyclical. However, business cycles can only explain a small part of this common component.},
  citeulike-article-id = {14402005},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2915450},
  groups               = {Predictability_Return_Other, Predictability_FinInfo, RiskRet_BusCycle},
  posted-at            = {2017-07-29 18:42:18},
  timestamp            = {2020-02-25 23:19},
}

@Article{Casassus-Higuera-2012,
  author               = {Casassus, Jaime and Higuera, Freddy},
  date                 = {2012-12},
  journaltitle         = {Quantitative Finance},
  title                = {Short-horizon return predictability and oil prices},
  doi                  = {10.1080/14697688.2012.751122},
  number               = {12},
  pages                = {1909--1934},
  volume               = {12},
  abstract             = {This paper shows that oil price changes, measured as short-term futures returns, are a strong predictor of excess stock returns at short horizons. Ours is a leading variable for the business cycle and exhibits low persistence which avoids the fictitious long-horizon predictability associated with other predictors used in the literature.

We compare our variable with the most popular predictors in a sample period that includes the recent financial crisis. Our results suggest that oil price changes are the only variable with forecasting power for stock returns. This significant predictive ability is robust against the inclusion of other variables and out-of-sample tests. We also study the cross-section of expected stock returns in a conditional CAPM framework based on oil price shocks

. Our model displays high statistical significance and a better fit than all the conditional and unconditional models considered, including the Fama?French three-factor model. From a practical perspective, ours is a high-frequency, observable variable that has the advantage of being readily available to market-timing investors.},
  citeulike-article-id = {13933210},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2012.751122},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.751122},
  day                  = {1},
  groups               = {Predictability_FinInfo},
  owner                = {cristi},
  posted-at            = {2016-02-15 16:21:20},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 23:19},
}

@Article{Chang-2016a,
  author               = {Chang, Wayne},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Information Aggregation for Stock Return Predictability},
  doi                  = {10.2139/ssrn.2841389},
  issn                 = {1556-5068},
  abstract             = {The literature on stock return predictability has identified macroeconomic and technical predictors that when combined, leads to out-of-sample outperformance relative to the historical mean null. This paper investigates a new method for aggregating information beyond using forecast combination or principal components. By sequentially layering groups of information, the predictive performance of this new approach outperforms that of prior methods. Applying layering to volatility forecasting yields more mixed results. In all, a mean-variance investor investing in monthly stock returns gains from this new method as much as 4.5 percent per year.},
  citeulike-article-id = {14212372},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2841389},
  groups               = {Predictability_FinInfo, FcstQWIM_Equity},
  owner                = {cristi},
  posted-at            = {2016-11-21 20:49:40},
  timestamp            = {2020-02-25 23:19},
}

@Article{Chincoli-Guidolin-2017a,
  author               = {Chincoli, Francesco and Guidolin, Massimo},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Linear and Nonlinear Predictability in Investment Style Factors: Multivariate Evidence},
  url                  = {https://ssrn.com/abstract=2961653},
  abstract             = {This paper studies the predictive performance of multivariate models at forecasting the (excess) returns of portfolios mimicking the Market, Size, Value, Momentum, and Low Volatility factors isolated in asset pricing research. We evaluate the accuracy of the point forecasts of a number of linear and regime switching models in recursive, out-of-sample forecasting experiments. We assess the accuracy of the models using several measures of unbiasedness and predictive accuracy, and, using Diebold and Mariano's approach to test whether differences in expected losses from all possible pairs of forecast models are statistically significant. We fail to find evidence that complex statistical models are uniformly more accurate than a naive constant expected return model for factor-mimicking portfolio (excess) returns. However, we show that it is possible to build simple portfolio strategies that profit from the higher out-of-sample predictive accuracy of forecasting models with Markov switching in conditional mean coefficients. These results appear to be independent of the forecasting horizon and robust to changes in the loss function that captures the investors' objectives.},
  citeulike-article-id = {14350171},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2961653},
  groups               = {Predictability_Return_Other, Predictability_FinInfo},
  posted-at            = {2017-05-03 18:59:02},
  timestamp            = {2020-02-25 23:19},
}

@Article{Chliamovitch-et-al-2014,
  author               = {Chliamovitch, G. and Dupuis, A. and Golub, A. and Chopard, B.},
  date                 = {2014-04},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Improving predictability of time series using maximum entropy methods},
  doi                  = {10.1209/0295-5075/110/10003},
  eprint               = {1411.7805},
  eprinttype           = {arXiv},
  issn                 = {0295-5075},
  number               = {1},
  pages                = {10003+},
  volume               = {110},
  abstract             = {We discuss how maximum entropy methods may be applied to the reconstruction of Markov processes underlying empirical time series and compare this approach to usual frequency sampling. It is shown that, at least in low dimension, there exists a subset of the space of stochastic matrices for which the MaxEnt method is more efficient than sampling, in the sense that shorter historical samples have to be considered to reach the same accuracy. Considering short samples is of particular interest when modelling smoothly non-stationary processes, for then it provides, under some conditions, a powerful forecasting tool. The method is illustrated for a discretized empirical series of exchange rates.},
  citeulike-article-id = {14398885},
  citeulike-linkout-0  = {http://arxiv.org/abs/1411.7805},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1411.7805},
  citeulike-linkout-2  = {http://dx.doi.org/10.1209/0295-5075/110/10003},
  day                  = {01},
  groups               = {Predictability_Return_Other, Predictability_FinInfo},
  posted-at            = {2017-07-24 18:58:55},
  timestamp            = {2020-02-25 23:19},
}

@Article{Choi-et-al-2016,
  author               = {Choi, Yongok and Jacewitz, Stefan and Park, Joon Y.},
  date                 = {2016-05},
  journaltitle         = {Journal of Econometrics},
  title                = {A reexamination of stock return predictability},
  doi                  = {10.1016/j.jeconom.2015.02.048},
  issn                 = {0304-4076},
  number               = {1},
  pages                = {168--189},
  volume               = {192},
  abstract             = {We provide a simple and innovative approach to test for predictability in stock returns. Our approach consists of two methodologies, time change and instrumental variable estimation, which are employed respectively to deal effectively with persistent stochastic volatility in stock returns and endogenous nonstationarity in their predictors. These are prominent characteristics of the data used in predictive regressions, which are known to have a substantial impact on the test of predictability, if not properly taken care of. Our test finds no evidence supporting stock return predictability, at least if we use the common predictive ratios such as dividend price and earnings price ratios.},
  citeulike-article-id = {14070591},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jeconom.2015.02.048},
  groups               = {Predictability_FinInfo},
  owner                = {zkgst0c},
  posted-at            = {2016-06-16 23:17:22},
  timestamp            = {2020-02-25 23:19},
}

@Article{Cochrane-2008,
  author               = {Cochrane, John H.},
  date                 = {2008-07},
  journaltitle         = {Review of Financial Studies},
  title                = {The Dog That Did Not Bark: A Defense of Return Predictability},
  doi                  = {10.1093/rfs/hhm046},
  issn                 = {1465-7368},
  number               = {4},
  pages                = {1533--1575},
  volume               = {21},
  abstract             = {If returns are not predictable, dividend growth must be predictable, to generate the observed variation in divided yields. I find that the absence of dividend growth predictability gives stronger evidence than does the presence of return predictability. Long-horizon return forecasts give the same strong evidence. These tests exploit the negative correlation of return forecasts with dividend-yield autocorrelation across samples, together with sensible upper bounds on dividend-yield autocorrelation, to deliver more powerful statistics. I reconcile my findings with the literature that finds poor power in long-horizon return forecasts, and with the literature that notes the poor out-of-sample R2 of return-forecasting regressions.},
  citeulike-article-id = {2689341},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rfs/hhm046},
  citeulike-linkout-1  = {http://rfs.oxfordjournals.org/content/21/4/1533.abstract},
  citeulike-linkout-2  = {http://rfs.oxfordjournals.org/content/21/4/1533.full.pdf},
  day                  = {01},
  groups               = {Predictability_FinInfo},
  owner                = {zkgst0c},
  posted-at            = {2016-08-11 21:21:13},
  publisher            = {Oxford University Press},
  timestamp            = {2020-02-25 23:19},
}

@Article{Abe-Nakayama-2018,
  author               = {Abe, Masaya and Nakayama, Hideki},
  date                 = {2018-01-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Deep Learning for Forecasting Stock Returns in the Cross-Section},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1801.01777},
  abstract             = {Many studies have been undertaken by using machine learning techniques, including neural networks, to predict stock returns. Recently, a method known as deep learning, which achieves high performance mainly in image recognition and speech recognition, has attracted attention in the machine learning field. This paper implements deep learning to predict one-month-ahead stock returns in the cross-section in the Japanese stock market and investigates the performance of the method. Our results show that deep neural networks generally outperform shallow neural networks, and the best networks also outperform representative machine learning models. These results indicate that deep learning shows promise as a skillful machine learning method to predict stock returns in the cross-section.},
  citeulike-article-id = {14514574},
  citeulike-linkout-0  = {http://arxiv.org/abs/1801.01777},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1801.01777},
  day                  = {3},
  groups               = {ML_Forecast_QWIM, FrcstQWIM_ML, FcstQWIM_Equity},
  posted-at            = {2018-01-10 17:22:53},
  timestamp            = {2020-02-25 23:20},
}

@Article{Ahmed-et-al-2010,
  author               = {Ahmed, Nesreen K. and Atiya, Amir F. and Gayar, Neamat E. and El-Shishiny, Hisham},
  date                 = {2010-08},
  journaltitle         = {Econometric Reviews},
  title                = {An Empirical Comparison of Machine Learning Models for Time Series Forecasting},
  doi                  = {10.1080/07474938.2010.481556},
  number               = {5-6},
  pages                = {594--621},
  volume               = {29},
  abstract             = {In this work we present a large scale comparison study for the major machine learning models for time series forecasting. Specifically, we apply the models on the monthly M3 time series competition data (around a thousand time series). There have been very few, if any, large scale comparison studies for machine learning models for the regression or the time series forecasting problems, so we hope this study would fill this gap. The models considered are multilayer perceptron, Bayesian neural networks, radial basis functions, generalized regression neural networks (also called kernel regression), K-nearest neighbor regression, CART regression trees, support vector regression, and Gaussian processes. The study reveals significant differences between the different methods. The best two methods turned out to be the multilayer perceptron and the Gaussian process regression. In addition to model comparisons, we have tested different preprocessing methods and have shown that they have different impacts on the performance.},
  citeulike-article-id = {14072242},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/07474938.2010.481556},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/07474938.2010.481556},
  day                  = {30},
  groups               = {FrcstQWIM_ML, ML_PerfMetrics, ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-06-20 02:41:52},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-25 23:20},
}

@MastersThesis{BatresEstrada-et-al-2015,
  author               = {Batres-Estrada, B.},
  date                 = {2015},
  institution          = {KTH Royal Institute of Technology},
  title                = {Deep learning for multivariate financial time series},
  url                  = {http://www.diva-portal.org/smash/get/diva2:820891/FULLTEXT01.pdf},
  abstract             = {Abstract Deep learning is a framework for training and modelling neural networks which recently have surpassed all conventional methods in many learning tasks, prominently image and voice recognition. This thesis uses deep learning algorithms to forecast financial data. The deep learning framework is used to train a neural network. The deep neural network is a DBN coupled to a MLP. It is used to choose stocks to form portfolios. The portfolios have better returns than the median of the stocks forming the list. The stocks forming the SandP 500 are included in the study. The results obtained from the deep neural network are compared to bench- marks from a logistic regression network, a multilayer perceptron and a naive benchmark. The results obtained from the deep neural network are better and more stable than the benchmarks. The findings support that deep learning methods will find their way in finance due to their reliability and good performance.},
  citeulike-article-id = {14500287},
  groups               = {Machine learning and investment strategies, DeepLearning_QWIM, FrcstQWIM_ML, FcstQWIM_Equity, ML_ForcstTimeSrs},
  posted-at            = {2017-12-11 00:51:42},
  timestamp            = {2020-02-25 23:20},
}

@Article{Bloch-2018,
  author         = {Bloch, Daniel Alexandre},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Recipe for Quantitative Trading with Machine Learning},
  doi            = {10.2139/ssrn.3232143},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3232143},
  abstract       = {On one hand, financial time series are multifractal, thus exhibiting non-Gaussian distribution, the presence of extreme values (outliers), and long-range dependent dynamics. On the other hand, machine learning (ML) models are processes relying heavily on statistical models and methodologies, but treated as black box models due to their inability to explicitly know the relations established between explanatory variables (input) and dependent variables (output). However, when forecasting market returns, or generating autonomous patterns, it is crucial to know the statistical properties of the time series produced by the ML model under consideration. Taking into consideration these statistical characteristics, we present a recipe using technical indicators to forecast both market returns and their directions. We choose to reverse the causality and propose a solution consisting in deciding upon the framework by defining how the model should be specified before beginning to analyse the actual data. We should define the framework by properly formulating model hypotheses which make financial or economic sense, and then carefully determining the number of dependent variables in a regression model, or the number of factors and components in a stochastic model.},
  f1000-projects = {QuantInvest},
  groups         = {ML_Forecast_QWIM, ML_Classif_QWIM, FrcstQWIM_ML, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:20},
}

@Article{Dantas-Oliveira-2018,
  author         = {Dantas, Tiago Mendes and Oliveira, Fernando Luiz Cyrino},
  date           = {2018-10},
  journaltitle   = {International Journal of Forecasting},
  title          = {Improving time series forecasting: An approach combining bootstrap aggregation, clusters and exponential smoothing},
  doi            = {10.1016/j.ijforecast.2018.05.006},
  issn           = {0169-2070},
  number         = {4},
  pages          = {748--761},
  volume         = {34},
  abstract       = {Some recent papers have demonstrated that combining bagging (bootstrap aggregating) with exponential smoothing methods can produce highly accurate forecasts and improve the forecast accuracy relative to traditional methods. We therefore propose a new approach that combines the bagging, exponential smoothing and clustering methods. The existing methods use bagging to generate and aggregate groups of forecasts in order to reduce the variance. However, none of them consider the effect of covariance among the group of forecasts, even though it could have a dramatic impact on the variance of the group, and therefore on the forecast accuracy. The proposed approach, referred to here as Bagged Cluster ETS, aims to reduce the covariance effect by using partitioning around medoids (PAM) to produce clusters of similar forecasts, then selecting several forecasts from each cluster to create a group with a reduced variance. This approach was tested on various different time series sets from the M3 and CIF 2016 competitions. The empirical results have shown a substantial reduction in the forecast error, considering sMAPE and MASE.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, Scenario_TimeSeries},
  timestamp      = {2020-02-25 23:21},
}

@Article{DeGooijer-Hyndman-2006,
  author               = {{De Gooijer}, Jan G. and Hyndman, Rob J.},
  date                 = {2006-01},
  journaltitle         = {International Journal of Forecasting},
  title                = {25 years of time series forecasting},
  doi                  = {10.1016/j.ijforecast.2006.01.001},
  issn                 = {0169-2070},
  number               = {3},
  pages                = {443--473},
  volume               = {22},
  abstract             = {We review the past 25 years of research into time series forecasting. In this silver jubilee issue, we naturally highlight results published in journals managed by the International Institute of Forecasters (Journal of Forecasting 1982 1985 and International Journal of Forecasting 1985 2005). During this period, over one third of all papers published in these journals concerned time series forecasting. We also review highly influential works on time series forecasting that have been published elsewhere during this period. Enormous progress has been made in many areas, but we find that there are a large number of topics in need of further development. We conclude with comments on possible future research directions in this field.},
  citeulike-article-id = {9168361},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2006.01.001},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-04-04 02:23:56},
  timestamp            = {2020-02-25 23:21},
}

@Article{deMattosNeto-et-al-2017,
  author               = {{de Mattos Neto}, Paulo S. G. and Cavalcanti, George D. C. and Madeiro, Francisco},
  date                 = {2017-08},
  journaltitle         = {Pattern Recognition Letters},
  title                = {Nonlinear combination method of forecasters applied to PM time series},
  doi                  = {10.1016/j.patrec.2017.06.008},
  issn                 = {0167-8655},
  pages                = {65--72},
  volume               = {95},
  abstract             = {Hybrid systems that combine Artificial Neural Networks with other forecasters have been widely employed for time series forecasting. In this context, some architectures use temporal patterns extracted from the error series (residuals), i.e., the difference between the time series and the forecasting of this time series. These architectures have reached relevant theoretical and practical results. However, in the learning process of complex time series using these hybrid systems two open questions arise: it is hard to ensure that the linear and nonlinear patterns, underlying the time series, are properly modeled; and the best function to combine the time series forecaster and error series forecaster is unknown. In this context, this work proposes a Nonlinear Combination (NoLiC) method to combine forecasters. The NoLiC method is a hybrid system that is composed of two steps: i) estimation of the models' parameters for the time series and their respective residuals, and ii) search for the best function that combines these models using a multi-layer perceptron. Experimental simulations are conducted using four real-world complex time series of great importance for public health and evaluated using six performance measures. The results show that the NoLiC method reaches superior results when compared with literature works.},
  citeulike-article-id = {14499071},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patrec.2017.06.008},
  groups               = {Forecast_Hybrid, FrcstQWIM_TimeSrs, FrcstQWIM_Hybrid, ML_ForcstTimeSrs},
  posted-at            = {2017-12-08 00:02:36},
  timestamp            = {2020-02-25 23:21},
}

@Article{Efendi-et-al-2018,
  author         = {Efendi, Riswan and Arbaiy, Nureize and Deris, Mustafa Mat},
  date           = {2018-05},
  journaltitle   = {Inf Sci (Ny)},
  title          = {A new procedure in stock market forecasting based on fuzzy random auto-regression time series model},
  doi            = {10.1016/j.ins.2018.02.016},
  issn           = {0020-0255},
  pages          = {113--132},
  volume         = {441},
  abstract       = {Abstract Various models used in stock market forecasting presented have been classified according to the data preparation, forecasting methodology, performance evaluation, and performance measure. However, these models have not sufficiently discussed in data preparation to overcome randomness, as well as uncertainty and volatility of stock prices issues in achieving high forecasting accuracy. Therefore, the focus of this paper is the data preparation procedure of triangular fuzzy number to build an improved fuzzy random auto-regression model using non-stationary stock market data for forecasting purposes. The improved forecasting model considers two types of input, which are data with low-high and single point values of stock market prices. Even though, low-high data present variability and volatility in nature, the single data has to be form in symmetry left-right spread to present variability and standard error. Then, expectations and variances, confidence-intervals of fuzzy random data are constructed for fuzzy input-output data. By using the input-output data and simplex approach, parameters of the model can be estimated. In this study, some real data sets were used to represent both types of inputs, which are the Kuala Lumpur stock exchange and Alabama University enrollment. The study found that variability and spread adjustment are important factors in data preparation to improve accuracy of the fuzzy random auto-regression model.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, NonStatry_FinTimeSrs},
  timestamp      = {2020-02-25 23:21},
}

@InCollection{Eirola-Lendasse-2013,
  author               = {Eirola, Emil and Lendasse, Amaury},
  booktitle            = {Proceedings of the 12th International Symposium on Advances in Intelligent Data Analysis XII - Volume 8207},
  date                 = {2013},
  title                = {Gaussian Mixture Models for Time Series Modelling, Forecasting, and Interpolation},
  doi                  = {10.1007/978-3-642-41398-8\_15},
  isbn                 = {978-3-642-41397-1},
  location             = {London, UK},
  pages                = {162--173},
  publisher            = {Springer-Verlag New York, Inc.},
  series               = {IDA 2013},
  abstract             = {Gaussian mixture models provide an appealing tool for time series modelling. By embedding the time series to a higher-dimensional space, the density of the points can be estimated by a mixture model. The model can directly be used for short-to-medium term forecasting and missing value imputation. The modelling setup introduces some restrictions on the mixture model, which when appropriately taken into account result in a more accurate model. Experiments on time series forecasting show that including the constraints in the training phase particularly reduces the risk of overfitting in challenging situations with missing values or a large number of Gaussian components.},
  address              = {New York, NY, USA},
  citeulike-article-id = {14272460},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=2951042},
  citeulike-linkout-1  = {http://dx.doi.org/10.1007/978-3-642-41398-815},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-02-06 21:51:38},
  timestamp            = {2020-02-25 23:21},
}

@Article{Elliot-Hsu-2017,
  author               = {Elliot, Aaron and Hsu, Cheng H.},
  date                 = {2017-10-19},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Time Series Prediction : Predicting Stock Price},
  eprint               = {1710.05751},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1710.05751},
  abstract             = {Time series forecasting is widely used in a multitude of domains. In this paper, we present four models to predict the stock price using the SPX index as input time series data. The martingale and ordinary linear models require the strongest assumption in stationarity which we use as baseline models. The generalized linear model requires lesser assumptions but is unable to outperform the martingale. In empirical testing, the RNN model performs the best comparing to other two models, because it will update the input through LSTM instantaneously, but also does not beat the martingale. In addition, we introduce an online to batch algorithm and discrepancy measure to inform readers the newest research in time series predicting method, which doesn't require any stationarity or non mixing assumptions in time series data. Finally, to apply these forecasting to practice, we introduce basic trading strategies that can create Win win and Zero sum situations.},
  citeulike-article-id = {14507011},
  citeulike-linkout-0  = {http://arxiv.org/abs/1710.05751},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1710.05751},
  day                  = {19},
  groups               = {Characteristics and return prediction, FrcstQWIM_TimeSrs, FcstQWIM_Equity},
  posted-at            = {2017-12-22 06:03:50},
  timestamp            = {2020-02-25 23:21},
}

@Article{Ertuna-2016,
  author               = {Ertuna, Lev},
  date                 = {2016},
  journaltitle         = {ResearchGate},
  title                = {Stock Market Prediction Using Neural Network Time Series Forecasting},
  url                  = {https://www.researchgate.net/publication/303543459_Stock_Market_Prediction_Using_Neural_Network_Time_Series_Forecasting},
  abstract             = {Time series forecasting is a very powerful computati onal tool that allows predict ing future outcomes of a system based on how the system behaved previously, i t has a great number of applications in many areas of science. It i s possible to predict a wide range of various events, both of natural and human generated origin, using neural networks and machine learning approach and methodology . Predictions based only on previous re sponses of the system are especially appealing for solving problems where input variables of the system cannot be clearly defined. But forecasting with neural networks has its own challenges : in particular it is difficult to choose a neural n etwork architecture for a specific problem, since a very simplified model may not be able to learn successfully , and excessively complicated model may lead to overfitting and data memorization . In this paper the application of time series prediction to stock market forecasting is examined, and a comparative study of different neural network structures and different learning methods is performed in order to obtain a better understanding of how the quality of predictions change s with various approaches to sol ving a given problem.},
  citeulike-article-id = {14503714},
  groups               = {FrcstQWIM_TimeSrs, ML_Overfitting, ML_ForcstTimeSrs},
  posted-at            = {2017-12-16 12:38:50},
  timestamp            = {2020-02-25 23:21},
}

@Article{Exterkate-et-al-2016,
  author               = {Exterkate, Peter and Groenen, Patrick J. F. and Heij, Christiaan and van Dijk, Dick},
  date                 = {2016-07},
  journaltitle         = {International Journal of Forecasting},
  title                = {Nonlinear forecasting with many predictors using kernel ridge regression},
  doi                  = {10.1016/j.ijforecast.2015.11.017},
  issn                 = {0169-2070},
  number               = {3},
  pages                = {736--753},
  volume               = {32},
  abstract             = {This paper puts forward kernel ridge regression as an approach for forecasting with many predictors that are related to the target variable nonlinearly. In kernel ridge regression, the observed predictor variables are mapped nonlinearly into a high-dimensional space, where estimation of the predictive regression model is based on a shrinkage estimator in order to avoid overfitting. We extend the kernel ridge regression methodology to enable its use for economic time series forecasting, by including lags of the dependent variable or other individual variables as predictors, as is typically desired in macroeconomic and financial applications. Both Monte Carlo simulations and an empirical application to various key measures of real economic activity confirm that kernel ridge regression can produce more accurate forecasts than traditional linear and nonlinear methods for dealing with many predictors based on principal components.},
  citeulike-article-id = {14030252},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2015.11.017},
  groups               = {Regression_Nonlinear, FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-05-09 00:08:14},
  timestamp            = {2020-02-25 23:21},
}

@Article{Faloutsos-et-al-2018,
  author         = {Faloutsos, Christos and Gasthaus, Jan and Januschowski, Tim and Wang, Yuyang},
  date           = {2018-08-01},
  journaltitle   = {Proc. VLDB Endow.},
  title          = {Forecasting big time series: old and new},
  doi            = {10.14778/3229863.3229878},
  issn           = {2150-8097},
  number         = {12},
  pages          = {2102--2105},
  volume         = {11},
  day            = {1},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs},
  timestamp      = {2020-02-25 23:21},
}

@Article{Fan-et-al-2015a,
  author               = {Fan, Jianqing and Xue, Lingzhou and Yao, Jiawei},
  date                 = {2015-12},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Sufficient Forecasting Using Factor Models},
  eprint               = {1505.07414},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1505.07414},
  abstract             = {We consider forecasting a single time series when there is a large number of predictors and a possible nonlinear effect. The dimensionality was first reduced via a high-dimensional (approximate) factor model implemented by the principal component analysis. Using the extracted factors, we develop a novel forecasting method called the sufficient forecasting, which provides a set of sufficient predictive indices, inferred from high-dimensional predictors, to deliver additional predictive power. The projected principal component analysis will be employed to enhance the accuracy of inferred factors when a semi-parametric (approximate) factor model is assumed. Our method is also applicable to cross-sectional sufficient regression using extracted factors. The connection between the sufficient forecasting and the deep learning architecture is explicitly stated. The sufficient forecasting correctly estimates projection indices of the underlying factors even in the presence of a nonparametric forecasting function. The proposed method extends the sufficient dimension reduction to high-dimensional regimes by condensing the cross-sectional information through factor models. We derive asymptotic properties for the estimate of the central subspace spanned by these projection directions as well as the estimates of the sufficient predictive indices. We further show that the natural method of running multiple regression of target on estimated factors yields a linear estimate that actually falls into this central subspace. Our method and theory allow the number of predictors to be larger than the number of observations. We finally demonstrate that the sufficient forecasting improves upon the linear forecasting in both simulation studies and an empirical study of forecasting macroeconomic variables.},
  citeulike-article-id = {13987321},
  citeulike-linkout-0  = {http://arxiv.org/abs/1505.07414},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1505.07414},
  day                  = {24},
  groups               = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-03-24 15:06:03},
  timestamp            = {2020-02-25 23:21},
}

@Article{Fawaz-et-al-2018,
  author         = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  date           = {2018-09-12},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep learning for time series classification: a review},
  url            = {https://arxiv.org/abs/1809.04356v1},
  abstract       = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state of the art performance for document classification and speech recognition. In this article, we study the current state of the art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
  day            = {12},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Forecast_QWIM, FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:21},
}

@Article{Fawaz-et-al-2019b,
  author         = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  date           = {2019-09-12},
  journaltitle   = {Data Mining and Knowledge Discovery},
  title          = {Deep learning for time series classification: a review},
  url            = {https://arxiv.org/abs/1809.04356v1},
  abstract       = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state of the art performance for document classification and speech recognition. In this article, we study the current state of the art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
  day            = {12},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Forecast_QWIM, FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:21},
}

@Article{Ferreira-SantaClara-2011,
  author               = {Ferreira, Miguel A. and Santa-Clara, Pedro},
  date                 = {2011-06},
  journaltitle         = {Journal of Financial Economics},
  title                = {Forecasting stock market returns: The sum of the parts is more than the whole},
  doi                  = {10.1016/j.jfineco.2011.02.003},
  issn                 = {0304-405X},
  number               = {3},
  pages                = {514--537},
  volume               = {100},
  abstract             = {We propose forecasting separately the three components of stock market returns-the dividend-price ratio, earnings growth, and price-earnings ratio growth-the sum-of-the-parts (SOP) method. Our method exploits the different time series persistence of the components and obtains out-of-sample R-squares (compared with the historical mean) of more than 1.3 percent with monthly data and 13.4 percent with yearly data. This compares with typically negative R-squares obtained in a similar experiment with predictive regressions. The performance of the SOP method comes mainly from the dividend-price ratio and earnings growth components, and the robustness of the method is due to its low estimation error. An investor who timed the market using our method would have had a Sharpe ratio gain of 0.3.},
  citeulike-article-id = {8799981},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jfineco.2011.02.003},
  day                  = {08},
  groups               = {FrcstQWIM_TimeSrs, FcstQWIM_Equity, FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-11-21 17:27:43},
  timestamp            = {2020-02-25 23:21},
}

@Article{Firmino-et-al-2014,
  author               = {Firmino, Paulo R. and de Mattos Neto, Paulo S. G. and Ferreira, Tiago A. E.},
  date                 = {2014-02},
  journaltitle         = {Neural Networks},
  title                = {Correcting and combining time series forecasters},
  doi                  = {10.1016/j.neunet.2013.10.008},
  issn                 = {0893-6080},
  pages                = {1--11},
  volume               = {50},
  abstract             = {Combined forecasters have been in the vanguard of stochastic time series modeling. In this way it has been usual to suppose that each single model generates a residual or prediction error like a white noise. However, mostly because of disturbances not captured by each model, it is yet possible that such supposition is violated. The present paper introduces a two-step method for correcting and combining forecasting models. Firstly, the stochastic process underlying the bias of each predictive model is built according to a recursive ARIMA algorithm in order to achieve a white noise behavior. At each iteration of the algorithm the best ARIMA adjustment is determined according to a given information criterion (e.g. Akaike). Then, in the light of the corrected predictions, it is considered a maximum likelihood combined estimator. Applications involving single ARIMA and artificial neural networks models for Dow Jones Industrial Average Index, SandP 500 Index, Google Stock Value, and Nasdaq Index series illustrate the usefulness of the proposed framework.},
  citeulike-article-id = {13995919},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neunet.2013.10.008},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-04-04 02:44:25},
  timestamp            = {2020-02-25 23:21},
}

@PhdThesis{Fresoli-2014,
  author               = {Fresoli, Diego E.},
  date                 = {2014},
  institution          = {Universidad Carlos III de Madrid},
  title                = {Bootstrap forecasts of multivariate time series},
  abstract             = {In this thesis we study the performance of bootstrap procedures to approximate forecast densities and their intervals and regions for multivariate time series data. In particular, we develop bootstrap procedures for VAR and DCC models which are often implemented when modeling and forecasting macroeconomic and financial time series. The bootstrap methodology considered in this thesis is attractive since it is free of distributional assumptions and well suited to incorporate the parameter uncertainty and can be even designed to deal with the model uncertainty. Chapter 1 introduces the VAR and DCC models and the standard approach to construct forecast densities within them. The problems that arise when forecasting these models push us to consider alternatives, some of them based on bootstrapping. By then, it will be a propitious time to briefly introduce the bootstrap methodology for time dependent data and review its implementation to forecast problems. Chapter 2 establishes the asymptotic validity and analyses the finite sample performance of a simple bootstrap procedure to construct multi-step multivariate forecast densities in the context of non-Gaussian unrestricted VAR models. This bootstrap procedure avoids the backward representation used by existing alternatives and, consequently, can be implemented to obtain multivariate forecast densities in, for example, VARMA or VAR-GARCH models. In the context of bivariate stationary VAR models, we carry out several Monte Carlo experiments to study its finite sample properties, finding that these are comparable to those of alternatives based on the backward representation. Hence we remark that nothing is lost when we abandon the more complicated backward representation. This result is also suggested by one of the empirical applications, in which we construct joint forecast densities of US quarterly inflation, unemployment and GDP growth and the corresponding forecast regions with their empirical coverages obtained using a rolling window scheme. Finally, we reproduce a textbook example that applies the standard Gaussian methodology to forecast West German investment, consumption and income and then, for comparison purposes, we include the bootstrap forecast intervals. The model, the parameters and the error distribution are rarely known without uncertainty by the forecaster and, thus, the sampling variability caused by the use of the estimated model needs to be taken into account. Bootstrap methods are successfully designed to deal with different sources of uncertainties in the context of forecasting VAR models. For this reason, Chapter 3 compares the forecast performance of the regions constructed using the traditional Gaussian methodology and several variants of the bootstrap procedure that successively incorporate error distribution, parameter uncertainty, bias correction and lag order uncertainty. Our Monte Carlo study suggests that the parameter uncertainty plays a prominent role when forecasting highly persistent VAR models. Regarding DCC forecast, there are two problem that deserve attention. First, the non-Gaussianity of returns demands alternative ways of approximating its forecast density. Second, only point forecasts of volatilities, covariances and correlations can be obtained at each moment of time. These issues encouraged us to developed bootstrap procedure to forecast returns, volatilities, covariances and correlations in corrected DCC models, which is described in Chapter 4. We conduct Monte Carlo simulations in order to evaluate its finite sample properties, which show a rather good performance of the bootstrap procedure under different sample sizes and error distributions. We apply the proposed bootstrap algorithm to two systems of returns. First, we obtain out-of-sample forecast of returns, volatilities, covariances and correlations in the context of a system of daily exchange rates returns of the Euro, Japanese Yen and Australian Dollar against the US Dollar. Second, we construct within sample forecast intervals of the conditional correlation of SandP 500 and NASDAQ returns. Both empirical applications point out that our bootstrap algorithm can provide additional information that enriches the DCC forecast approach. Finally, Chapter 5 concludes and presents the research lines that are still open.},
  citeulike-article-id = {14374613},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-06-12 18:10:18},
  timestamp            = {2020-02-25 23:21},
}

@Article{Galicia-et-al-2019,
  author         = {Galicia, A. and Talavera-Llames, R. and Troncoso, A. and Koprinska, I. and Martinez-Alvarez, F.},
  date           = {2019-01},
  journaltitle   = {Knowledge-Based Systems},
  title          = {Multi-step forecasting for big data time series based on ensemble learning},
  doi            = {10.1016/j.knosys.2018.10.009},
  issn           = {0950-7051},
  pages          = {830--841},
  volume         = {163},
  abstract       = {This paper presents ensemble models for forecasting big data time series. An ensemble composed of three methods (decision tree, gradient boosted trees and random forest) is proposed due to the good results these methods have achieved in previous big data applications. The weights of the ensemble are computed by a weighted least square method. Two strategies related to the weight update are considered, leading to a static or dynamic ensemble model. The predictions for each ensemble member are obtained by dividing the forecasting problem into hh forecasting sub-problems, one for each value of the prediction horizon. These sub-problems have been solved using machine learning algorithms from the big data engine Apache Spark, ensuring the scalability of our methodology. The performance of the proposed ensemble models is evaluated on Spanish electricity consumption data for 10 years measured with a 10-minute frequency. The results showed that both the dynamic and static ensembles performed well, outperforming the individual ensemble members they combine. The dynamic ensemble was the most accurate model achieving a MRE of 2\%, which is a very promising result for the prediction of big time series. Proposed ensembles are also evaluated using solar power from Australia for two years measured with 30-min frequency. The results are successfully compared with Artificial Neural Network, Pattern Sequence-based Forecasting and Deep Learning, improving their results.},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Classif_QWIM, ML_Network_QWIM, ML_Forecast_QWIM, FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_PerfMetrics, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:21},
}

@Article{Gamboa-2017,
  author               = {John Cristian Borges Gamboa},
  date                 = {2016},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Deep Learning for Time-Series Analysis},
  eprint               = {1701.01887},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1701.01887},
  abstract             = {In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.},
  citeulike-article-id = {14255493},
  citeulike-linkout-0  = {http://arxiv.org/abs/1701.01887},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1701.01887},
  day                  = {7},
  groups               = {FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_ForcstTimeSrs},
  posted-at            = {2017-10-11 17:52:42},
  timestamp            = {2020-02-25 23:21},
  year                 = {2017},
}

@InCollection{GonzalezRivera-Lee-2009,
  author               = {Gonzalez-Rivera, Gloria and Lee, Tae-Hwy},
  booktitle            = {Complex Systems in Finance and Econometrics},
  date                 = {2011},
  title                = {Financial Forecasting, Non-linear Time Series},
  doi                  = {10.1007/978-1-4419-7701-4\_23},
  editor               = {Meyers, Robert A.},
  pages                = {394--423},
  publisher            = {Springer New York},
  citeulike-article-id = {14067728},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-1-4419-7701-423},
  citeulike-linkout-1  = {http://link.springer.com/referenceworkentry/10.1007/978-1-4419-7701-423},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-06-12 16:20:47},
  timestamp            = {2020-02-25 23:21},
}

@Article{Grzesica-2017,
  author               = {Grzesica, Dariusz},
  date                 = {2017-01-27},
  journaltitle         = {International conference KNOWLEDGE-BASED ORGANIZATION},
  title                = {The Decomposition Issue of a Time Series in the Forecasting Process},
  doi                  = {10.1515/kbo-2017-0154},
  issn                 = {2451-3113},
  number               = {3},
  volume               = {23},
  abstract             = {Decomposition of time series is the estimate and extraction of deterministic part of the series - trend, cyclical and seasonal fluctuations in the hope that the rest of the data, that is, theoretically, a random variable will be stationary random process. During the process of predicting the time series elements affects significantly on the determination of the future values, which are characterized by a low forecast error. Therefore, the purpose of this article is to identify the elements of the time series decomposition and to determine the extent to which they affect the forecasting process. Problems that often appear when you run the forecast and methods of building models and forecasts based on time series will be presented. Observations will be described on the basis of nonparametric time series modeling.},
  citeulike-article-id = {14503531},
  citeulike-linkout-0  = {http://dx.doi.org/10.1515/kbo-2017-0154},
  day                  = {27},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-12-15 17:55:33},
  timestamp            = {2020-02-25 23:21},
}

@Article{Guerrero-et-al-2014,
  author               = {Guerrero, Vctor M. and Silva, Eliud and Gomez, Nicolas},
  date                 = {2014-01},
  journaltitle         = {Journal of Forecasting},
  title                = {Building Scenarios of Multiple Time Series that Take into Account the Effects of an Expected Intervention},
  doi                  = {10.1002/for.2271},
  number               = {1},
  pages                = {32--46},
  volume               = {33},
  abstract             = {We consider a forecasting problem that arises when an intervention is expected to occur on an economic system during the forecast horizon. The time series model employed is seen as a statistical device that serves to capture the empirical regularities of the observed data on the variables of the system without relying on a particular theoretical structure. Either the deterministic or the stochastic structure of a vector autoregressive error correction model of the system is assumed to be affected by the intervention. The information about the intervention effect is just provided by some linear restrictions imposed on the future values of the variables involved.

Formulas for restricted forecasts with intervention effects and their mean squared errors are derived as a particular case of Catlin's static updating theorem. An empirical illustration uses Mexican macroeconomic data on five variables and the restricted forecasts consider targets for years 2011-2014.},
  citeulike-article-id = {13935009},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2271},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:09:42},
  timestamp            = {2020-02-25 23:21},
}

@InCollection{Gupta-Chatterjee-2018,
  author               = {Gupta, Kartikay and Chatterjee, Niladri},
  booktitle            = {Information and Communication Technology for Intelligent Systems (ICTIS 2017) - Volume 2},
  date                 = {2018},
  title                = {Financial Time Series Clustering},
  doi                  = {10.1007/978-3-319-63645-0\_16},
  editor               = {Satapathy, Suresh C. and Joshi, Amit},
  pages                = {146--156},
  publisher            = {Springer International Publishing},
  series               = {Smart Innovation, Systems and Technologies},
  volume               = {84},
  abstract             = {Financial time series clustering finds application in forecasting, noise reduction and enhanced index tracking. The central theme in all the available clustering algorithms is the dissimilarity measure employed by the algorithm. The dissimilarity measures, applicable in financial domain, as used or suggested in past researches, are correlation based dissimilarity measure, temporal correlation based dissimilarity measure and dynamic time wrapping (DTW) based dissimilarity measure. One shortcoming of these dissimilarity measures is that they do not take into account the lead or lag existing between the returns of different stocks which changes with time. Mostly, such stocks with high value of correlation at some lead or lag belong to the same cluster (or sector). The present paper, proposes two new dissimilarity measures which show superior clustering results as compared to past measures when compared over 3 data sets comprising of 526 companies.},
  citeulike-article-id = {14435138},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-63645-016},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-63645-016},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-09-21 00:23:24},
  timestamp            = {2020-02-25 23:21},
}

@InCollection{Hadla-et-al-2017,
  author               = {Hadla, Masar and Coakley, Jerry and Snaith, Stuart},
  booktitle            = {European Financial Management Association Annual Meeting Athens},
  date                 = {2017},
  title                = {Forecasting Exchange Rates - a factor approach},
  abstract             = {In this paper, our analysis of exchange rate prediction builds upon on the factor approach proposed by Engel, Mark and West (2015). Our analysis is applied to 13 OECD currencies 1991:01-2013:04 quoted in US dollars at a monthly frequency. Following the Engel et al. (2015) approach factors do not rely on economic fundamentals, but are extracted from the panel of exchange rates. The underlying rationale is exchange rate series tend to co-move over time and thus contain information that is hard to extract from observable fundamentals. Thereafter these factors and economic fundamentals are used to demonstrate a degree of predictability in foreign exchange. We contribute the existing literature by augmenting Engel et al.'s (2015) approach by utilising a more exhaustive set of economic fundamentals for each of these economies. Whilst Engel et al. test forecasting models against the random walk, we adopt a more comprehensive forecasting exercise addressing the performance of each competing model to try to determine which model is best able to forecast spot exchange rates in each economy tested. Finally, we examine the effect of the recent financial crisis on our forecasting models. For the full sample, forecasting results demonstrate that models which relied on PPP fundamentals did particular well at beating the random walk across a large cross-section of economies. However, in a horse race of all competing forecasting models a surprising level heterogeneity in model performance is uncovered. Turning to how the forecasts were affected by the crisis, results indicate that, across all economies, the random walk is more readily rejected during the crisis period. Whilst this result is expected, comparing forecasting performance between economies again uncovers a surprising degree of heterogeneity.},
  citeulike-article-id = {14364558},
  groups               = {Predictability_FinInfo, FrcstQWIM_TimeSrs, FrcstQWIM_Other},
  posted-at            = {2017-05-29 16:22:22},
  timestamp            = {2020-02-25 23:21},
}

@Article{Hillebrand-Medeiros-2015,
  author               = {Hillebrand, Eric and Medeiros, Marcelo C.},
  date                 = {2016-01},
  journaltitle         = {Journal of Business and Economic Statistics},
  title                = {Nonlinearity, Breaks, and Long-Range Dependence in Time-Series Models},
  doi                  = {10.1080/07350015.2014.985828},
  number               = {1},
  pages                = {23--41},
  volume               = {34},
  abstract             = {We study the simultaneous occurrence of long memory and nonlinear effects, such as parameter changes and threshold effects, in time series models and apply our modeling framework to daily realized measures of integrated variance. We develop asymptotic theory for parameter estimation and propose two model-building procedures. The methodology is applied to stocks of the Dow Jones Industrial Average during the period 2000 to 2009. We find strong evidence of nonlinear effects in financial volatility. An out-of-sample analysis shows that modeling these effects can improve forecast performance. Supplementary materials for this article are available online.},
  citeulike-article-id = {14014331},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/07350015.2014.985828},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/07350015.2014.985828},
  day                  = {2},
  groups               = {FrcstQWIM_TimeSrs, Data_NonLinear},
  owner                = {cristi},
  posted-at            = {2016-04-17 17:26:51},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-25 23:21},
}

@Article{Hsu-2017,
  author               = {Hsu, Daniel},
  date                 = {2017-07-06},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Time Series Forecasting Based on Augmented Long Short-Term Memory},
  eprint               = {1707.00666},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1707.00666},
  abstract             = {In this paper, we use recurrent autoencoder model to predict the time series in single and multiple steps ahead. Previous prediction methods, such as recurrent neural network (RNN) and deep belief network (DBN) models, cannot learn long term dependencies. And conventional long short-term memory (LSTM) model doesn't remember recent inputs. Combining LSTM and autoencoder (AE), the proposed model can capture long-term dependencies across data points and uses features extracted from recent observations for augmenting LSTM at the same time. Based on comprehensive experiments, we show that the proposed methods significantly improves the state-of-art performance on chaotic time series benchmark and also has better performance on real-world data. Both single-output and multiple-output predictions are investigated.},
  citeulike-article-id = {14503534},
  citeulike-linkout-0  = {http://arxiv.org/abs/1707.00666},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1707.00666},
  day                  = {6},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-12-15 18:04:36},
  timestamp            = {2020-02-25 23:21},
}

@Article{DalPra-et-al-2018,
  author               = {Dal Pra, Giulia and Guidolin, Massimo and Pedio, Manuela and Vasile, Fabiola},
  date                 = {2018-01-24},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Regime Shifts in Excess Stock Return Predictability: An Out-of-Sample Portfolio Analysis},
  doi                  = {10.3905/jpm.2018.44.3.010},
  issn                 = {0095-4918},
  number               = {3},
  pages                = {10--24},
  volume               = {44},
  abstract             = {The authors analyze the out-of-sample performance of asset allocation decisions based on financial ratio predictability of aggregate stock market returns under linear and regime-switching models. The authors adopt both a statistical perspective to analyze whether models based on valuation ratios can forecast excess equity returns, and an economic approach that turns predictions into portfolio strategies. These consist of a portfolio switching approach, a mean-variance framework, and a long-run dynamic model. The authors find a disconnect between the statistical perspective, whereby the ratios yield a modest forecasting power, and a portfolio approach, by which a moderate predictability is often sufficient to yield significant portfolio outperformance, especially before transaction costs and when regimes are taken into account. However, also when regimes are considered, predictability gives high payoffs only to long horizon, highly risk-averse investors. Moreover, different strategies deliver different performance rankings across predictors.},
  citeulike-article-id = {14525018},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2018.44.3.010},
  day                  = {24},
  groups               = {DAA, Predictability_Return_Other, Regime_Invest, Predictability_FinInfo, Regime_Model, Invest_Regime, FcstQWIM_Equity, FrcstQWIM_Test, [nbkcbu3:]},
  posted-at            = {2018-01-27 17:01:51},
  timestamp            = {2020-02-25 23:21},
}

@Article{Davis-et-al-2018,
  author               = {Davis, Joseph and Aliaga-Diaz, Roger and Ahluwalia, Harshdeep and Tolani, Ravi},
  date                 = {2018-01-24},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Improving U.S. Stock Return Forecasts: A "Fair-Value" CAPE Approach},
  doi                  = {10.3905/jpm.2018.44.3.043},
  issn                 = {0095-4918},
  number               = {3},
  pages                = {43--55},
  volume               = {44},
  abstract             = {The accuracy of U.S. stock return forecasts based on the cyclically-adjusted P/E (CAPE) ratio has deteriorated since 1985. A primary reason has been that traditional CAPE regressions assume the CAPE ratio reverts mechanically to its long-run average, regardless of the macroeconomic environment. In this article, the authors propose an enhancement to standard CAPE-based forecasts that conditions mean reversion in the CAPE ratio on real (not nominal) bond yields, expected inflation rates, and financial volatility in a VAR model. The forecasted results are promising for both real and nominal returns, with out-of-sample forecast errors approximately 50\% lower than traditional approaches. The differences are economically meaningful and statistically significant over the forecast period examined. At present, low real bond yields imply low real earnings yields and an above-average "fair-value" CAPE ratio. Nevertheless, with Shiller's CAPE ratio now well above its fair value, the model proposed by the author predicts nominal U.S. stock returns centered near 5\% over the next decade.},
  citeulike-article-id = {14525034},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2018.44.3.043},
  day                  = {24},
  groups               = {FcstQWIM_Bond, FcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2018-01-27 17:21:38},
  timestamp            = {2020-02-25 23:21},
}

@Article{Degiannakis-et-al-2018,
  author         = {Degiannakis, Stavros and Filis, George and Hassani, Hossein},
  date           = {2018-03},
  journaltitle   = {Journal of Empirical Finance},
  title          = {Forecasting global stock market implied volatility indices},
  doi            = {10.1016/j.jempfin.2017.12.008},
  issn           = {0927-5398},
  pages          = {111--129},
  volume         = {46},
  abstract       = {Abstract This study compares parametric and non-parametric techniques in terms of their forecasting power on implied volatility indices. We extend our comparisons using combined and model-averaging models. The forecasting models are applied on eight implied volatility indices of the most important stock market indices. We provide evidence that the non-parametric models of Singular Spectrum Analysis combined with Holt-Winters (-HW) exhibit statistically superior predictive ability for the one and ten trading days ahead forecasting horizon. By contrast, the model-averaged forecasts based on both parametric (Autoregressive Integrated model) and non-parametric models (-HW) are able to provide improved forecasts, particularly for the ten trading days ahead forecasting horizon. For robustness purposes, we build two trading strategies based on the aforementioned forecasts, which further confirm that the -HW and the ARI--HW are able to generate significantly higher net daily returns in the out-of-sample period.},
  f1000-projects = {QuantInvest},
  groups         = {FcstQWIM_Equity, FrcstQWIM_Test},
  timestamp      = {2020-02-25 23:21},
}

@Article{Detzel-Strauss-2017,
  author               = {Detzel, Andrew and Strauss, Jack},
  date                 = {2017-07},
  journaltitle         = {Review of Finance},
  title                = {Combination Return Forecasts and Portfolio Allocation with the Cross-Section of Book-to-Market Ratios},
  doi                  = {10.1093/rof/rfx035},
  issn                 = {1572-3097},
  abstract             = {In this paper, we forecast industry returns out-of-sample using the cross-section of book-to-market (BM) ratios and investigate whether investors can exploit this predictability in portfolio allocation. Cash-flow and return forecasting regressions show that cross-industry BM ratios contain significant predictive information beyond aggregate and industry-specific BM ratios. Forecast combination methods based on industry BM ratios generate significant out-of-sample predictability for many industries. Real-time portfolio-rotation strategies that buy industries with high predicted returns and short industries with low predicted returns based on combination forecasts earn significant alpha with respect to standard asset pricing models net of transaction costs.},
  citeulike-article-id = {14412352},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rof/rfx035},
  day                  = {10},
  groups               = {Predictability_Return_Other, FrcstQWIM_MedLngTerm, FrcstQWIM_Test},
  posted-at            = {2017-08-11 06:31:42},
  timestamp            = {2020-02-25 23:21},
}

@Article{Diebold-2015,
  author               = {Diebold, Francis X.},
  date                 = {2015-01},
  journaltitle         = {Journal of Business and Economic Statistics},
  title                = {Comparing Predictive Accuracy, Twenty Years Later: A Personal Perspective on the Use and Abuse of Diebold-Mariano Tests},
  doi                  = {10.1080/07350015.2014.983236},
  number               = {1},
  pages                = {1},
  volume               = {33},
  abstract             = {The Diebold-Mariano (DM) test was intended for comparing forecasts; it has been, and remains, useful in that regard. The DM test was not intended for comparing models. Much of the large ensuing literature, however, uses DM-type tests for comparing models, in pseudo-out-of-sample environments. In that case, simpler yet more compelling full-sample model comparison procedures exist; they have been, and should continue to be, widely used. The hunch that pseudo-out-of-sample analysis is somehow the only , or best , or even necessarily a good way to provide insurance against in-sample overfitting in model comparisons proves largely false. On the other hand, pseudo-out-of-sample analysis remains useful for certain tasks, perhaps most notably for providing information about comparative predictive performance during particular historical episodes.},
  citeulike-article-id = {13796157},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/07350015.2014.983236},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/07350015.2014.983236},
  day                  = {2},
  groups               = {FrcstQWIM_Test},
  owner                = {zkgst0c},
  posted-at            = {2016-06-06 19:31:25},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-25 23:21},
}

@Article{Dowd-et-al-2010a,
  author               = {Dowd, Kevin and Cairns, Andrew J. G. and Blake, David and Coughlan, Guy D. and Epstein, David and Khalaf-Allah, Marwa},
  date                 = {2010-07},
  journaltitle         = {North American Actuarial Journal},
  title                = {Backtesting Stochastic Mortality Models},
  doi                  = {10.1080/10920277.2010.10597592},
  number               = {3},
  pages                = {281--298},
  volume               = {14},
  abstract             = {This study sets out a backtesting framework applicable to the multiperiod-ahead forecasts from stochastic mortality models and uses it to evaluate the forecasting performance of six different stochastic mortality models applied to English a version of Renshaw-Haberman?s 2006 extension of the Lee-Carter model to allow for a cohort effect; the age-period-cohort model, which is a simplified version of Renshaw-Haberman; Cairns, Blake, and Dowd 2006 two-factor model; and two generalized versions of the last named with an added cohort effect.

For the data set used herein, the results from applying this methodology suggest that the models perform adequately by most backtests and that prediction intervals that incorporate parameter uncertainty are wider than those that do not. We also find little difference between the performances of five of the models, but the remaining model shows considerable forecast instability.},
  citeulike-article-id = {13940548},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/10920277.2010.10597592},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/10920277.2010.10597592},
  day                  = {1},
  groups               = {FrcstQWIM_Test},
  owner                = {zkgst0c},
  posted-at            = {2016-02-25 19:50:25},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 23:21},
}

@Article{Esposito-Cummins-2016,
  author               = {Esposito, Francesco P. and Cummins, Mark},
  date                 = {2016-01},
  journaltitle         = {Journal of Forecasting},
  title                = {Multiple Hypothesis Testing of Market Risk Forecasting Models},
  doi                  = {10.1002/for.2381},
  pages                = {n/a},
  abstract             = {Extending previous risk model backtesting literature, we construct multiple hypothesis testing (MHT) with the stationary bootstrap. We conduct multiple tests which control for the generalized confidence level and employ the bootstrap MHT to design multiple comparison testing. We consider absolute and relative predictive ability to test a range of competing risk models, focusing on value-at-risk and expected shortfall (ExS). In devising the test for the absolute predictive ability, we take the route of recent literature and construct balanced simultaneous confidence sets that control for the generalized family-wise error rate, which is the joint probability of rejecting true hypotheses. We implement a step-down method which increases the power of the MHT in isolating false discoveries. In testing for the ExS model predictive ability, we design a new simple test to draw inference about recursive model forecasting capability. In the second suite of statistical testing, we develop a novel device for measuring the relative predictive ability in the bootstrap MHT framework. The device, which we coin multiple comparison mapping, provides a statistically robust instrument designed to answer the question: 'Which model is the best model?'},
  citeulike-article-id = {14071866},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2381},
  day                  = {1},
  groups               = {Test_MultiHypotheses, FrcstQWIM_Test, Stat_Test, Proba_Test},
  owner                = {cristi},
  posted-at            = {2016-06-18 19:05:28},
  timestamp            = {2020-02-25 23:21},
}

@Article{Fischer-et-al-2016,
  author               = {Fischer, Henning and Blanco-Fernandez, Angela and Winker, Peter},
  date                 = {2016-03},
  journaltitle         = {Journal of Forecasting},
  title                = {Predicting Stock Return Volatility: Can We Benefit from Regression Models for Return Intervals?},
  doi                  = {10.1002/for.2371},
  number               = {2},
  pages                = {113--146},
  volume               = {35},
  abstract             = {We study the performance of recently developed linear regression models for interval data when it comes to forecasting the uncertainty surrounding future stock returns. These interval data models use easy-to-compute daily return intervals during the modeling, estimation and forecasting stage. They have to stand up to comparable point-data models of the well-known capital asset pricing model type which employ single daily returns based on successive closing prices and might allow for GARCH effects in a comprehensive out-of-sample forecasting competition. The latter comprises roughly 1000 daily observations on all 30 stocks that constitute the DAX, Germany's main stock index, for a period covering both the calm market phase before and the more turbulent times during the recent financial crisis. The interval data models clearly outperform simple random walk benchmarks as well as the point-data competitors in the great majority of cases. This result does not only hold when one-day-ahead forecasts of the conditional variance are considered, but is even more evident when the focus is on forecasting the width or the exact location of the next day's return interval. Regression models based on interval arithmetic thus prove to be a promising alternative to established point-data volatility forecasting tools.},
  citeulike-article-id = {14030233},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2371},
  day                  = {1},
  groups               = {FcstQWIM_Equity, FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-05-08 23:55:06},
  timestamp            = {2020-02-25 23:21},
}

@Article{Fugazza-et-al-2015,
  author               = {Fugazza, Carolina and Guidolin, Massimo and Nicodano, Giovanna},
  date                 = {2015-09},
  journaltitle         = {European Financial Management},
  title                = {Equally Weighted vs. Long-Run Optimal Portfolios},
  doi                  = {10.1111/eufm.12042},
  number               = {4},
  pages                = {742--789},
  volume               = {21},
  abstract             = {Out-of-sample experiments cast doubt on the ability of portfolio optimising strategies to outperform equally weighted portfolios, when investors have a 1-month time horizon. This paper examines whether this finding holds for longer investment horizons over which the optimising strategy exploits linear predictability in returns. Our experiments indicate that investors with longer horizons on average would have benefited, ex post, from an optimising strategy over the period 1995 2009. We analyse performance sensitivity to investor risk aversion, to the number of predictors included in the forecasting model and to the deduction of transaction costs from portfolio performance.},
  citeulike-article-id = {13990153},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/eufm.12042},
  day                  = {1},
  groups               = {FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:50:44},
  timestamp            = {2020-02-25 23:21},
}

@Article{Gagnon-et-al-2017,
  author               = {Gagnon, Marie-Helene and Power, Gabriel and Toupin, Dominique},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Forecasting International Index Returns Using Option-Implied Variables},
  url                  = {https://ssrn.com/abstract=3091953},
  abstract             = {This paper investigates international index return predictability using option-implied information. We document the significant predictive power of the variance risk premium, Foster-Hart risk (FH) and higher-order moments for horizons ranging from 1 to 250 days. Our results from predictive regressions and out-of-sample forecasts suggest that a model containing these four risk-neutral metrics has predictive power internationally. The variance risk premium is a significant predictor for several horizons, including less than one month, while FH risk is significant particularly for longer horizons. Risk-neutral skewness and kurtosis are significant for several countries in predictive regressions, but often lose significance in out-of-sample forecasts.},
  citeulike-article-id = {14510834},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3091953},
  groups               = {Predictability_FinInfo, FrcstQWIM_MedLngTerm, FrcstQWIM_Test},
  posted-at            = {2018-01-02 00:25:00},
  timestamp            = {2020-02-25 23:21},
}

@Article{Gambacciani-Paolella-2017,
  author               = {Gambacciani, Marco and Paolella, Marc S.},
  date                 = {2017-03},
  journaltitle         = {Econometrics and Statistics},
  title                = {Robust normal mixtures for financial portfolio allocation},
  doi                  = {10.1016/j.ecosta.2017.02.003},
  issn                 = {2452-3062},
  abstract             = {A new approach for multivariate modelling and prediction of asset returns is proposed. It is based on a two-component normal mixture, estimated using a fast new variation of the minimum covariance determinant (MCD) method made suitable for time series. It outperforms the (shrinkage-augmented) MLE in terms of out-of-sample density forecasts and portfolio performance. In addition to the usual stylized facts of skewness and leptokurtosis, the model also accommodates leverage and contagion effects, but is i.i.d., and thus does not embody, for example, a GARCH-type structure. Owing to analytic tractability of the moments and the expected shortfall, portfolio optimization is straightforward, and, for daily equity returns data, is shown to substantially outperform the equally weighted and classical long-only Markowitz framework, as well as DCC-GARCH (despite not using any kind of GARCH-type filter).},
  citeulike-article-id = {14334320},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ecosta.2017.02.003},
  groups               = {PortfOptim_Robust, FcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2017-04-08 19:12:50},
  timestamp            = {2020-02-25 23:21},
}

@Article{Gargano-et-al-2018,
  author               = {Gargano, Antonio and Pettenuzzo, Davide and Timmermann, Allan},
  date                 = {2018-09-22},
  journaltitle         = {Management Science},
  title                = {Bond Return Predictability: Economic Value and Links to the Macroeconomy},
  doi                  = {10.1287/mnsc.2017.2829},
  issn                 = {0025-1909},
  abstract             = {Studies of bond return predictability find a puzzling disparity between strong statistical evidence of return predictability and the failure to convert return forecasts into economic gains. We show that resolving this puzzle requires accounting for important features of bond return models such as volatility dynamics and unspanned macro factors. A three-factor model comprising a forward spread, a weighted combination of forward rates, and a macro factor generates notable gains in out-of-sample forecast accuracy compared with a model based on the expectations hypothesis. Such gains in predictive accuracy translate into higher risk-adjusted portfolio returns after accounting for estimation error and model uncertainty. Consistent with models featuring unspanned macro factors, our forecasts of future bond excess returns are strongly negatively correlated with survey forecasts of short rates.},
  citeulike-article-id = {14514965},
  citeulike-linkout-0  = {http://dx.doi.org/10.1287/mnsc.2017.2829},
  day                  = {22},
  groups               = {Predictability_FinInfo, FcstQWIM_Bond, FrcstQWIM_Test},
  posted-at            = {2018-01-10 23:34:11},
  timestamp            = {2020-02-25 23:21},
}

@Article{Guidolin-et-al-2017,
  author               = {Guidolin, Massimo and Orlov, Alexei G. and Pedio, Manuela},
  date                 = {2017-08-31},
  journaltitle         = {Quantitative Finance},
  title                = {How good can heuristic-based forecasts be? A comparative performance of econometric and heuristic models for UK and US asset returns},
  doi                  = {10.1080/14697688.2017.1351619},
  issn                 = {1469-7688},
  pages                = {1--31},
  abstract             = {This paper systematically investigates the sources of differential out-of-sample predictive accuracy of heuristic frameworks based on internet search frequencies and a large set of econometric models. The volume of internet searches helps gauge the degree of investors' time-varying interest in specific assets. We use a wide range of state-of-the-art models, both of linear and nonlinear type (regime-switching predictive regressions, threshold autoregressive, smooth transition autoregressive), extended to capture conditional heteroskedasticity through GARCH models. The predictor variables investigated are those typical of the literature featuring a range of macroeconomic and market leading indicators. Our out-of-sample forecasting exercises are conducted with reference to US, UK, French and German data, both stocks and bonds, and for 1- and 12-months-ahead horizons. We employ several forecast performance metrics and predictive accuracy tests. Internet-search-based models are found to perform better than the average of all of the alternative models. For several country-asset-horizon combinations, particularly for UK bond returns, our heuristic models compare favourably with sophisticated econometric methods. The heuristic models are also shown to perform well in forecasting realized volatility. The baseline results are supported by several extensions and robustness checks, such as using alternative search keywords, controlling for Fama-French and Cochrane-Piazzesi factors, and implementing heuristic-based trading strategies.},
  citeulike-article-id = {14497329},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2017.1351619},
  day                  = {31},
  groups               = {Regression_Nonlinear, FrcstQWIM_Test},
  posted-at            = {2017-12-06 05:13:51},
  timestamp            = {2020-02-25 23:21},
}

@Article{Habibnia-2017,
  author               = {Habibnia, Ali},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Nonlinear Forecasting Using a Large Number of Predictors},
  url                  = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2020212},
  abstract             = {This paper considers forecasting in a big data environment. We develop a category of nonlinear forecasting based on factor models to benefit from many potential predictors while accounting for any possible nonlinear dynamics within the environment. The problem of forecasting with factor models is a two-step procedure. Proposed model, at the first step, employs an autoassociative neural networkto estimate nonlinear factors from a large panel of predictors, and at the second step, applies a nonlinear function on the estimated factors to predict a single time series. Such features can go beyond the covariance structure analysis and enhance the accuracy of forecasting. Applying this approach to forecast equity returns, the proposed model captures the nonlinear dynamic between equities to enhance the performance of the subsequent forecast. This offers a significant improvement to current univariate and multivariate models. We emphasize the fact that linear models can be seen as a special case of the proposed nonlinear model, which basically implies that in the event that nonlinearity is absent between series, the model will subsequently be reduced to a linear model. The empirical results on daily returns of equities on the SandP 500 index from 2005 - 2014 proved the superiority of the out-of-sample forecasting ability of this model vis-a-vis competing approaches.},
  citeulike-article-id = {14334322},
  citeulike-linkout-0  = {http://personal.lse.ac.uk/habibnia/AliHabibniaJMP.pdf},
  groups               = {Data_NonLinear, FcstQWIM_Equity, FrcstQWIM_Test},
  howpublished         = {Available at http://personal.lse.ac.uk/habibnia/AliHabibniaJMP.pdf},
  posted-at            = {2017-04-08 19:30:20},
  timestamp            = {2020-02-25 23:21},
}

@Article{Hambuckers-Heuchenne-2016,
  author               = {Hambuckers, Julien and Heuchenne, Cedric},
  date                 = {2016-07},
  journaltitle         = {Journal of Forecasting},
  title                = {Estimating the Out-of-Sample Predictive Ability of Trading Rules: A Robust Bootstrap Approach},
  doi                  = {10.1002/for.2380},
  number               = {4},
  pages                = {347--372},
  volume               = {35},
  abstract             = {In this paper, we provide a novel way to estimate the out-of-sample predictive ability of a trading rule. Usually, this ability is estimated using a sample-splitting scheme, true out-of-sample data being rarely available. We argue that this method makes poor use of the available data and creates data-mining possibilities. Instead, we introduce an alternative.632 bootstrap approach. This method enables building in-sample and out-of-sample bootstrap datasets that do not overlap but exhibit the same time dependencies. We show in a simulation study that this technique drastically reduces the mean squared error of the estimated predictive ability. We illustrate our methodology on IBM, MSFT and DJIA stock prices, where we compare 11 trading rules specifications. For the considered datasets, two different filter rule specifications have the highest out-of-sample mean excess returns. However, all tested rules cannot beat a simple buy-and-hold strategy when trading at a daily frequency.},
  citeulike-article-id = {14071857},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2380},
  day                  = {1},
  groups               = {FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-06-18 19:17:36},
  timestamp            = {2020-02-25 23:21},
}

@Article{Hammerschmid-Lohre-2015,
  author               = {Hammerschmid, Regina and Lohre, Harald},
  date                 = {2015-06},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Regime Shifts and Stock Return Predictability},
  url                  = {https://ssrn.com/abstract=2445086},
  abstract             = {Identifying economic regimes ought to be useful in a world of time-varying risk premia. We apply regime switching models to common factors proxying for the macroeconomic regime and document the ensuing regime factors to be relevant in forecasting returns. Moreover, the relevance of these regime factors is preserved in the presence of fundamental variables known to predict equity risk premia. This finding continues to hold when additionally considering technical indicators. In particular, the predictive power of the three information sets (fundamental, technical and macroeconomic regime) is complementary and gives rise to significant out-of-sample predictability.},
  citeulike-article-id = {13925719},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2445086},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2652487code694027.pdf?abstractid=2445086 and mirid=1},
  day                  = {4},
  groups               = {Regime_Identif, FcstQWIM_Equity, FrcstQWIM_Test},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2445086},
  owner                = {zkgst0c},
  posted-at            = {2016-02-04 22:30:24},
  timestamp            = {2020-02-25 23:21},
}

@Article{Hammerschmid-Lohre-2018,
  author               = {Hammerschmid, Regina and Lohre, Harald},
  date                 = {2017-11},
  journaltitle         = {International Review of Economics and Finance},
  title                = {Regime Shifts and Stock Return Predictability},
  doi                  = {10.1016/j.iref.2017.10.021},
  url                  = {https://www.sciencedirect.com/science/article/pii/S1059056017308134},
  volume               = {56},
  abstract             = {Identifying economic regimes is useful in a world of time-varying risk premia. We apply regime switching models to common factors proxying for the macroeconomic regime and show that the ensuing regime factor is relevant in forecasting the equity risk premium. Moreover, the relevance of this regime factor is preserved in the presence of fundamental variables and technical indicators which are known to predict equity risk premia. Based on multiple predictive regressions and pooled forecasts, the macroeconomic regime factor is deemed complementary relative to the fundamental and technical information sets. Finally, these forecasts exhibit significant out-of-sample predictability that ultimately translates into considerable utility gains in a mean-variance portfolio strategy.},
  citeulike-article-id = {13925719},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2445086},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2652487code694027.pdf?abstractid=2445086 and mirid=1},
  day                  = {4},
  groups               = {Regime_Invest, Regime_Identif, Predictability_FinInfo, Invest_Regime, FcstQWIM_Equity, FrcstQWIM_Test},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2445086},
  journal              = {International Review of Economics and Finance},
  owner                = {zkgst0c},
  posted-at            = {2016-02-04 22:30:24},
  timestamp            = {2020-02-25 23:21},
  year                 = {2018},
}

@Article{Hanauer-Lauterbach-2018,
  author         = {Hanauer, Matthias Xaver and Lauterbach, Jochim},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {The Cross-Section of Emerging Market Stock Returns},
  doi            = {10.2139/ssrn.3233614},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3233614},
  abstract       = {Using monthly stock returns from 28 emerging market countries and a total sample period of 21 years, we investigate the predictive power of a broad set of factors. We document that the factor definitions of the Fama and French (2015) five-factor model are less robust compared to alternative factor definitions. In contrast, the anomalous returns associated with cash flow-to-price, gross profitability, composite equity issuance, and momentum are pervasive as they show up in equal- and value-weighted portfolio sorts as well as in cross-sectional regressions. In contrast to financial theory and in line with previous findings, we do not find a positive cross-sectional relationship between risk and return. Finally, return forecasts derived from the alternative factor definitions are superior in their out-of-sample predictive ability to the ones derived from the five-factor model.},
  f1000-projects = {QuantInvest},
  groups         = {FcstQWIM_Equity, FrcstQWIM_Test},
  timestamp      = {2020-02-25 23:21},
}

@Article{Harvey-et-al-2017c,
  author               = {Harvey, David I. and Leybourne, Stephen J. and Whitehouse, Emily J.},
  date                 = {2017},
  journaltitle         = {International Journal of Forecasting},
  title                = {Forecast evaluation tests and negative long-run variance estimates in small samples},
  doi                  = {10.1016/j.ijforecast.2017.05.001},
  issn                 = {0169-2070},
  number               = {4},
  pages                = {833--847},
  volume               = {33},
  abstract             = {This paper shows that the long-run variance can frequently be negative when computing standard Diebold-Mariano-type tests for equal forecast accuracy and forecast encompassing if one is dealing with multi-step-ahead predictions in small, but empirically relevant, sample sizes. We therefore consider a number of alternative approaches for dealing with this problem, including direct inference in the problem cases and the use of long-run variance estimators that guarantee positivity. The finite sample size and power of the different approaches are evaluated using extensive Monte Carlo simulation exercises. Overall, for multi-step-ahead forecasts, we find that the test recently proposed by Coroneo and Iacone (2016), which is based on a weighted periodogram long-run variance estimator, offers the best finite sample size and power performance.},
  citeulike-article-id = {14478279},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2017.05.001},
  groups               = {FrcstQWIM_Test},
  posted-at            = {2017-11-30 23:37:43},
  timestamp            = {2020-02-25 23:21},
}

@Article{Heaton-2015,
  author         = {Heaton, Chris},
  date           = {2015-07},
  journaltitle   = {International Journal of Forecasting},
  title          = {Testing for multiple-period predictability between serially dependent time series},
  doi            = {10.1016/j.ijforecast.2014.09.004},
  issn           = {0169-2070},
  number         = {3},
  pages          = {587--597},
  volume         = {31},
  abstract       = {This paper reports the results of a simulation study that considers the finite-sample performances of a range of approaches for testing multiple-period predictability between two potentially serially correlated time series. In many empirically relevant situations, but not all, most of the test statistics considered are significantly oversized. In contrast, both an analytical approach proposed in this paper and a bootstrap are found to have accurate empirical sizes. In a small number of cases, the bootstrap is found to have a superior power. The test procedures considered are applied to an empirical analysis of the predictive power of a Phillips curve model during the moderation period, which illustrates the practical importance of using test statistics with accurate empirical sizes.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_Test},
  timestamp      = {2020-02-25 23:21},
}

@Article{Hjalmarsson-2012,
  author         = {Hjalmarsson, Erik},
  date           = {2012-06},
  journaltitle   = {Finance Research Letters},
  title          = {Some curious power properties of long-horizon tests},
  doi            = {10.1016/j.frl.2011.10.001},
  issn           = {1544-6123},
  number         = {2},
  pages          = {81--91},
  volume         = {9},
  abstract       = {Based on simulations and asymptotic results, I highlight three distinct properties of long-horizon predictive tests. (i) The asymptotic power of long-horizon tests increases only with the sample size relative to the forecasting horizon. Keeping this ratio fixed as the sample size increases does not lead to any power gains asymptotically. (ii) Although the power of long-horizon tests increases with the magnitude of the slope coefficient for alternatives close to the null hypothesis, there are no gains in power as the slope coefficient grows large. That is, the power curve is asymptotically horizontal when viewed as a function of the slope coefficient. (iii) For endogenous regressors.e., when the innovations to the regressand are contemporaneously correlated with the innovations to the regressor tests based on the standard long-run OLS estimator result in power curves that are sometimes decreasing in the magnitude of the slope coefficient.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_Test},
  timestamp      = {2020-02-25 23:21},
}

@Article{Deszi-Nistor-2016,
  author               = {Deszi, Eva and Nistor, Ioan A.},
  date                 = {2016},
  journaltitle         = {Romanian Economic and Business Review},
  title                = {Can Deep Machine Learning Outsmart The Market? A Comparison Between Econometric Modelling And Long- Short Term Memory},
  number               = {1},
  url                  = {https://ideas.repec.org/a/rau/journl/v11y2016i4.1p54-73.html},
  volume               = {4},
  abstract             = {Using long-short term memory (LSTM) recurrent neural network (RNN) architecture, we analyse data from the Romanian stock markets in the attempt to forecast its future trend. Then we try to compare the results using the classical statistical modelling tools, further employing back testing to prove our findings. We believe that the LSTM should be the next tool in balancing portfolios and reducing market risk.},
  citeulike-article-id = {14503713},
  groups               = {FrcstQWIM_ML},
  posted-at            = {2017-12-16 12:28:50},
  timestamp            = {2020-02-25 23:21},
}

@Article{Feng-et-al-2018a,
  author         = {Feng, Guanhao and He, Jingyu and Polson, Nicholas G.},
  date           = {2018-04-25},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep Learning for Predicting Asset Returns},
  url            = {https://arxiv.org/abs/1804.09314},
  abstract       = {Deep learning searches for nonlinear factors for predicting asset returns. Predictability is achieved via multiple layers of composite factors as opposed to additive ones. Viewed in this way, asset pricing studies can be revisited using multi-layer deep learners, such as rectified linear units (ReLU) or long-short-term-memory (LSTM) for time-series effects. State-of-the-art algorithms including stochastic gradient descent (SGD), TensorFlow and dropout design provide imple- mentation and efficient factor exploration. To illustrate our methodology, we revisit the equity market risk premium dataset of Welch and Goyal (2008). We find the existence of nonlinear factors which explain predictability of returns, in particular at the extremes of the characteristic space. Finally, we conclude with directions for future research.},
  day            = {25},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Forecast_QWIM, Predictability_FinInfo, FrcstQWIM_ML, FrcstQWIM_ShortTerm, FrcstQWIM_MedLngTerm, FcstQWIM_Equity, ML_ForcstTimeSrs, ML_AssetPricing},
  timestamp      = {2020-02-25 23:21},
}

@Article{Feng-et-al-2019b,
  author         = {Feng, Guanhao and Polson, Nick and Xu, Jianeng},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Deep Learning in Characteristics-Sorted Factor Models},
  issn           = {1556-5068},
  url            = {https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=3243683},
  abstract       = {To study the characteristics-sorted factor model in asset pricing, we develop a bottom-up approach with state-of-the-art deep learning optimization. With an economic objective to minimize pricing errors, we train a non-reduced-form neural network using firm characteristics [inputs], and generate factors [intermediate features], to fit security returns [outputs]. Sorting securities on firm characteristics provides a nonlinear activation to create long-short portfolio weights, as a hidden layer, from lag characteristics to realized returns. Our model offers an alternative perspective for dimension reduction on firm characteristics [inputs], rather than factors [intermediate features], and allows for both nonlinearity and interactions on inputs. Our empirical findings are twofold. We find robust statistical and economic evidence in out-of-sample portfolios and individual stock returns. To interpret our deep factors, we show highly significant results in factor investing via the squared Sharpe ratio test, as well as improvement in dissecting anomalies.},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Forecast_QWIM, FrcstQWIM_ML, ML_Test_OOS, ML_Test_CrossVal, ML_Validation, FrcstQWIM_ShortTerm, FcstQWIM_Equity, ML_ForcstTimeSrs, ML_AssetPricing},
  timestamp      = {2020-02-25 23:21},
}

@Article{Fernandez-et-al-2018,
  author         = {Fernandez, Cesar and Salinas, Luis and Torres, Claudio E.},
  date           = {2018-09-05},
  journaltitle   = {Applied Intelligence},
  title          = {A meta extreme learning machine method for forecasting financial time series},
  doi            = {10.1007/s10489-018-1282-3},
  issn           = {0924-{669X}},
  abstract       = {In the last decade, the problem of forecasting time series in very different fields has received increasing attention due to its many real-world applications. In particular, in the very challenging case of financial time series, the underlying phenomenon of stock time series exhibits complex behaviors, including non-stationary, non-linearity and non-trivial scaling properties. In the literature, a wide-used strategy to improve the forecasting capability is the combination of several models. However, the majority of the published researches in the field of financial time series use different machine learning models where only one type of predictor, either linear or nonlinear, is considered. In this paper we first measure relevant features present in the underlying process to propose a forecast method. We select the Sample Entropy and Hurst Exponent to characterize the behavior of stock time series. The characterization reveals the presence of moderate randomness, long-term memory and scaling properties. Thus, based on the measured properties, this paper proposes a novel one-step-ahead off-line meta-learning model, called mu-XNW, for the prediction of the next value xt+1 of a financial time series xt, that integrates a naive or linear predictor (LP), for which the predicted value of xt + 1 is just repeating the last value xt, an extreme learning machine (ELM) and a discrete wavelet transform (DWT), both based on the n previous values of xt + 1. LP, ELM and DWT are the constituent of the proposed model mu-XNW. We evaluate the proposed model using four well-known performance measures and validated the usefulness of the model using six high-frequency stock time series belong to the technology sector. The experimental results validate that including internal estimators that are able to the capture the relevant features measured (randomness, long-term memory and scaling properties) successfully improve the accuracy of the forecasting over methods that do not include them.},
  day            = {5},
  f1000-projects = {QuantInvest},
  groups         = {ML_Forecast_QWIM, FrcstQWIM_ML, NonStatry_FinTimeSrs, Data_NonLinear, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:21},
}

@Book{Guida-2019,
  author    = {Tony Guida},
  date      = {2019},
  title     = {Big Data and Machine Learning in Quantitative Investment},
  doi       = {10.1002/9781119522225},
  publisher = {John Wiley \& Sons, Ltd},
  abstract  = {Big Data and Machine Learning in Quantitative Investment is not just about demonstrating the maths or the coding. Instead, it's a book by practitioners for practitioners, covering the questions of why and how of applying machine learning and big data to quantitative finance.

The book is split into 13 chapters, each of which is written by a different author on a specific case. The chapters are ordered according to the level of complexity; beginning with the big picture and taxonomy, moving onto practical applications of machine learning and finally finishing with innovative approaches using deep learning.},
  groups    = {DeepLearning_QWIM, ML_Classif_QWIM, ML_Network_QWIM, ML_Forecast_QWIM, ML_Test_QWIM, ML_Factor_Invest, ML_BestPractices, FrcstQWIM_ML},
  timestamp = {2020-02-25 23:21},
}

@InCollection{Gunduz-et-al-2018,
  author         = {Gunduz, Hakan and Yaslan, Yusuf and Cataltepe, Zehra},
  booktitle      = {2018 26th Signal Processing and Communications Applications Conference (SIU)},
  date           = {2018-05-02},
  title          = {Stock market prediction with deep learning using financial news},
  doi            = {10.1109/{SIU}.2018.8404616},
  isbn           = {978-1-5386-1501-0},
  pages          = {1--4},
  publisher      = {IEEE},
  abstract       = {In this study, the hourly movement directions of 9 banking stocks in Borsa Istanbul were predicted using Long-Short Term Memory(LSTM) networks with features obtained from financial news. In the feature creation phase, the word embedding referred as Fasttext, and the financial sentiment dictionary were utilized. Class labels indicating the movement direction were computed based on hourly close prices of the stocks and they were aligned with obtained feature vectors. Two different LSTM networks were trained to perform the prediction, and the performance of the classification process was evaluated by the Macro Averaged (M.A) F-Measure. In the experiments, the movement directions of the 9 stocks were predicted with an average M.A F-measure rate of 0.540. Although the results of both LSTM networks were higher than the Random and Naive benchmark methods, the use of Attention Mechanism in the second LSTM network did not positively affect the results.},
  day            = {2},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Forecast_QWIM, FrcstQWIM_ML, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:21},
}

@Article{Hatcher-Yu-2018,
  author         = {Hatcher, William Grant and Yu, Wei},
  date           = {2018},
  journaltitle   = {IEEE Access},
  title          = {A survey of deep learning: platforms, applications and emerging research trends},
  doi            = {10.1109/{ACCE\SS}.2018.2830661},
  issn           = {2169-3536},
  pages          = {24411--24432},
  volume         = {6},
  abstract       = {Deep learning has exploded in the public consciousness, primarily as predictive and analytical products suffuse our world, in the form of numerous human-centered smart-world systems, including targeted advertisements, natural language assistants and interpreters, and prototype self-driving vehicle systems. Yet to most, the underlying mechanisms that enable such human-centered smart products remain obscure. In contrast, researchers across disciplines have been incorporating deep learning into their research to solve problems that could not have been approached before. In this paper, we seek to provide a thorough investigation of deep learning in its applications and mechanisms. Specifically, as a categorical collection of state of the art in deep learning research, we hope to provide a broad reference for those seeking a primer on deep learning and its various implementations, platforms, algorithms, and uses in a variety of smart-world systems. Furthermore, we hope to outline recent key advancements in the technology, and provide insight into areas, in which deep learning can improve investigation, as well as highlight new areas of research that have yet to see the application of deep learning, but could nonetheless benefit immensely. We hope this survey provides a valuable reference for new deep learning practitioners, as well as those seeking to innovate in the application of deep learning.},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Forecast_QWIM, FrcstQWIM_ML},
  timestamp      = {2020-02-25 23:21},
}

@Article{Hiransha-et-al-2018,
  author         = {Hiransha, M. and Gopalakrishnan, E.A. and Menon, Vijay Krishna and Soman, K.P.},
  date           = {2018},
  journaltitle   = {Procedia Computer Science},
  title          = {NSE Stock Market Prediction Using Deep-Learning Models},
  doi            = {10.1016/j.procs.2018.05.050},
  issn           = {1877-0509},
  pages          = {1351--1362},
  volume         = {132},
  abstract       = {The neural network, one of the intelligent data mining technique that has been used by researchers in various areas for the past 10 years. Prediction and analysis of stock market data have got an important role in today economy. The various algorithms used for forecasting can be categorized into linear (AR, MA, ARIMA, ARMA) and non-linear models (ARCH, GARCH, Neural Network). In this paper, we are using four types of deep learning architectures i.e Multilayer Perceptron (MLP), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) for predicting the stock price of a company based on the historical prices available. Here we are using day-wise closing price of two different stock markets, National Stock Exchange (NSE) of India and New York Stock Exchange (NYSE). The network was trained with the stock price of a single company from NSE and predicted for five different companies from both NSE and NYSE. It has been observed that CNN is outperforming the other models. The network was able to predict for NYSE even though it was trained with NSE data. This was possible because both the stock markets share some common inner dynamics. The results obtained were com- pared with ARIMA model and it has been observed that the neural networks are outperforming the existing linear model (ARIMA).},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_ML},
  timestamp      = {2020-02-25 23:21},
}

@Article{Jammalamadaka-et-al-2018,
  author         = {Jammalamadaka, S. Rao and Qiu, Jinwen and Ning, Ning},
  date           = {2018-01-10},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Multivariate Bayesian Structural Time Series Model},
  url            = {https://arxiv.org/abs/1801.03222},
  abstract       = {This paper deals with inference and prediction for multiple correlated time series, where one has also the choice of using a candidate pool of contemporaneous predictors for each target series. Starting with a structural model for the time-series, Bayesian tools are used for model fitting, prediction, and feature selection, thus extending some recent work along these lines for the univariate case. The Bayesian paradigm in this multivariate setting helps the model avoid overfitting as well as capture correlations among the multiple time series with the various state components. The model provides needed flexibility to choose a different set of components and available predictors for each target series. The cyclical component in the model can handle large variations in the short term, which may be caused by external shocks. We run extensive simulations to investigate properties such as estimation accuracy and performance in forecasting. We then run an empirical study with one-step-ahead prediction on the max log return of a portfolio of stocks that involve four leading financial institutions. Both the simulation studies and the extensive empirical study confirm that this multivariate model outperforms three other benchmark models, viz. a model that treats each target series as independent, the autoregressive integrated moving average model with regression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model.},
  day            = {10},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs},
  timestamp      = {2020-02-25 23:22},
}

@Article{Jouini-2015,
  author               = {Jouini, Tarek},
  date                 = {2015-11},
  journaltitle         = {Journal of Forecasting},
  title                = {Efficient Multistep Forecast Procedures for Multivariate Time Series},
  doi                  = {10.1002/for.2363},
  number               = {7},
  pages                = {604--618},
  volume               = {34},
  abstract             = {Upon the evidence that infinite-order vector autoregression setting is more realistic in time series models, we propose new model selection procedures for producing efficient multistep forecasts. They consist of order selection criteria involving the sample analog of the asymptotic approximation of the h-step-ahead forecast mean squared error matrix, where h is the forecast horizon. These criteria are minimized over a truncation order nT under the assumption that an infinite-order vector autoregression can be approximated, under suitable conditions, with a sequence of truncated models, where nT is increasing with sample size.

Using finite-order vector autoregressive models with various persistent levels and realistic sample sizes, Monte Carlo simulations show that, overall, our criteria outperform conventional competitors. Specifically, they tend to yield better small-sample distribution of the lag-order estimates around the true value, while estimating it with relatively satisfactory probabilities. They also produce more efficient multistep (and even stepwise) forecasts since they yield the lowest h-step-ahead forecast mean squared errors for the individual components of the holding pseudo-data to forecast. Thus estimating the actual autoregressive order as well as the best forecasting model can be achieved with the same selection procedure.

Such results stand in sharp contrast to the belief that parsimony is a virtue in itself, and state that the relative accuracy of strongly consistent criteria such as the Schwarz information criterion, as claimed in the literature, is overstated. Our criteria are new tools extending those previously existing in the literature and hence can suitably be used for various practical situations when necessary.},
  citeulike-article-id = {13934973},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2363},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-02-18 06:10:48},
  timestamp            = {2020-02-25 23:22},
}

@Article{Kolsrud-2015,
  author               = {Kolsrud, Dag},
  date                 = {2015-12},
  journaltitle         = {Journal of Forecasting},
  title                = {A Time-Simultaneous Prediction Box for a Multivariate Time Series},
  doi                  = {10.1002/for.2366},
  number               = {8},
  pages                = {675--693},
  volume               = {34},
  abstract             = {A sample-based method in Kolsrud (Journal of Forecasting 2007; 26(3): 171-188) for the construction of a time-simultaneous prediction band for a univariate time series is extended to produce a variable- and time-simultaneous prediction box for a multivariate time series. A measure of distance based on the Linfinity -norm is applied to a learning sample of multivariate time trajectories, which can be mean- and/or variance-nonstationary. Based on the ranking of distances to the centre of the sample, a subsample of the most central multivariate trajectories is selected. A prediction box is constructed by circumscribing the subsample with a hyperrectangle. The fraction of central trajectories selected into the subsample can be calibrated by bootstrap such that the expected coverage of the box equals a prescribed nominal level. The method is related to the concept of data depth, and thence modified to increase coverage. Applications to simulated and empirical data illustrate the method, which is also compared to several other methods in the literature adapted to the multivariate setting..},
  citeulike-article-id = {14249788},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2366},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs, NonStatry_FinTimeSrs, ML_ForcstTimeSrs},
  posted-at            = {2017-01-10 21:29:09},
  timestamp            = {2020-02-25 23:22},
}

@Book{Konar-Bhattacharya-2017,
  author               = {Konar, Amit and Bhattacharya, Diptendu},
  date                 = {2017},
  title                = {Time-Series Prediction and Applications},
  doi                  = {10.1007/978-3-319-54597-4},
  isbn                 = {978-3-319-54596-7},
  publisher            = {Springer International Publishing},
  volume               = {127},
  abstract             = {This book presents machine learning and type-2 fuzzy sets for the prediction of time-series with a particular focus on business forecasting applications. It also proposes new uncertainty management techniques in an economic time-series using type-2 fuzzy sets for prediction of the time-series at a given time point from its preceding value in fluctuating business environments. It employs machine learning to determine repetitively occurring similar structural patterns in the time-series and uses stochastic automaton to predict the most probabilistic structure at a given partition of the time-series. Such predictions help in determining probabilistic moves in a stock index time-series Primarily written for graduate students and researchers in computer science, the book is equally useful for researchers/professionals in business intelligence and stock index prediction. A background of undergraduate level mathematics is presumed, although not mandatory, for most of the sections. Exercises with tips are provided at the end of each chapter to the readers' ability and understanding of the topics covered.},
  citeulike-article-id = {14503285},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-54597-4},
  groups               = {ML_BestPractices, FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  posted-at            = {2017-12-15 06:46:17},
  timestamp            = {2020-02-25 23:22},
}

@Article{Konzen-Ziegelmann-2016,
  author               = {Konzen, Evandro and Ziegelmann, Flavio A.},
  date                 = {2016-01},
  journaltitle         = {Journal of Forecasting},
  title                = {LASSO-Type Penalties for Covariate Selection and Forecasting in Time Series},
  doi                  = {10.1002/for.2403},
  pages                = {n/a},
  abstract             = {This paper studies some forms of LASSO-type penalties in time series to reduce the dimensionality of the parameter space as well as to improve out-of-sample forecasting performance. In particular, we propose a method that we call WLadaLASSO (weighted lag adaptive LASSO), which assigns not only different weights to each coefficient but also further penalizes coefficients of higher-lagged covariates. In our Monte Carlo implementation, the WLadaLASSO is superior in terms of covariate selection, parameter estimation precision and forecasting, when compared to both LASSO and adaLASSO, especially for a higher number of candidate lags and a stronger linear dependence between predictors. Empirical studies illustrate our approach for US risk premium and US inflation forecasting with good results.},
  citeulike-article-id = {14070593},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2403},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {zkgst0c},
  posted-at            = {2016-06-16 23:19:22},
  timestamp            = {2020-02-25 23:22},
}

@PhdThesis{Lai-2017a,
  author               = {Lai, Kwok C.},
  date                 = {2017},
  institution          = {The Hong Kong Polytechnic University},
  title                = {Financial time series forecasting using conditional Restricted Boltzmann Machine},
  url                  = {http://ira.lib.polyu.edu.hk/handle/10397/65255},
  abstract             = {Inspired by the success of deep learning in big data image recognition in Restricted Boltzmann Machine, Conditional Restricted Boltzmann Machine which was the original design to forecast human motion movement had been modified to forecast financial time series. As far as the author is aware of, this is the first attempt to apply deep learning in financial time series forecasting. Conventionally, deep learning is applied in image classification and several layers of deep learning in the huge dataset could increase its accuracy. The traditional forecasting method is using Euclidean distance to map the dataset into a higher dimension which facilitates to draw a hyperplane to separate the data. The more the cluster of the data in the hyperplane, the closer the distance of those neighbour data. As a result, those cluster data are the foundation to forecast. A new approach in Restricted Boltzmann Machine is to assign low energy based on probability concept to those connections that are relevant to each other while high energy is assigned to those that are irrelevant. The advantage of this method over Euclidean distance is that the probability energy assignment can be done one layer at a time and extend to many layers. Each layer information is retained and passed on to another layer to be trained again. As a consequence, all the information in the dataset is carefully scrutinized to obtain the best result. In this research, it has been demonstrated in the following Chapters that deep learning using modified Conditional Restricted Boltzmann Machine is able to handle high dimensionality data which is over 100 with the dataset array as big as 600000x100. This setup enables it to capture the information of the high dimensions in each layer. Eventually, it will improve the forecasting accuracy. This was not possible before as our previous research has experienced. Historical records are not as important as the dimension of the financial time series problem domain. 30 or 20 years of stock history may not have that much impact on the current stock price in one stock. As the financial market is closely related to other markets, the stock price of a particular security is heavily dependent on other stocks in the same market as well as the performance of other markets. Hence, it is more important to increase the dimensionality or features of the data. In other words, including more factors such as the price of others stocks, economic factors such as interest rates and GDP can enhance the performance. The algorithm based on Conditional Restricted Boltzmann Machine has demonstrated remarkable forecasting accuracy as reported in Chapter 4 and 5.},
  citeulike-article-id = {14503730},
  groups               = {FrcstQWIM_TimeSrs, ML_PerfMetrics, ML_ForcstTimeSrs, ML_ClustTimeSrs},
  posted-at            = {2017-12-16 13:57:02},
  timestamp            = {2020-02-25 23:22},
}

@Article{Lai-et-al-2017,
  author               = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  date                 = {2017-07-05},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks},
  eprint               = {1703.07015},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1703.07015},
  abstract             = {Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) to extract short-term local dependency patterns among variables, and the Recurrent Neural Network (RNN) to discover long-term patterns and trends. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. The dataset and experiment code both are uploaded to Github.},
  citeulike-article-id = {14503311},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.07015},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.07015},
  day                  = {5},
  groups               = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  posted-at            = {2017-12-15 07:46:04},
  timestamp            = {2020-02-25 23:22},
}

@Article{Li-Chen-2014,
  author               = {Li, Jiahan and Chen, Weiye},
  date                 = {2014-10},
  journaltitle         = {International Journal of Forecasting},
  title                = {Forecasting macroeconomic time series: LASSO-based approaches and their forecast combinations with dynamic factor models},
  doi                  = {10.1016/j.ijforecast.2014.03.016},
  issn                 = {0169-2070},
  number               = {4},
  pages                = {996--1015},
  volume               = {30},
  abstract             = {In a data-rich environment, forecasting economic variables amounts to extracting and organizing useful information from a large number of predictors. So far, the dynamic factor model and its variants have been the most successful models for such exercises.

In this paper, we investigate a category of LASSO-based approaches and evaluate their predictive abilities for forecasting twenty important macroeconomic variables. These alternative models can handle hundreds of data series simultaneously, and extract useful information for forecasting. We also show, both analytically and empirically, that combing forecasts from LASSO-based models with those from dynamic factor models can reduce the mean square forecast error (MSFE) further.

Our three main findings can be summarized as follows. First, for most of the variables under investigation, all of the LASSO-based models outperform dynamic factor models in the out-of-sample forecast evaluations. Second, by extracting information and formulating predictors at economically meaningful block levels, the new methods greatly enhance the interpretability of the models. Third, once forecasts from a LASSO-based approach are combined with those from a dynamic factor model by forecast combination techniques, the combined forecasts are significantly better than either dynamic factor model forecasts or the na ive random walk benchmark.},
  citeulike-article-id = {13932972},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2014.03.016},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-03-12 17:33:13},
  timestamp            = {2020-02-25 23:22},
}

@InCollection{Mahalakshmi-et-al-2016,
  author               = {Mahalakshmi, G. and Sridevi, S. and Rajaram, S.},
  booktitle            = {2016 International Conference on Computing Technologies and Intelligent Data Engineering (ICCTIDE'16)},
  date                 = {2016-01},
  title                = {A survey on forecasting of time series data},
  doi                  = {10.1109/icctide.2016.7725358},
  isbn                 = {978-1-4673-8437-7},
  location             = {Kovilpatti, India},
  pages                = {1--8},
  publisher            = {IEEE},
  abstract             = {Time series analysis and forecasting future values has been a major research focus since years ago. Time series analysis and forecasting in time series data finds it significance in many applications such as business, stock market and exchange, weather, electricity demand, cost and usage of products such as fuels, electricity, etc. and in any kind of place that has specific seasonal or trendy changes with time. The forecasting of time series data provides the organization with useful information that is necessary for making important decisions. In this paper, a detailed survey of the various techniques applied for forecasting different types of time series dataset is provided. This survey covers the overall forecasting models, the algorithms used within the model and other optimization techniques used for better performance and accuracy. The various performance evaluation parameters used for evaluating the forecasting models are also discussed in this paper. This study gives the reader an idea about the various researches that take place within forecasting using the time series data.},
  citeulike-article-id = {14525978},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/icctide.2016.7725358},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2018-01-29 23:37:48},
  timestamp            = {2020-02-25 23:22},
}

@Article{Makridakis-et-al-2018,
  author         = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  date           = {2018-03-27},
  journaltitle   = {PLoS ONE},
  title          = {Statistical and Machine Learning forecasting methods: Concerns and ways forward},
  doi            = {10.1371/journal.pone.0194889},
  number         = {3},
  pages          = {e0194889},
  volume         = {13},
  abstract       = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
  day            = {27},
  f1000-projects = {QuantInvest},
  groups         = {ML_Forecast_QWIM, ML_Interpretability, FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_PerfMetrics, ML_Test_OOS, ML_ForcstTimeSrs},
  pmcid          = {PMC5870978},
  pmid           = {29584784},
  timestamp      = {2020-02-25 23:22},
}

@Article{Marczak-Proietti-2016,
  author               = {Marczak, Martyna and Proietti, Tommaso},
  date                 = {2016-01},
  journaltitle         = {International Journal of Forecasting},
  title                = {Outlier detection in structural time series models: The indicator saturation approach},
  doi                  = {10.1016/j.ijforecast.2015.04.005},
  issn                 = {0169-2070},
  number               = {1},
  pages                = {180--202},
  volume               = {32},
  abstract             = {Structural change affects the estimation of economic signals, such as the growth rate or the seasonally adjusted series. One important issue that has attracted a great deal of attention in the seasonal adjustment literature is its detection by an expert procedure. The general-to-specific approach to the detection of structural change, which is currently implemented in Autometrics via indicator saturation, has proven to be both practical and effective in the context of stationary dynamic regression models and unit-root autoregressions. By focusing on impulse- and step-indicator saturation, we use Monte Carlo simulations to investigate the performance of this approach for detecting additive outliers and level shifts in the analysis of nonstationary seasonal time series. The reference model is the basic structural model, featuring a local linear trend, possibly integrated of order two, stochastic seasonality and a stationary component. Further, we apply both kinds of indicator saturation to the detection of additive outliers and level shifts in the industrial production series of five European countries.},
  citeulike-article-id = {14030179},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2015.04.005},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-05-09 00:04:47},
  timestamp            = {2020-02-25 23:22},
}

@Article{Montero-Vilar-2015,
  author       = {Pablo Montero and Jose A. Vilar},
  date         = {2015},
  journaltitle = {Journal of Statistical Software},
  title        = {TSclust: An R Package for Time Series Clustering},
  url          = {https://www.jstatsoft.org/article/view/v062i01},
  volume       = {62},
  abstract     = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity measure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to implement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
  groups       = {FrcstQWIM_TimeSrs},
  timestamp    = {2020-02-25 23:22},
}

@Book{Montgomery-et-al-2015,
  author    = {Douglas C. Montgomery and Cheryl L. Jennings and Murat Kulahci},
  date      = {2015},
  title     = {Introduction to Time Series Analysis and Forecasting},
  edition   = {Second Edition},
  publisher = {Wiley-Interscience},
  url       = {https://www.wiley.com/en-us/Introduction+to+Time+Series+Analysis+and+Forecasting%2C+2nd+Edition-p-9781118745113},
  abstract  = {Authored by highly-experienced academics and professionals in engineering statistics, the Second Edition features discussions on both popular and modern time series methodologies as well as an introduction to Bayesian methods in forecasting. Introduction to Time Series Analysis and Forecasting, Second Edition also includes: Over 300 exercises from diverse disciplines including health care, environmental studies, engineering, and finance More than 50 programming algorithms using JMP, SAS, and R that illustrate the theory and practicality of forecasting techniques in the context of time-oriented data New material on frequency domain and spatial temporal data analysis Expanded coverage of the variogram and spectrum with applications as well as transfer and intervention model functions A supplementary website featuring PowerPoint slides, data sets, and select solutions to the problems Introduction to Time Series Analysis and Forecasting, Second Edition is an ideal textbook upper-undergraduate and graduate-levels courses in forecasting and time series. The book is also an excellent reference for practitioners and researchers who need to model and analyze time series data to generate forecasts.},
  groups    = {FrcstQWIM_TimeSrs},
  timestamp = {2020-02-25 23:22},
}

@Article{Nystrup-et-al-2017a,
  author               = {Nystrup, Peter and Madsen, Henrik and Lindstrom, Erik},
  date                 = {2017-12},
  journaltitle         = {Journal of Forecasting},
  title                = {Long Memory of Financial Time Series and Hidden Markov Models with Time-Varying Parameters},
  doi                  = {10.1002/for.2447},
  number               = {8},
  pages                = {898-1002},
  url                  = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2447},
  volume               = {36},
  abstract             = {Hidden Markov models are often used to model daily returns and to infer the hidden state of financial markets. Previous studies have found that the estimated models change over time, but the implications of the time-varying behavior have not been thoroughly examined. This paper presents an adaptive estimation approach that allows for the parameters of the estimated models to be time varying. It is shown that a two-state Gaussian hidden Markov model with time-varying parameters is able to reproduce the long memory of squared daily returns that was previously believed to be the most difficult fact to reproduce with a hidden Markov model. Capturing the time-varying behavior of the parameters also leads to improved one-step density forecasts. Finally, it is shown that the forecasting performance of the estimated models can be further improved using local smoothing to forecast the parameter variations.},
  citeulike-article-id = {14150032},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2447},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-10-01 18:51:06},
  publisher            = {John Wiley and Sons, Ltd},
  timestamp            = {2020-02-25 23:22},
}

@InCollection{Oliveira-et-al-2013,
  author               = {Oliveira, T. F. and Firmino, P. R. A. and Ferreira, T. A. E.},
  booktitle            = {Computational Intelligence and 11th Brazilian Congress on Computational Intelligence (BRICS-CCI and CBIC), 2013 BRICS Congress on},
  date                 = {2013},
  title                = {Combining Time Series Forecasting Models via Gumbel-Hougaard Copulas},
  doi                  = {10.1109/brics-cci-cbic.2013.100},
  pages                = {568--573},
  publisher            = {IEEE},
  abstract             = {Researchers have been challenged to combine time series forecasting models, with the intention of enhancing forecast accuracy and efficiency. In this way, to weight models accuracy, efficiency, and mutual dependency becomes paramount. A promising way to address this issue is via copulas. Copulas are joint probability distribution functions aimed to envelop both the marginal distribution as well as the dependency among variables (e:g: forecasting models). This paper introduces copulas in the problem of combining time series forecasting models and proposes a maximum likelihood-based methodology in this context. Specifically, a Gumbel-Hougaard copulas model is presented. The usefulness of the resulting methodology is illustrated by means of simulated cases involving the combination of two single ARIMA models.},
  citeulike-article-id = {13995917},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/brics-cci-cbic.2013.100},
  citeulike-linkout-1  = {http://ieeexplore.ieee.org/xpls/absall.jsp?arnumber=6855909},
  groups               = {FrcstQWIM_TimeSrs},
  institution          = {Dept. of Stat. and Inf., Fed. Rural Univ. of Pernambuco, Recife, Brazil},
  owner                = {cristi},
  posted-at            = {2016-04-04 02:38:59},
  timestamp            = {2020-02-25 23:22},
}

@Article{Jacobsen-et-al-2019a,
  author         = {Jacobsen, Ben and Jiang, Fuwei and Zhang, Hongwei},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Ensemble machine learning and stock return predictability},
  doi            = {10.2139/ssrn.3310289},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3310289},
  abstract       = {Many, even sophisticated, models cannot beat a simple mean combination of univariate stock market return forecasts. We introduce an ensemble machine learning method, which averages forecasts from sophisticated models (like BMA, WALS and LA) based on random subsamples and which learns from its mistakes by adaptively changing sampling distributions. Empirically, our novel method improves the simple mean forecast with statistically significant monthly out-of-sample R2 of around 2-3\% and annual utility gains around 3\%. Our approach benefits from predicting well in volatile periods and especially from extreme market drops. The forecasting gains of our new method stem from improved diversity among individual forecasts. We obtain similar gains in forecasting accuracy when we use our method to predict factor portfolios and other macro economic variables.},
  f1000-projects = {QuantInvest},
  groups         = {ML_Forecast_QWIM, Predictability_FinInfo, FrcstQWIM_ML, ML_Test_OOS, FcstQWIM_Equity, FrcstQWIM_Test},
  timestamp      = {2020-02-25 23:22},
}

@Article{Januario-2016,
  author               = {Januario, Afonso V.},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {How Predictable are Returns Out-of-Sample? Evidence from Value and Momentum},
  doi                  = {10.2139/ssrn.2475813},
  issn                 = {1556-5068},
  abstract             = {This paper examines the performance of variables that have been suggested in the literature as being good predictors of the returns of value and momentum equity investment strategies out-of-sample. Using simple linear regression models with shrunk estimators at monthly and annual frequency, I find that, value is predicted by book-to-market, earnings-price ratio, smooth earning-price ratio (Asness et al. (2000) and Cohen et al. (2003)), book-to-market or dividend yield together with earnings price ratio growth, and by different forecast combinations. The results are not robust to leverage constraints (with the exception of lagged returns at monthly frequency) and do not hold for constant volatility portfolios. Momentum is predicted at monthly and annual frequency by book-to-market, at monthly frequency by lagged-one-month return, and at annual frequency by book-to-market together with earnings-price ratio growth. Results are robust to targeted volatility portfolios (Daniel et al. (2013)), but are not robust to leverage constraints, finer portfolios, or industry controls. All other predictors perform poorly, are unstable between sub-samples, and are not robust to various robustness tests. The results suggest links of value to the real economy and that the variety of predictors proposed by the literature are of little value for predicting momentum returns out-of-sample.},
  citeulike-article-id = {14325790},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2475813},
  groups               = {Regression_Linear, FcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2017-03-31 23:05:46},
  timestamp            = {2020-02-25 23:22},
}

@Article{Kong-et-al-2011,
  author               = {Kong, Aiguo and Rapach, David E. and Strauss, Jack K. and Zhou, Guofu},
  date                 = {2011-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Predicting Market Components Out of Sample: Asset Allocation Implications},
  doi                  = {10.3905/jpm.2011.37.4.029},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {29--41},
  volume               = {37},
  abstract             = {The authors analyze out-of-sample return predictability for components of the aggregate market, focusing on the well-known Fama French size/value-sorted portfolios. Employing a forecast combination approach based on a variety of economic variables and lagged component returns as predictors, they find significant evidence of out-of-sample return predictability for nearly all component portfolios. Moreover, return predictability is typically much stronger for small-cap/high book-to-market value stocks. The pattern of component return predictability is enhanced during business cycle recessions, linking component return predictability to the real economy. Considering various component-rotation investment strategies, the authors show that out-of-sample component return predictability can be exploited to substantially improve portfolio performance.},
  citeulike-article-id = {13989309},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2011.37.4.029},
  groups               = {FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-03-28 03:52:06},
  timestamp            = {2020-02-25 23:22},
}

@Article{Koundouri-et-al-2016,
  author               = {Koundouri, Phoebe and Kourogenis, Nikolaos and Pittis, Nikitas and Samartzis, Panagiotis},
  date                 = {2016-08},
  journaltitle         = {Journal of Forecasting},
  title                = {Factor Models of Stock Returns: GARCH Errors versus Time-Varying Betas},
  doi                  = {10.1002/for.2387},
  number               = {5},
  pages                = {445--461},
  volume               = {35},
  abstract             = {This paper investigates the implications of time-varying betas in factor models for stock returns. It is shown that a single-factor model (SFMT) with autoregressive betas and homoscedastic errors (SFMT-AR) is capable of reproducing the most important stylized facts of stock returns. An empirical study on the major US stock market sectors shows that SFMT-AR outperforms, in terms of in-sample and out-of-sample performance, SFMT with constant betas and conditionally heteroscedastic (GARCH) errors, as well as two multivariate GARCH-type models.},
  citeulike-article-id = {14150036},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2387},
  day                  = {1},
  groups               = {FcstQWIM_Equity, FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-10-01 19:00:28},
  timestamp            = {2020-02-25 23:22},
}

@Article{Lawrenz-Zorn-2017a,
  author               = {Lawrenz, Jochen and Zorn, Josef},
  date                 = {2017-09},
  journaltitle         = {Journal of Empirical Finance},
  title                = {Predicting international stock returns with conditional price-to-fundamental ratios},
  doi                  = {10.1016/j.jempfin.2017.06.003},
  issn                 = {0927-5398},
  pages                = {159--184},
  volume               = {43},
  abstract             = {Taking the perspective of international asset allocation, this paper tests if predictive regressions conditional on time-series and cross-sectional information can improve forecasts of stock index returns. We use different current price-to-fundamental ratios as predictors and condition the sample on the indicator if time-series and cross-section deliver consistent versus opposing signals. Using panel regressions, we find that only consistent ratios (i) display significant mean-reverting behavior, (ii) provide strong in-sample as well as out-of-sample evidence for return predictability, and (iii) yield economic gains in a Bayesian asset allocation framework.},
  citeulike-article-id = {14497262},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jempfin.2017.06.003},
  groups               = {Predictability_FinInfo, FcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2017-12-06 03:12:51},
  timestamp            = {2020-02-25 23:22},
}

@Article{Lee-2014a,
  author               = {Lee, Jin},
  date                 = {2014-04},
  journaltitle         = {Journal of Nonparametric Statistics},
  title                = {Nonparametric testing for long-horizon predictability with persistent covariates},
  doi                  = {10.1080/10485252.2013.870173},
  number               = {2},
  pages                = {359--372},
  volume               = {26},
  abstract             = {We propose a testing procedure for long-horizon predictability via kernel-based nonparametric estimators of long-run covariances between multiperiod returns and persistent covariates. Asymptotic properties of the proposed tests are studied. As for implementation of the test, sieve bootstrap methods are employed to obtain reasonable approximation to the sample distribution of the test statistics. Monte Carlo simulations are conducted to verify the theoretical conjecture. Empirical analysis, using US monthly data from 1929 to 2011, are presented for testing stock return predictability of some forecasting financial variables. Long-term interest rates, unlike default spreads or price-earning ration, are found to show some forecasting power.},
  citeulike-article-id = {14325792},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/10485252.2013.870173},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/10485252.2013.870173},
  day                  = {3},
  groups               = {FrcstQWIM_Test, Returns_Not_Normal},
  posted-at            = {2017-03-31 23:07:53},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-25 23:22},
}

@Article{Lima-Meng-2016,
  author               = {Lima, Luiz R. and Meng, Fanning},
  date                 = {2016-01},
  journaltitle         = {Journal of Applied Economics},
  title                = {Out-of-Sample Return Predictability: A Quantile Combination Approach},
  doi                  = {10.1002/jae.2549},
  pages                = {n/a},
  abstract             = {This paper develops a novel forecasting method that minimizes the effects of weak predictors and estimation errors on the accuracy of equity premium forecasts. The proposed method is based on an averaging scheme applied to quantiles conditional on predictors selected by LASSO. The resulting forecasts outperform the historical average, and other existing models, by statistically and economically meaningful margins.},
  citeulike-article-id = {14218351},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/jae.2549},
  day                  = {1},
  groups               = {Predictability_FinInfo, FcstQWIM_Equity, FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-12-01 18:56:10},
  publisher            = {John Wiley and Sons, Ltd},
  timestamp            = {2020-02-25 23:22},
}

@Article{Lin-et-al-2011a,
  author               = {Lin, Eric S. and Chou, Ping-Hung and Chou, Ta-Sheng},
  date                 = {2011-08},
  journaltitle         = {Journal of Forecasting},
  title                = {Testing for the usefulness of forecasts},
  doi                  = {10.1002/for.1180},
  number               = {5},
  pages                = {469--489},
  volume               = {30},
  abstract             = {Ashley (Journal of Forecasting 1983; 2(3): 211 223) proposes a criterion (known as Ashley's index) to judge whether the external macroeconomic variables are well forecast to serve as explanatory variables in forecasting models, which is crucial for policy makers. In this article, we try to extend Ashley's work by providing three testing procedures, including a ratio-based test, a difference-based test, and the Bayesian approach. The Bayesian approach has the advantage of allowing the flexibility of adapting all possible information content within a decision-making environment such as the change of variable's definition due to the evolving system of national accounts. We demonstrate the proposed methods by applying six macroeconomic forecasts in the Survey of Professional Forecasters. Researchers or practitioners can thus formally test whether the external information is helpful.},
  citeulike-article-id = {13995908},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.1180},
  day                  = {1},
  groups               = {FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-04-04 02:25:54},
  publisher            = {John Wiley and Sons, Ltd.},
  timestamp            = {2020-02-25 23:22},
}

@Article{Maio-2016,
  author               = {Maio, Paulo},
  date                 = {2016-06},
  journaltitle         = {Journal of Financial Markets},
  title                = {Cross-sectional return dispersion and the equity premium},
  doi                  = {10.1016/j.finmar.2015.09.001},
  issn                 = {1386-4181},
  pages                = {87--109},
  volume               = {29},
  abstract             = {In this paper, I examine whether stock return dispersion (RD) provides useful information about future stock returns. RD consistently forecasts a decline in the excess market return at multiple horizons, and compares favorably with alternative predictors used in the literature. The out-of-sample performance of RD tends to beat the alternative predictors, and is economically significant as indicated by the certainty equivalent gain associated with a trading investment strategy. RD has greater forecasting power for big and growth stocks compared to small and value stocks, respectively. I discuss a theoretical mechanism giving rise to the negative correlation between RD and the equity premium.},
  citeulike-article-id = {14166660},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.finmar.2015.09.001},
  groups               = {FrcstQWIM_Equity, FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-10-17 04:55:39},
  timestamp            = {2020-02-25 23:22},
}

@Article{Mariano-Preve-2012,
  author         = {Mariano, Roberto S. and Preve, Daniel},
  date           = {2012-07},
  journaltitle   = {Journal of Economics},
  title          = {Statistical tests for multiple forecast comparison},
  doi            = {10.1016/j.jeconom.2012.01.014},
  issn           = {0304-4076},
  number         = {1},
  pages          = {123--130},
  volume         = {169},
  abstract       = {We consider a multivariate version of the Diebold-Mariano test for equal predictive ability of three or more forecasting models. The Wald-type test, which has a null distribution that is asymptotically chi-squared, is shown to be generally invariant with respect to the ordering of the models being compared. Finite-sample corrections for the test are also developed. Monte Carlo simulations indicate that has reasonable size properties in large samples but tends to be oversized in moderate samples. The finite-sample correction succeeds in correcting for size, but only partially. For the size-adjusted tests, power increases with sample size, as expected. It is speculated that further finite-sample improvements can be achieved using Hotelling T2T2 or bootstrap critical values.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_Test, Stat_Test},
  timestamp      = {2020-02-25 23:22},
}

@Article{Martins-Perron-2016,
  author               = {Martins, Luis F. and Perron, Pierre},
  date                 = {2016-09},
  journaltitle         = {Journal of Time Series Analysis},
  title                = {Improved Tests for Forecast Comparisons in the Presence of Instabilities},
  doi                  = {10.1111/jtsa.12179},
  issn                 = {0143-9782},
  number               = {5},
  pages                = {650--659},
  volume               = {37},
  abstract             = {Of interest is comparing the out-of-sample forecasting performance of two competing models in the presence of possible instabilities. To that effect, we suggest using simple structural change tests, sup-Wald and UDmax for changes in the mean of the loss differences. It is shown that Giacomini and Rossi (2010) tests have undesirable power properties, power that can be low and non-increasing as the alternative becomes further from the null hypothesis. On the contrary, our statistics are shown to have higher monotonic power, especially the UDmax version. We use their empirical examples to show the practical relevance of the issues raised.},
  citeulike-article-id = {14332235},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/jtsa.12179},
  groups               = {FrcstQWIM_Test},
  posted-at            = {2017-04-05 11:24:54},
  timestamp            = {2020-02-25 23:22},
}

@Article{Mayer-et-al-2017,
  author               = {Mayer, Walter J. and Liu, Feng and Dang, Xin},
  date                 = {2017-07},
  journaltitle         = {International Journal of Forecasting},
  title                = {Improving the power of the Diebold-Mariano-West test for least squares predictions},
  doi                  = {10.1016/j.ijforecast.2017.01.008},
  issn                 = {0169-2070},
  number               = {3},
  pages                = {618--626},
  volume               = {33},
  abstract             = {We propose a more powerful version of the test of Diebold and Mariano (1995) and West (1996) for comparing least squares predictors based on non-nested models when the parameter being tested is the expected difference between the squared prediction errors. The proposed test improves the asymptotic power by using a more efficient estimator of the parameter being tested than that used in the literature. The estimator used by the standard version of the test depends on the individual predictions and realizations only through the observations on the prediction errors. However, the parameter being tested can also be expressed in terms of moments of the predictors and the predicted variable, some of which cannot be identified separately by the observations on the prediction errors alone. Parameterizing these moments in a GMM framework and drawing on the theory of West (1996), we devise more powerful versions of the test by exploiting a restriction that is maintained routinely under the null hypothesis by West (1996, Assumption 2b) and later studies. This restriction requires only finite second-order moments and covariance stationarity in order to ensure that the population linear projection exists. Simulation experiments show that the potential gains in power can be substantial.},
  citeulike-article-id = {14383773},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2017.01.008},
  groups               = {FrcstQWIM_Test},
  posted-at            = {2017-06-28 03:16:17},
  timestamp            = {2020-02-25 23:22},
}

@Article{McMillan-2017,
  author               = {McMillan, David G.},
  date                 = {2016-11},
  journaltitle         = {International Review of Applied Economics},
  title                = {Stock return predictability: the role of inflation and threshold dynamics},
  doi                  = {10.1080/02692171.2016.1257581},
  pages                = {357--375},
  volume               = {31},
  abstract             = {AbstractThis paper argues that the nature of stock return predictability varies with the level of inflation. We contend that the nature of relations between economic variables and returns differs according to the level of inflation, due to different economic risk implications. An increase in low level inflation may signal improving economic conditions and lower expected returns, while the opposite is true with an equal rise in high level inflation. Linear estimation provides contradictory coefficient values, which we argue arises from mixing coefficient values across regimes. We test for and estimate threshold models with inflation and the term structure as the threshold variable. These models reveal a change in either the sign or magnitude of the parameter values across the regimes such that the relation between stock returns and economic variables is not constant. Measures of in-sample fit and a forecast exercise support the threshold models. They produce a higher adjusted R2, lower MAE and RMSE and higher trading related measures. These results help explain the lack of consistent empirical evidence in favour of stock return predictability and should be of interest to those engaged in stock market modelling as well as trading and portfolio management.},
  citeulike-article-id = {14218346},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/02692171.2016.1257581},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/02692171.2016.1257581},
  day                  = {22},
  groups               = {Predictability_FinInfo, FrcstQWIM_Equity, FrcstQWIM_Test, Risk_Inflation, FrcstQWIM_Inflation},
  journal              = {International Review of Applied Economics},
  owner                = {cristi},
  posted-at            = {2016-12-01 18:52:27},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 23:22},
  year                 = {2017},
}

@InCollection{McMillan-2018b,
  author               = {McMillan, DavidG},
  booktitle            = {Predicting Stock Returns},
  date                 = {2018},
  title                = {Which Variables Predict and Forecast Stock Market Returns?},
  doi                  = {10.1007/978-3-319-69008-7\_5},
  pages                = {77--101},
  publisher            = {Springer International Publishing},
  abstract             = {Movements in stock returns arise from changes in expected future discount rates and cash-flow growth. However, which variables best proxy for these changes remains unknown. This chapter considers twenty-five variables arranged into five groups and examines both in-sample predictability and out-of-sample forecasting. Our variables span categories including financial ratios, macro-, labour market and housing variables as well as others, which incorporate measures of sentiment and leverage. Significant in-sample results occur across these five groups. Of note, price ratios, GDP acceleration, inflation, unemployment and consumer sentiment feature prominently. In conducting out-of-sample forecasts, we utilise a range of forecast performance measures and consider single model and combined forecasts. The results show that, with one exception, the combined model forecasts outperform the single model forecasts across all measures. This supports the view that a range of variables from across the economy can help predict future stock returns.},
  citeulike-article-id = {14504275},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-69008-75},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-69008-75},
  groups               = {Predictability_FinInfo, FrcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2017-12-18 00:21:18},
  timestamp            = {2020-02-25 23:22},
}

@Article{McMillan-2018c,
  author         = {McMillan, David G.},
  date           = {2018-05-15},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Stock Return Predictability: Using the Cyclical Component of the Price Ratio},
  url            = {https://ssrn.com/abstract=3178714},
  abstract       = {This paper considers whether the cyclical component of the log dividend-price and price-earnings ratios contain forecast power for stock returns. While the levels of these series contain slow moving information for predicting long horizon returns, for short-horizon returns it is the relative movement between prices and fundamental that matters for investors, and whether prices are accelerating away or converging with fundamentals. We use three approaches to extract the cyclical component of these ratios and conduct a range of in-sample and out-of-sample tests. In addition to the cyclical components, we include further predictive variables that account for economic growth and the relation between stocks and bonds. In-sample estimation using the ratio levels reveals results that do not accord with economic intuition. In contrast, using the cyclical component leads to economically sensible values, as well as improved in-sample fit. Out of-sample forecasting reveals that in comparison to a historical mean model, the performance of our predictive models is generally better, although that depends on metrics used to evaluate the forecasts. Moreover, the cyclical component models outperform the levels based models. Notably, the historical mean model is preferred using standard mean absolute and squared errors measures but the predictive models perform better using Mincer-Zarnowitz and related encompassing regressions. Notably, when using economic based forecast evaluation, the predictive models are clearly preferred, with a stronger ability to predict the future direction of return movements and in obtaining higher trading returns. A further examination of the results reveals that this greater performance largely arises from a superior ability to predict future negative returns.},
  day            = {15},
  f1000-projects = {QuantInvest},
  groups         = {Predictability_FinInfo, FrcstQWIM_Equity, FrcstQWIM_Test},
  timestamp      = {2020-02-25 23:22},
}

@Article{Metaxoglou-Smith-2017,
  author               = {Metaxoglou, Konstantinos and Smith, Aaron},
  date                 = {2017},
  journaltitle         = {Journal of Financial Econometrics},
  title                = {Forecasting Stock Returns Using Option-Implied State Prices},
  doi                  = {10.1093/jjfinec/nbx009/3091786/Forecasting-Stock-Returns-Using-Option-Implied},
  abstract             = {Options prices embed the risk preferences that determine expected returns in asset pricing models. Therefore, functions of options prices should predict returns. In this paper, we show that the State Prices of Conditional Quantiles (SPOCQ) - functions of options prices introduced in Metaxoglou and Smith (2016) - exhibit strong predictive ability for the U.S. equity premium. These SPOCQ series provide estimates of the market's willingness to pay for insurance against outcomes in various quantiles of the return distribution. They also relate to expected returns in prominent asset pricing models. Our SPOCQ series that captures relative risk aversion exhibits strong predictive ability for SandP 500 returns at horizons between 6 and 18 months, both in the full sample, 1990-2012, and out of sample. Our SPOCQ series that captures volatility aversion, however, exhibits no predictive ability due to the lack of skewness in the return distribution for the horizons considered.},
  citeulike-article-id = {14334799},
  citeulike-linkout-0  = {https://academic.oup.com/jfec/article-abstract/doi/10.1093/jjfinec/nbx009/3091786/Forecasting-Stock-Returns-Using-Option-Implied?redirectedFrom=fulltext},
  groups               = {FrcstQWIM_MedLngTerm, FrcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2017-04-09 22:57:31},
  timestamp            = {2020-02-25 23:22},
}

@Article{Molyboga-2017,
  author               = {Molyboga, Marat},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Predicting Out-of-Sample Returns: Using Basis to Beat the Historical Average},
  url                  = {https://ssrn.com/abstract=3068321},
  abstract             = {This paper introduces an adaptive predictor that pools information across securities in four major asset classes (commodities, equities, fixed income and foreign exchange) while imposing restrictions on the sign and magnitude of coefficients in return forecasts. I demonstrate that the basis between spot and futures contracts predicts future returns across the asset classes. The predictor consistently beats the historical average, producing a median monthly out-of-sample R2, measured over the period between January 1986 and December 2016, of approximately 0.36 a value that is comparable to those of the best equity premium predictors considered in Campbell and Thompson (2008). A simple long-short strategy based on the new predictor delivers an out-of-sample alpha of 2.54.5\% per annum with respect to the asset pricing models considered and produces an out-of-sample Sharpe ratio of almost 0.5, which is particularly striking since the strategy is countercyclical. This performance is robust across sub-periods, market environments, portfolio construction methodologies and transaction costs. A cross-sectional structure analysis reveals that two observable common factors, constructed as equally-weighted indices of the bases of financial assets and commodities, are related to the short-term interest rate and the business cycle, respectively.},
  citeulike-article-id = {14487897},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3068321},
  groups               = {FrcstQWIM_ShortTerm, FrcstQWIM_MedLngTerm, FrcstQWIM_Bond, FrcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2017-12-04 00:37:03},
  timestamp            = {2020-02-25 23:22},
}

@TechReport{JPMorgan-2017,
  author      = {Marko Kolanovic and Rajesh T. Krishnamachari},
  date        = {2017},
  institution = {JP Morgan},
  title       = {Big Data and AI Strategies: Machine Learning and Alternative Data Approach to Investing},
  url         = {https://www.cfasociety.org/cleveland/Lists/Events%20Calendar/Attachments/1045/BIG-Data_AI-JPMmay2017.pdf},
  groups      = {ML_Classif_QWIM, ML_Network_QWIM, ML_Forecast_QWIM, DeepLearning_QWIM, ML_Test_QWIM, ML_Factor_Invest, FrcstQWIM_ML, ML_AltData},
  timestamp   = {2020-02-25 23:23},
}

@Article{Kim-Swanson-2016a,
  author               = {Kim, Hyun H. and Swanson, Norman R.},
  date                 = {2016-08},
  journaltitle         = {International Journal of Forecasting},
  title                = {Mining big data using parsimonious factor, machine learning, variable selection and shrinkage methods},
  doi                  = {10.1016/j.ijforecast.2016.02.012},
  issn                 = {0169-2070},
  abstract             = {A number of recent studies in the economics literature have focused on the usefulness of factor models in the context of prediction using "big data"(see Bai and Ng, 2008; Dufour and Stevanovic, 2010; Forni, Hallin, Lippi, and Reichlin, 2000; Forni et al., 2005; Kim and Swanson, 2014a; Stock and Watson, 2002b, 2006, 2012, and the references cited therein). We add to this literature by analyzing whether "big data"are useful for modelling low frequency macroeconomic variables, such as unemployment, inflation and GDP. In particular, we analyze the predictive benefits associated with the use of principal component analysis (PCA), independent component analysis (ICA), and sparse principal component analysis (SPCA). We also evaluate machine learning, variable selection and shrinkage methods, including bagging, boosting, ridge regression, least angle regression, the elastic net, and the non-negative garotte. Our approach is to carry out a forecasting "horse-race"using prediction models that are constructed based on a variety of model specification approaches, factor estimation methods, and data windowing methods, in the context of predicting 11 macroeconomic variables that are relevant to monetary policy assessment. In many instances, we find that various of our benchmark models, including autoregressive (AR) models, AR models with exogenous variables, and (Bayesian) model averaging, do not dominate specifications based on factor-type dimension reduction combined with various machine learning, variable selection, and shrinkage methods (called "combination"models). We find that forecast combination methods are mean square forecast error (MSFE) "best"for only three variables out of 11 for a forecast horizon of h=1, and for four variables when h=3 or 1212. In addition, non-PCA type factor estimation methods yield MSFE-best predictions for nine variables out of 11 for h=1h=1, although PCA dominates at longer horizons. Interestingly, we also find evidence of the usefulness of combination models for approximately half of our variables when h>1h>1. Most importantly, we present strong new evidence of the usefulness of factor-based dimension reduction when utilizing "big data"for macroeconometric forecasting.},
  citeulike-article-id = {14219688},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2016.02.012},
  groups               = {FrcstQWIM_ML},
  owner                = {cristi},
  posted-at            = {2016-12-04 19:44:54},
  timestamp            = {2020-02-25 23:23},
}

@Article{MasnadiShirazi-2017,
  author         = {Masnadi-Shirazi, Hamed},
  date           = {2017-07-08},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Combining Forecasts Using Ensemble Learning},
  url            = {https://arxiv.org/abs/1707.02430},
  abstract       = {The problem of combining individual forecasters to produce a forecaster with improved performance is considered. The connections between probability elicitation and classification are used to pose the combining forecaster problem as that of ensemble learning. With this connection in place, a number of theoretically sound ensemble learning methods such as Bagging and Boosting are adapted for combining forecasters. It is shown that the simple yet effective method of averaging the forecasts is equivalent to Bagging. This provides theoretical insight into why the well established averaging of forecasts method works so well. Also, a nonlinear combination of forecasters can be attained through Boosting which is shown to theoretically produce combined forecasters that are both calibrated and highly refined. Finally, the proposed methods of combining forecasters are applied to the Good Judgment Project data set and are shown to outperform the individual forecasters.},
  day            = {8},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_ML},
  timestamp      = {2020-02-25 23:23},
}

@Article{Milosevic-2016,
  author               = {Milosevic, Nikola},
  date                 = {2016-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Equity forecast: Predicting long term stock price movement using machine learning},
  eprint               = {1603.00751},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.00751},
  abstract             = {Long term investment is one of the major investment strategies. However, calculating intrinsic value of some company and evaluating shares for long term investment is not easy, since analyst have to care about a large number of financial indicators and evaluate them in a right manner. So far, little help in predicting the direction of the company value over the longer period of time has been provided from the machines.

In this paper we present a machine learning aided approach to evaluate the equity's future price over the long time. Our method is able to correctly predict whether some company's value will be 10 percent higher or not over the period of one year in 76.5 percent of cases.},
  citeulike-article-id = {13963998},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.00751},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.00751},
  day                  = {2},
  groups               = {FrcstQWIM_ML},
  owner                = {cristi},
  posted-at            = {2016-03-03 14:43:14},
  timestamp            = {2020-02-25 23:23},
}

@Article{Pena-et-al-2017,
  author         = {Pena, Daniel and Smucler, Ezequiel and Yohai, Victor J.},
  date           = {2017-08-15},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Forecasting Multiple Time Series with One-Sided Dynamic Principal Components},
  url            = {https://arxiv.org/abs/1708.04705},
  abstract       = {We define one-sided dynamic principal components (ODPC) for time series as linear combinations of the present and past values of the series that minimize the reconstruction mean squared error. Previous definitions of dynamic principal components depend on past and future values of the series. For this reason, they are not appropriate for forecasting purposes. On the contrary, it is shown that the ODPC introduced in this paper can be successfully used for forecasting high-dimensional multiple time series. An alternating least squares algorithm to compute the proposed ODPC is presented. We prove that for stationary and ergodic time series the estimated values converge to their population analogues. We also prove that asymptotically, when both the number of series and the sample size go to infinity, if the data follows a dynamic factor model, the reconstruction obtained with ODPC converges, in mean squared error, to the common part of the factor model. Monte Carlo results shows that forecasts obtained by the ODPC compare favourably with other forecasting methods based on dynamic factor models.},
  day            = {15},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs},
  timestamp      = {2020-02-25 23:23},
}

@Article{Petropoulos-et-al-2018,
  author         = {Petropoulos, Fotios and Hyndman, Rob J. and Bergmeir, Christoph},
  date           = {2018-07},
  journaltitle   = {European Journal of Operational Research},
  title          = {Exploring the sources of uncertainty: Why does bagging for time series forecasting work?},
  doi            = {10.1016/j.ejor.2018.01.045},
  issn           = {0377-2217},
  number         = {2},
  pages          = {545--554},
  url            = {http://linkinghub.elsevier.com/retrieve/pii/{S037722171830081X}},
  volume         = {268},
  abstract       = {In a recent study, Bergmeir, Hyndman and Benitez (2016) successfully employed a bootstrap aggregation (bagging) technique for improving the performance of exponential smoothing. Each series is Box-Cox transformed, and decomposed by Seasonal and Trend decomposition using Loess (STL); then bootstrapping is applied on the remainder series before the trend and seasonality are added back, and the transformation reversed to create bootstrapped versions of the series. Subsequently, they apply automatic exponential smoothing on the original series and the bootstrapped versions of the series, with the final forecast being the equal-weight combination across all forecasts. In this study we attempt to address the question: why does bagging for time series forecasting work? We assume three sources of uncertainty (model uncertainty, data uncertainty, and parameter uncertainty) and we separately explore the benefits of bagging for time series forecasting for each one of them. Our analysis considers 4004 time series (from the M- and M3-competitions) and two families of models. The results show that the benefits of bagging predominantly originate from the model uncertainty: the fact that different models might be selected as optimal for the bootstrapped series. As such, a suitable weighted combination of the most suitable models should be preferred to selecting a single model.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs},
  timestamp      = {2020-02-25 23:23},
}

@Article{Petropoulos-et-al-2018a,
  author         = {Petropoulos, Fotios and Wang, Xun and Disney, Stephen M.},
  date           = {2018-03},
  journaltitle   = {International Journal of Forecasting},
  title          = {The inventory performance of forecasting methods: Evidence from the M3 competition data},
  doi            = {10.1016/j.ijforecast.2018.01.004},
  issn           = {0169-2070},
  url            = {http://linkinghub.elsevier.com/retrieve/pii/S0169207018300232},
  abstract       = {Forecasting competitions have been a major driver not only of improvements in forecasting methods performances, but also of the development of new forecasting approaches. However, despite the tremendous value and impact of these competitions, they do suffer from the limitation that performances are measured only in terms of the forecast accuracy and bias, ignoring utility metrics. Using the monthly industry series of the M3 competition, we empirically explore the inventory performances of various widely used forecasting techniques, including exponential smoothing, ARIMA models, the Theta method, and approaches based on multiple temporal aggregation. We employ a rolling simulation approach and analyse the results for the order-up-to policy under various lead times. We find that the methods that are based on combinations result in superior inventory performances, while the Naive, Holt, and Holt-Winters methods perform poorly.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs},
  timestamp      = {2020-02-25 23:23},
}

@Article{Rodrigues-Salish-2015,
  author               = {Rodrigues, PauloM and Salish, Nazarii},
  date                 = {2015},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Modeling and forecasting interval time series with threshold models},
  doi                  = {10.1007/s11634-014-0170-x},
  number               = {1},
  pages                = {41--57},
  volume               = {9},
  abstract             = {This paper proposes threshold models to analyze and forecast interval-valued time series. A relatively simple algorithm is proposed to obtain least square estimates of the threshold and slope parameters. The construction of forecasts based on the proposed model and methods for the analysis of their forecast performance are also introduced and discussed, as well as forecasting procedures based on the combination of different models. To illustrate the usefulness of the proposed methods, an empirical application on a weekly sample of S\&P 500 index returns is provided. The results obtained are encouraging and compare very favorably to available procedures.},
  citeulike-article-id = {13995878},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-014-0170-x},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11634-014-0170-x},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-04-04 01:08:29},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-25 23:23},
}

@Article{RodriguezRivero-et-al-2017,
  author         = {Rodriguez Rivero, Cristian and Pucheta, Julian and Orjuela Canon, Alvaro and Franco, Leonardo and Tupac Valdivia, Yvan and Otano, Paula and Sauchelli, Victor},
  date           = {2017},
  journaltitle   = {IEEE Latin America Transactions},
  title          = {Noisy Chaotic time series forecast approximated by combining Reny's entropy with Energy associated to series method: application to rainfall series},
  doi            = {10.1109/{TLA}.2017.7959353},
  issn           = {1548-0992},
  number         = {7},
  pages          = {1318--1325},
  url            = {http://ieeexplore.ieee.org/document/7959353/},
  volume         = {15},
  abstract       = {This article proposes that the combination of smoothing approach considering the entropic information provided by Renyi's method, has an acceptable performance in term of forecasting errors. The methodology of the proposed scheme is examined through benchmark chaotic time series, such as Mackey Glass, Lorenz, Henon maps, the Lynx and rainfall from Santa Francisca - Cordoba, with addition of white noise by using neural networks-based energy associated (EAS) predictor filter modified by Renyi's entropy of the series. When the time series is short or long, the underlying dynamical system is nonlinear and temporal dependencies span long time intervals, in which this are also called long memory process. In such cases, the inherent nonlinearity of neural networks models and a higher robustness to noise seem to partially explain their better prediction performance when entropic information is extracted from the series. Then, to demonstrate that permutation entropy is computationally efficient, robust to outliers, and effective to measure complexity of time series, computational results are evaluated against several non-linear ANN predictors to show the predictability of noisy rainfall and chaotic time series reported in the literature.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, Data_NonLinear},
  timestamp      = {2020-02-25 23:23},
}

@Article{Sambasivan-Das-2017,
  author               = {Sambasivan, Rajiv and Das, Sourish},
  date                 = {2017-03-04},
  journaltitle         = {arXiv Electronic Journal},
  title                = {A Statistical Machine Learning Approach to Yield Curve Forecasting},
  eprint               = {1703.01536},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1703.01536},
  abstract             = {Yield curve forecasting is an important problem in finance. In this work we explore the use of Gaussian Processes in conjunction with a dynamic modeling strategy, much like the Kalman Filter, to model the yield curve. Gaussian Processes have been successfully applied to model functional data in a variety of applications. A Gaussian Process is used to model the yield curve. The hyper-parameters of the Gaussian Process model are updated as the algorithm receives yield curve data. Yield curve data is typically available as a time series with a frequency of one day. We compare existing methods to forecast the yield curve with the proposed method. The results of this study showed that while a competing method (a multivariate time series method) performed well in forecasting the yields at the short term structure region of the yield curve, Gaussian Processes perform well in the medium and long term structure regions of the yield curve. Accuracy in the long term structure region of the yield curve has important practical implications. The Gaussian Process framework yields uncertainty and probability estimates directly in contrast to other competing methods. Analysts are frequently interested in this information. In this study the proposed method has been applied to yield curve forecasting, however it can be applied to model high frequency time series data or data streams in other domains.},
  citeulike-article-id = {14510859},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.01536},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.01536},
  day                  = {4},
  groups               = {FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_ForcstTimeSrs},
  posted-at            = {2018-01-02 02:11:16},
  timestamp            = {2020-02-25 23:23},
}

@Article{Sergio-et-al-2016,
  author               = {Sergio, Anderson T. and de Lima, Tiago P. F. and Ludermir, Teresa B.},
  date                 = {2016-12},
  journaltitle         = {Neurocomputing},
  title                = {Dynamic selection of forecast combiners},
  doi                  = {10.1016/j.neucom.2016.08.072},
  issn                 = {0925-2312},
  pages                = {37--50},
  volume               = {218},
  abstract             = {Time series forecasting is an important research field in machine learning. Since the literature shows several techniques for the solution of this problem, combining outputs of different models is a simple and robust strategy. However, even when using combiners, the experimenter may face the following dilemma: which technique should one use to combine the individual predictors? Inspired by classification and pattern recognition algorithms, this work presents a dynamic selection method of forecast combiners. In the dynamic selection, each test pattern is submitted to a certain combiner according to a nearest neighbor rule. The proposed method was used to forecast eight time series with chaotic behavior in short and long term. In general, the dynamic selection presented satisfactory results for all datasets.},
  citeulike-article-id = {14503762},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neucom.2016.08.072},
  groups               = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  posted-at            = {2017-12-16 16:27:27},
  timestamp            = {2020-02-25 23:23},
}

@Book{Shmueli-Lichtendahl-2016,
  author               = {Shmueli, Galit and Lichtendahl, Kenneth C.},
  date                 = {2016},
  title                = {Practical time series forecasting with R : a hands-on guide},
  isbn                 = {9780997847918},
  publisher            = {Axelrod Schnall Publishers},
  url                  = {http://www.forecastingbook.com},
  abstract             = {Practical Time Series Forecasting with R: A Hands-On Guide, Second Edition provides an applied approach to time-series forecasting. Forecasting is an essential component of predictive analytics. The book introduces popular forecasting methods and approaches used in a variety of business applications.

The book offers clear explanations, practical examples, and end-of-chapter exercises and cases. Readers will learn to use forecasting methods using the free open-source R software to develop effective forecasting solutions that extract business value from time-series data.},
  citeulike-article-id = {14332368},
  citeulike-linkout-0  = {http://www.worldcat.org/isbn/9780997847918},
  citeulike-linkout-1  = {http://books.google.com/books?vid=ISBN9780997847918},
  citeulike-linkout-2  = {http://www.amazon.com/gp/search?keywords=9780997847918 and index=books and linkCode=qs},
  citeulike-linkout-3  = {http://www.librarything.com/isbn/9780997847918},
  citeulike-linkout-4  = {http://www.worldcat.org/oclc/957532009},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-04-05 14:42:15},
  timestamp            = {2020-02-25 23:23},
}

@Article{Stock-Watson-2002,
  author               = {Stock, James H. and Watson, Mark W.},
  date                 = {2002-12},
  journaltitle         = {Journal of the American Statistical Association},
  title                = {Forecasting Using Principal Components From a Large Number of Predictors},
  doi                  = {10.1198/016214502388618960},
  issn                 = {0162-1459},
  number               = {460},
  pages                = {1167--1179},
  volume               = {97},
  abstract             = {This article considers forecasting a single time series when there are many predictors (N) and time series observations (T). When the data follow an approximate factor model, the predictors can be summarized by a small number of indexes, which we estimate using principal components. Feasible forecasts are shown to be asymptotically efficient in the sense that the difference between the feasible forecasts and the infeasible forecasts constructed using the actual values of the factors converges in probability to 0 as both N and T grow large. The estimated factors are shown to be consistent, even in the presence of time variation in the factor model.},
  citeulike-article-id = {9934797},
  citeulike-linkout-0  = {http://dx.doi.org/10.1198/016214502388618960},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1198/016214502388618960},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-10-18 08:56:11},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-25 23:23},
}

@Article{Sung-et-al-2018,
  author         = {Sung, Ming-Chien and McDonald, David C.J. and Johnson, Johnnie E.V. and Tai, Chung-Ching and Cheah, Eng-Tuck},
  date           = {2018-06},
  journaltitle   = {European Journal of Operational Research},
  title          = {Improving prediction market forecasts by detecting and correcting possible over-reaction to price movements},
  doi            = {10.1016/j.ejor.2018.06.024},
  issn           = {0377-2217},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0377221718305575},
  abstract       = {We examine the impact of price trends on the accuracy of forecasts from prediction markets. In particular, we study an electronic betting exchange market and construct independent variables from market price (odds) time series from 6058 individual markets (a dataset consisting of over 8.4 million price points). Using a conditional logit model, we find that a systematic relationship exists between trends in odds and the accuracy of odds-implied event probabilities; the relationship is consistent with participants over-reacting to price movements. In particular, in different time segments of the market, increasing and decreasing odds lead, respectively, to under- and over-estimation of odds-implied probabilities. We develop a methodology to detect and correct the erroneous forecasts associated with these trends in odds in order to considerably improve the quality of forecasts generated in prediction markets.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs},
  timestamp      = {2020-02-25 23:23},
}

@Article{Thomakos-Nikolopoulos-2015,
  author               = {Thomakos, Dimitrios D. and Nikolopoulos, Konstantinos},
  date                 = {2015-04},
  journaltitle         = {Journal of Forecasting},
  title                = {Forecasting Multivariate Time Series with the Theta Method},
  doi                  = {10.1002/for.2334},
  number               = {3},
  pages                = {220--229},
  volume               = {34},
  abstract             = {In this study building on earlier work on the properties and performance of the univariate Theta method for a unit root data-generating process we:

(a) derive new theoretical formulations for the application of the method on multivariate time series;

(b) investigate the conditions for which the multivariate Theta method is expected to forecast better than the univariate one;

(c) evaluate through simulations the bivariate form of the method;

(d) evaluate this latter model in real macroeconomic and financial time series.

The study provides sufficient empirical evidence to illustrate the suitability of the method for vector forecasting; furthermore it provides the motivation for further investigation of the multivariate Theta method for higher dimensions.},
  citeulike-article-id = {13935001},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2334},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:01:15},
  timestamp            = {2020-02-25 23:23},
}

@Article{Tong-et-al-2018,
  author         = {Tong, Tao and Shah, Manas and Cherukumalli, Manoj and Moulehiawy, Yasmine},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Investigating Long Short-Term Memory Neural Networks for Financial Time-Series Prediction},
  doi            = {10.2139/ssrn.3175336},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3175336},
  abstract       = {Stock prices are co-integrated with fundamental valuation factors (earnings, revenue, cash flow, etc.). This has allowed legendary value investors such as Warren Buffett to enjoy tremendous investment success over the long-term. However, the market is efficient at discounting past information and consensus future expectations. Having the ability to read non-obvious information and to have a better prediction at future valuation factors (surprises from consensus estimates) would likely lead to investment outperformance (generating alpha returns) over the long run. In a recently published work, Alberg and Lipton [1] used various artificial neural networks (ANN) to predict companies valuation ratios from their historical fundamental factors and suggested promising results. Aligned with this thought, this research project aims at using Long Short-Term Memory (LSTM) neural network, a type of recurrent neural network (RNN), to predict future fundamental valuation factors of companies and then test investment results by applying active risk-return optimized portfolio strategies [2]. The reasons for choosing LSTM network for this study are the following: (1) like a deep neural network, LSTM is a flexible universal function approximator suited for time-series forecast; (2) LSTM, unlike vanilla version of RNN, does not suffer from gradient problem [3] and is well suited in discovering long-range characteristics, hence its name. Given that LSTM network (or deep neural network in general) enjoys a reputation of being prone to over-fitting to in-sample data, we spend a significant amount of efforts in studying the over-fitting behavior and try to lay out systematic procedures in detecting and mitigating such issues. The current work gives us confidence and excitement that much more can be explored to potentially further improve the prediction performance and investment returns.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, PortfOptim_Network, FrcstQWIM_Equity, FrcstQWIM_Test},
  timestamp      = {2020-02-25 23:23},
}

@Article{Veeramachaneni-2012,
  author               = {Veeramachaneni, Sriharsha},
  date                 = {2012-11},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Time-series Scenario Forecasting},
  eprint               = {1211.3010},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1211.3010},
  abstract             = {Many applications require the ability to judge uncertainty of time-series forecasts. Uncertainty is often specified as point-wise error bars around a mean or median forecast. Due to temporal dependencies, such a method obscures some information. We would ideally have a way to query the posterior probability of the entire time-series given the predictive variables, or at a minimum, be able to draw samples from this distribution. We use a Bayesian dictionary learning algorithm to statistically generate an ensemble of forecasts. We show that the algorithm performs as well as a physics-based ensemble method for temperature forecasts for Houston. We conclude that the method shows promise for scenario forecasting where physics-based methods are absent.},
  citeulike-article-id = {13987664},
  citeulike-linkout-0  = {http://arxiv.org/abs/1211.3010},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1211.3010},
  day                  = {13},
  groups               = {Scenario generation, FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-03-24 20:36:09},
  timestamp            = {2020-02-25 23:23},
}

@Article{Wang-et-al-2013,
  author               = {Wang, Yongning and Tsay, Ruey S. and Ledolter, Johannes and Shrestha, Keshab M.},
  date                 = {2013-12},
  journaltitle         = {Journal of Forecasting},
  title                = {Forecasting Simultaneously High-Dimensional Time Series: A Robust Model-Based Clustering Approach},
  doi                  = {10.1002/for.2264},
  number               = {8},
  pages                = {673--684},
  volume               = {32},
  abstract             = {This paper considers the problem of forecasting high-dimensional time series. It employs a robust clustering approach to perform classification of the component series. Each series within a cluster is assumed to follow the same model and the data are then pooled for estimation. The classification is model-based and robust to outlier contamination. The robustness is achieved by using the intrinsic mode functions of the Hilbert-Huang transform at lower frequencies.

These functions are found to be robust to outlier contamination. The paper also compares out-of-sample forecast performance of the proposed method with several methods available in the literature. The other forecasting methods considered include vector autoregressive models with or without LASSO, group LASSO, principal component regression, and partial least squares.

The proposed method is found to perform well in out-of-sample forecasting of the monthly unemployment rates of 50 US states.},
  citeulike-article-id = {12519623},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2264},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:10:43},
  timestamp            = {2020-02-25 23:23},
}

@Article{Wong-et-al-2017,
  author               = {Wong, Shiu F. and Tong, Howell and Siu, Tak K. and Lu, Zudi},
  date                 = {2017-03},
  journaltitle         = {Journal of Time Series Analysis},
  title                = {A New Multivariate Nonlinear Time Series Model for Portfolio Risk Measurement: The Threshold Copula-Based TAR Approach},
  doi                  = {10.1111/jtsa.12206},
  issn                 = {0143-9782},
  number               = {2},
  pages                = {243--265},
  volume               = {38},
  abstract             = {We propose a threshold copula-based nonlinear time series model for evaluating quantitative risk measures for financial portfolios with a flexible structure to incorporate nonlinearities in both univariate (component) time series and their dependent structure. We incorporate different dependent structures of asset returns over different market regimes, which are manifested in their price levels. We estimate the model parameters by a two-stage maximum likelihood method. Real financial data and appropriate statistical tests are used to illustrate the efficacy of the proposed model. Simulated results for sampling distribution of parameters estimates are given. Empirical results suggest that the proposed model leads to significant improvement of the accuracy of value-at-risk forecasts at the portfolio level.},
  citeulike-article-id = {14327816},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/jtsa.12206},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-04-03 19:19:10},
  timestamp            = {2020-02-25 23:23},
}

@InCollection{Wu-et-al-2015b,
  author               = {Wu, Shin-Fu and Chang, Chia-Yung and Lee, Shie-Jue},
  booktitle            = {Proceedings of the 1st International Conference on Industrial Networks and Intelligent Systems},
  date                 = {2015},
  title                = {Time Series Forecasting with Missing Values},
  doi                  = {10.4108/icst.iniscom.2015.258269},
  isbn                 = {978-1-63190-022-8},
  location             = {Tokyo, Japan},
  publisher            = {ICST},
  abstract             = {Time series prediction has become more popular in various kinds of applications such as weather prediction, control engineering, financial analysis, industrial monitoring, etc. To deal with real-world problems, we are often faced with missing values in the data due to sensor malfunctions or human errors. Traditionally, the missing values are simply omitted or replaced by means of imputation methods. However, omitting those missing values may cause temporal discontinuity. Imputation methods, on the other hand, may alter the original time series. In this study, we propose a novel forecasting method based on least squares support vector machine (LSSVM). We employ the input patterns with the temporal information which is defined as local time index (LTI). Time series data as well as local time indexes are fed to LSSVM for doing forecasting without imputation. We compare the forecasting performance of our method with other imputation methods. Experimental results show that the proposed method is promising and is worth further investigations.},
  citeulike-article-id = {14272428},
  citeulike-linkout-0  = {http://dx.doi.org/10.4108/icst.iniscom.2015.258269},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-02-06 20:56:52},
  timestamp            = {2020-02-25 23:23},
}

@Article{Yu-et-al-2014b,
  author               = {Yu, Yufeng and Zhu, Yuelong and Li, Shijin and Wan, Dingsheng},
  date                 = {2014},
  journaltitle         = {Mathematical Problems in Engineering},
  title                = {Time Series Outlier Detection Based on Sliding Window Prediction},
  doi                  = {10.1155/2014/879736},
  issn                 = {1024-123X},
  pages                = {1--14},
  volume               = {2014},
  abstract             = {In order to detect outliers in hydrological time series data for improving data quality and decision-making quality related to design, operation, and management of water resources, this research develops a time series outlier detection method for hydrologic data that can be used to identify data that deviate from historical patterns. The method first built a forecasting model on the history data and then used it to predict future values. Anomalies are assumed to take place if the observed values fall outside a given prediction confidence interval (PCI), which can be calculated by the predicted value and confidence coefficient. The use of PCI as threshold is mainly on the fact that it considers the uncertainty in the data series parameters in the forecasting model to address the suitable threshold selection problem. The method performs fast, incremental evaluation of data as it becomes available, scales to large quantities of data, and requires no preclassification of anomalies. Experiments with different hydrologic real-world time series showed that the proposed methods are fast and correctly identify abnormal data and can be used for hydrologic time series analysis.},
  citeulike-article-id = {14527393},
  citeulike-linkout-0  = {http://dx.doi.org/10.1155/2014/879736},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2018-01-31 10:32:14},
  timestamp            = {2020-02-25 23:23},
}

@Article{Zhang-et-al-2018f,
  author         = {Zhang, Anderson Y. and Lu, Miao and Kong, Deguang and Yang, Jimmy},
  date           = {2018-02-15},
  journaltitle   = {openReview},
  title          = {Bayesian Time Series Forecasting with Change Point and Anomaly Detection},
  url            = {https://openreview.net/forum?id={rJLTTe}-{0W}},
  abstract       = {Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.},
  day            = {15},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, ChngPoints_TimeSrs},
  timestamp      = {2020-02-25 23:23},
}

@Book{Zucchini-et-al-2016,
  author               = {Zucchini, Walter and MacDonald, Iain L. and Langrock, Roland},
  date                 = {2016},
  title                = {Hidden Markov Models for Time Series: An Introduction Using R},
  edition              = {Second Edition},
  publisher            = {CRC Press},
  url                  = {https://www.crcpress.com/Hidden-Markov-Models-for-Time-Series-An-Introduction-Using-R-Second-Edition/Zucchini-MacDonald-Langrock/p/book/9781482253832},
  abstract             = {Hidden Markov Models for Time Series: An Introduction Using R, Second Edition illustrates the great flexibility of hidden Markov models (HMMs) as general-purpose models for time series data. The book provides a broad understanding of the models and their uses.

After presenting the basic model formulation, the book covers estimation, forecasting, decoding, prediction, model selection, and Bayesian inference for HMMs. Through examples and applications, the authors describe how to extend and generalize the basic model so that it can be applied in a rich variety of situations.

The book demonstrates how HMMs can be applied to a wide range of types of time series: continuous-valued, circular, multivariate, binary, bounded and unbounded counts, and categorical observations. It also discusses how to employ the freely available computing environment R to carry out the computations.},
  citeulike-article-id = {14499102},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-12-08 01:13:02},
  timestamp            = {2020-02-25 23:23},
}

@Article{Proietti-2013,
  author               = {Proietti, Tommaso},
  date                 = {2013-10},
  journaltitle         = {International Journal of Forecasting},
  title                = {Discussion of the paper Testing Time Series Data Compatibility for Benchmarking},
  doi                  = {10.1016/j.ijforecast.2012.03.001},
  issn                 = {0169-2070},
  number               = {4},
  pages                = {767--771},
  volume               = {29},
  abstract             = {Benchmarking is an important component of the reliability of macroeconomic information, and one which is relevant for both the final data user and the econometrician, as it deals with combining information from different sources, characterised by different frequencies of observation and different degrees of reliability.

The paper by Quenneville and Gagne proposes tests for checking the compatibility of the series as a step preliminary to benchmarking. This discussion focuses on the actual implementation of these tests, on alternative tests available in the econometric literature, and on the various sources of measurement errors that affect benchmarking.},
  citeulike-article-id = {10695253},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2012.03.001},
  groups               = {FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-02-15 07:55:03},
  timestamp            = {2020-02-25 23:23},
}

@Article{Pyo-et-al-2017,
  author               = {Pyo, Sujin and Lee, Jaewook and Cha, Mincheol and Jang, Huisu},
  date                 = {2017-11-14},
  journaltitle         = {PLOS ONE},
  title                = {Predictability of machine learning techniques to forecast the trends of market index prices: Hypothesis testing for the Korean stock markets},
  doi                  = {10.1371/journal.pone.0188107},
  number               = {11},
  pages                = {e0188107+},
  volume               = {12},
  abstract             = {The prediction of the trends of stocks and index prices is one of the important issues to market participants. Investors have set trading or fiscal strategies based on the trends, and considerable research in various academic fields has been studied to forecast financial markets. This study predicts the trends of the Korea Composite Stock Price Index 200 (KOSPI 200) prices using nonparametric machine learning models: artificial neural network, support vector machines with polynomial and radial basis function kernels. In addition, this study states controversial issues and tests hypotheses about the issues. Accordingly, our results are inconsistent with those of the precedent research, which are generally considered to have high prediction performance. Moreover, Google Trends proved that they are not effective factors in predicting the KOSPI 200 index prices in our frameworks. Furthermore, the ensemble methods did not improve the accuracy of the prediction.},
  citeulike-article-id = {14503587},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0188107},
  day                  = {14},
  groups               = {Predictability_FinInfo, FrcstQWIM_Test, FrcstQWIM_ML},
  posted-at            = {2017-12-15 22:19:04},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-25 23:23},
}

@Article{Quennevillle-Gagne-2013,
  author               = {Quennevillle, Benoit and Gagne, Christian},
  date                 = {2013-10},
  journaltitle         = {International Journal of Forecasting},
  title                = {Testing time series data compatibility for benchmarking},
  doi                  = {10.1016/j.ijforecast.2011.10.001},
  issn                 = {0169-2070},
  number               = {4},
  pages                = {754--766},
  volume               = {29},
  abstract             = {Compatibility testing determines whether two series, say a sub-annual and an annual series, both of which are subject to sampling errors, can be considered suitable for benchmarking. We derive statistical tests and discuss the issues with their implementation.

The results are illustrated using the artificial series from Denton (1971) and two empirical examples. A practical way of implementing the tests is also presented.},
  citeulike-article-id = {10136609},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2011.10.001},
  groups               = {FrcstQWIM_Test, Stat_Test},
  owner                = {cristi},
  posted-at            = {2016-02-15 07:54:52},
  timestamp            = {2020-02-25 23:23},
}

@Article{Shang-2015,
  author               = {Shang, Han L.},
  date                 = {2015-09},
  journaltitle         = {Population Studies},
  title                = {Statistically tested comparisons of the accuracy of forecasting methods for age-specific and sex-specific mortality and life expectancy},
  doi                  = {10.1080/00324728.2015.1074268},
  number               = {3},
  pages                = {317--335},
  volume               = {69},
  abstract             = {Although there are continuing developments in the methods for forecasting mortality, there are few comparisons of the accuracy of the forecasts. The subject of the statistical validity of these comparisons, which is essential to demographic forecasting, has all but been ignored. We introduce Friedman's test statistics to examine whether the differences in point and interval forecast accuracies are statistically significant between methods. We introduce the Nemenyi test statistic to identify which methods give results that are statistically significantly different from others. Using sex-specific and age-specific data from 20 countries, we apply these two test statistics to examine the forecast accuracy obtained from several principal component methods, which can be categorized into coherent and non-coherent forecasting methods.},
  citeulike-article-id = {14014326},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/00324728.2015.1074268},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/00324728.2015.1074268},
  day                  = {2},
  groups               = {FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-04-17 17:31:02},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 23:23},
}

@Article{Stivers-2018,
  author               = {Stivers, Adam},
  date                 = {2018-01},
  journaltitle         = {Journal of Empirical Finance},
  title                = {Equity premium predictions with many predictors: A risk-based explanation of the size and value factors},
  doi                  = {10.1016/j.jempfin.2017.10.004},
  issn                 = {0927-5398},
  pages                = {126--140},
  volume               = {45},
  abstract             = {This paper investigates whether a direct mechanism can be found that demonstrates that the size and value factors of Fama and French (1993) are indeed ICAPM factors, as some have suggested. The results endorse this hypothesis: small size and value portfolios reflect changes in future investment opportunities. To test the hypothesis, the paper forecasts the equity premium using disaggregated portfolio returns with a partial least squares (PLS) regression approach. PLS is chosen as it is particularly suited to condense a large set of portfolios into a single index. The "index" portfolio obtained from the forecast performs well out of sample and hedges against future market risk, in addition to explaining future market returns in sample. Thus, the index portfolio may be viewed as an additional risk factor in the form of a Merton (1973) state variable. The index places larger weights on small size and value portfolios. This also provides a possible explanation for why equal-weighted portfolios typically perform better out of sample compared to factors implied by traditional mean-variance approaches and asset pricing models.},
  citeulike-article-id = {14495959},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jempfin.2017.10.004},
  groups               = {Invest_Risk, Factor_Test, FrcstQWIM_MedLngTerm, FrcstQWIM_Equity, FrcstQWIM_Test},
  posted-at            = {2017-12-05 03:52:31},
  timestamp            = {2020-02-25 23:23},
}

@Article{Westerlund-Narayan-2016,
  author               = {Westerlund, Joakim and Narayan, Paresh},
  date                 = {2016-10},
  journaltitle         = {International Journal of Forecasting},
  title                = {Testing for predictability in panels of any time series dimension},
  doi                  = {10.1016/j.ijforecast.2016.02.009},
  issn                 = {0169-2070},
  number               = {4},
  pages                = {1162--1177},
  volume               = {32},
  abstract             = {The few panel data tests for the predictability of returns that exist are based on the prerequisite that both the number of time series observations, TT, and the number of cross-section units, NN, are large. As a result, it is impossible to apply these tests to stock markets, where lengthy time series of data are scarce. In response to this, the current paper develops a new test for predictability in panels where NN is large and T2T2 can be either small or large, or indeed anything in between. This consideration represents an advancement relative to the usual large-NN and large-TT requirement. The new test is also very general, especially when it comes to allowable predictors, and is easy to implement. As an illustration, we consider the Chinese stock market, for which data are available for only 17 years, but where the number of firms is relatively large, 160.},
  citeulike-article-id = {14070590},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2016.02.009},
  groups               = {Predictability_FinInfo, FrcstQWIM_Test},
  owner                = {zkgst0c},
  posted-at            = {2016-06-16 23:16:52},
  timestamp            = {2020-02-25 23:23},
}

@PhdThesis{Yao-2015,
  author               = {Yao, Jiawei},
  date                 = {2015},
  institution          = {Princeton University},
  title                = {Factor Models: Testing and Forecasting},
  url                  = {https://dataspace.princeton.edu/jspui/handle/88435/dsp01ww72bd75b},
  abstract             = {This dissertation focuses on two aspects of factor models, testing and forecasting. For testing, we investigate a more general high-dimensional testing problem, with an emphasis on panel data models. Specifically, we propose a novel technique to boost the power of testing a high-dimensional vector against sparse alternatives. Existing tests based on quadratic forms such as the Wald statistic often suffer from low powers, whereas more powerful tests such as thresholding and extreme-value tests require either stringent conditions or bootstrap to derive the null distribution, and often suffer from size distortions. Based on a screening technique, we introduce a ``power enhancement component , which is zero under the null hypothesis with high probability, but diverges quickly under sparse alternatives. The proposed test statistic combines the power enhancement component with an asymptotically pivotal statistic, and strengthens the power under sparse alternatives. As a byproduct, the power enhancement component also consistently identifies the elements that violate the null hypothesis. Next, we consider forecasting a single time series using many predictors when nonliearity is present. We develop a new methodology called sufficient forecasting, by connecting sliced inverse regression with factor models. The sufficient forecasting correctly estimates projections of the underlying factors and provides multiple predictive indices for further investigation. We derive asymptotic results for the estimate of the central space spanned by these projection directions. Our method allows the number of predictors larger than the sample size, and therefore extends the applicability of inverse regression. Numerical experiments demonstrate that the proposed method improves upon a linear forecasting model. Our results are further illustrated in an empirical study of macroeconomic variables, where sufficient forecasting is found to deliver additional predictive power over conventional methods.},
  citeulike-article-id = {14072066},
  groups               = {Test_MultiHypotheses, FrcstQWIM_Test, Proba_Test},
  owner                = {cristi},
  posted-at            = {2016-06-19 16:18:38},
  timestamp            = {2020-02-25 23:23},
}

@Article{Yun-2014,
  author               = {Yun, Jaeho},
  date                 = {2014-10},
  journaltitle         = {Journal of Banking and Finance},
  title                = {Out-of-sample density forecasts with affine jump diffusion models},
  doi                  = {10.1016/j.jbankfin.2014.06.024},
  issn                 = {0378-4266},
  pages                = {74--87},
  volume               = {47},
  abstract             = {We conduct density forecast evaluations of the affine jump diffusion models. We use the S\&P 500 stock index and its options contracts. Our results support the time-varying jump risk premia models. The options' information improves density forecasting ability. Beta transformation is used for density parameter updating. We conduct out-of-sample density forecast evaluations of the affine jump diffusion models for the S\&P 500 stock index and its options' contracts. We also examine the time-series consistency between the model-implied spot volatilities using options and returns and only returns. In particular, we focus on the role of the time-varying jump risk premia. Particle filters are used to estimate the model-implied spot volatilities. We also propose the beta transformation approach for recursive parameter updating. Our empirical analysis shows that the inconsistencies between options and returns and only returns are resolved by the introduction of the time-varying jump risk premia. For density forecasts, the time-varying jump risk premia models dominate the other models in terms of likelihood criteria. We also find that for medium-term horizons, the beta transformation can weaken the systematic effect of misspecified AJD models using options and returns.},
  citeulike-article-id = {14071732},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbankfin.2014.06.024},
  groups               = {FrcstQWIM_Test},
  owner                = {cristi},
  posted-at            = {2016-06-18 15:51:22},
  timestamp            = {2020-02-25 23:23},
}

@Article{Zakamulin-2015,
  author       = {Valeriy Zakamulin},
  date         = {2015},
  journaltitle = {The Journal of Portfolio Management},
  title        = {A Test of Covariance-Matrix Forecasting Methods},
  doi          = {10.3905/jpm.2015.41.3.097},
  url          = {https://jpm.pm-research.com/content/41/3/97},
  abstract     = {Providing a more accurate covariance matrix forecast can substantially improve the performance of optimized portfolios. Using out-of-sample tests we evaluate alternative covariance matrix forecasting methods by looking at

(1) their forecast accuracy

(2) their ability to track the volatility of the minimum-variance portfolio

(3) their ability to keep the volatility of the minimum-variance portfolio at a target level. We find large differences between the methods.

Our results suggest that shrinkage of the sample covariance matrix improves neither the forecast accuracy nor the performance of minimum-variance portfolios. In contrast, switching from the sample covariance matrix forecast to a multivariate GARCH forecast reduces forecasting error and portfolio tracking error by at least half.

Our findings also reveal that the exponentially weighted covariance matrix forecast performs only slightly worse than the multivariate GARCH forecast.},
  groups       = {FrcstQWIM_Test},
  owner        = {cristi},
  timestamp    = {2020-02-25 23:23},
}

@Article{Plakandaras-et-al-2015,
  author               = {Plakandaras, Vasilios and Papadimitriou, Theophilos and Gogas, Periklis},
  date                 = {2015-11},
  journaltitle         = {Journal of Forecasting},
  title                = {Forecasting Daily and Monthly Exchange Rates with Machine Learning Techniques},
  doi                  = {10.1002/for.2354},
  number               = {7},
  pages                = {560--573},
  volume               = {34},
  abstract             = {In this paper we propose and test a forecasting model on monthly and daily spot prices of five selected exchange rates. In doing so, we combine a novel smoothing technique (initially applied in signal processing) with a variable selection methodology and two regression estimation methodologies from the field of machine learning (ML). After the decomposition of the original exchange rate series using an ensemble empirical mode decomposition (EEMD) method into a smoothed and a fluctuation component, multivariate adaptive regression splines (MARS) are used to select the most appropriate variable set from a large set of explanatory variables that we collected. The selected variables are then fed into two distinctive support vector machines (SVR) models that produce one-period-ahead forecasts for the two components. Neural networks (NN) are also considered as an alternative to SVR. The sum of the two forecast components is the final forecast of the proposed scheme. We show that the above implementation exhibits a superior in-sample and out-of-sample forecasting ability when compared to alternative forecasting models. The empirical results provide evidence against the efficient market hypothesis for the selected foreign exchange markets.},
  citeulike-article-id = {14071208},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2354},
  day                  = {1},
  groups               = {FrcstQWIM_ML, FrcstQWIM_Other},
  owner                = {zkgst0c},
  posted-at            = {2016-06-17 22:29:58},
  timestamp            = {2020-02-25 23:24},
}

@InCollection{Qiu-et-al-2014,
  author               = {Qiu, Xueheng and Zhang, Le and Ren, Ye and Suganthan, P. and Amaratunga, Gehan},
  booktitle            = {2014 IEEE Symposium on Computational Intelligence in Ensemble Learning (CIEL)},
  date                 = {2014-12},
  title                = {Ensemble deep learning for regression and time series forecasting},
  doi                  = {10.1109/ciel.2014.7015739},
  isbn                 = {978-1-4799-4512-2},
  location             = {Orlando, FL, USA},
  pages                = {1--6},
  publisher            = {IEEE},
  abstract             = {In this paper, for the first time, an ensemble of deep learning belief networks (DBN) is proposed for regression and time series forecasting. Another novel contribution is to aggregate the outputs from various DBNs by a support vector regression (SVR) model. We show the advantage of the proposed method on three electricity load demand datasets, one artificial time series dataset and three regression datasets over other benchmark methods.},
  citeulike-article-id = {14510348},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ciel.2014.7015739},
  groups               = {FrcstQWIM_ML, ML_ForcstTimeSrs},
  posted-at            = {2017-12-30 12:31:54},
  timestamp            = {2020-02-25 23:24},
}

@InCollection{Vargas-et-al-2017,
  author         = {Vargas, Manuel R. and de Lima, Beatriz S. L. P. and Evsukoff, Alexandre G.},
  booktitle      = {2017 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA)},
  date           = {2017-06-26},
  title          = {Deep learning for stock market prediction from financial news articles},
  doi            = {10.1109/{CIVEMSA}.2017.7995302},
  isbn           = {978-1-5090-4253-1},
  pages          = {60--65},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/7995302/},
  abstract       = {This work uses deep learning methods for intraday directional movements prediction of Standard \& Poor's 500 index using financial news titles and a set of technical indicators as input. Deep learning methods can detect and analyze complex patterns and interactions in the data automatically allowing speed up the trading process. This paper focus on architectures such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), which have had good results in traditional NLP tasks. Results has shown that CNN can be better than RNN on catching semantic from texts and RNN is better on catching the context information and modeling complex temporal characteristics for stock market forecasting. The proposed method shows some improvement when compared with similar previous studies.},
  day            = {26},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, FrcstQWIM_ML, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:24},
}

@Article{Wolff-Neugebauer-2018,
  author         = {Wolff, Dominik and Neugebauer, Ulrich},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Machine learning approaches for equity market predictions},
  doi            = {10.2139/ssrn.3265107},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3265107},
  abstract       = {We empirically analyze equity premium predictions with 'traditional' linear models and machine learning approaches. Based on a commonly used dataset of equity market predictors extended by additional fundamental, macroeconomic, sentiment and risk indicators, we find mixed results for machine learning algorithms for equity market predictions. In contrast to sophisticated linear models such as penalized least squares or principal component regressions (PCR), the analyzed machine learning algorithms fail to significantly outperform the historical average benchmark forecast. However, an investment strategy that uses machine learning predictions in a market timing strategy, outperforms a passive buy-and-hold investment. Compared to sophisticated linear prediction models, in our problem set, machine learning algorithms do not improve forecast accuracy.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_ML, FrcstQWIM_Equity, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:24},
}

@Article{Salhi-et-al-2016,
  author               = {Salhi, Khaled and Deaconu, Madalina and Lejay, Antoine and Champagnat, Nicolas and Navet, Nicolas},
  date                 = {2016-11},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Regime switching model for financial data: Empirical risk analysis},
  doi                  = {10.1016/j.physa.2016.05.002},
  issn                 = {0378-4371},
  pages                = {148--157},
  volume               = {461},
  abstract             = {This paper constructs a regime switching model for the univariate Value-at-Risk estimation. Extreme value theory (EVT) and hidden Markov models (HMM) are combined to estimate a hybrid model that takes volatility clustering into account. In the first stage, HMM is used to classify data in crisis and steady periods, while in the second stage, EVT is applied to the previously classified data to rub out the delay between regime switching and their detection. This new model is applied to prices of numerous stocks exchanged on NYSE Euronext Paris over the period 2001-2011. We focus on daily returns for which calibration has to be done on a small dataset. The relative performance of the regime switching model is benchmarked against other well-known modeling techniques, such as stable, power laws and GARCH models. The empirical results show that the regime switching model increases predictive performance of financial forecasting according to the number of violations and tail-loss tests. This suggests that the regime switching model is a robust forecasting variant of power laws model while remaining practical to implement the VaR measurement.},
  citeulike-article-id = {14433191},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2016.05.002},
  groups               = {Regime_Model, FrcstQWIM_Hybrid, Vol_Cluster},
  posted-at            = {2017-09-17 14:45:39},
  timestamp            = {2020-02-25 23:24},
}

@Article{Symitsi-et-al-2018,
  author         = {Symitsi, Efthymia and Symeonidis, Lazaros and Kourtis, Apostolos and Markellos, Raphael},
  date           = {2018-11},
  journaltitle   = {Journal of Banking \& Finance},
  title          = {Covariance forecasting in equity markets},
  doi            = {10.1016/j.jbankfin.2018.08.013},
  issn           = {0378-4266},
  pages          = {153--168},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S037842661830181X}},
  volume         = {96},
  abstract       = {Abstract We compare the performance of popular covariance forecasting models in the context of a portfolio of major European equity indices. We find that models based on high-frequency data offer a clear advantage in terms of statistical accuracy. They also yield more theoretically consistent predictions from an empirical asset pricing perspective, and, lead to superior out-of-sample portfolio performance. Overall, a parsimonious Vector Heterogeneous Autoregressive (VHAR) model that involves lagged daily, weekly and monthly realised covariances achieves the best performance out of the competing models. A promising new simple hybrid covariance estimator is developed that exploits option-implied information and high-frequency data while adjusting for the volatility riskpremium. Relative model performance does not change during the global financial crisis, or, if a different forecast horizon, or, intraday sampling frequency is employed. Finally, our evidence remains robust when we consider an alternative sample of U.S. stocks.},
  f1000-projects = {QuantInvest},
  groups         = {Forecast_Hybrid, FrcstQWIM_Hybrid, FrcstQWIM_MedLngTerm, FrcstQWIM_Equity},
  timestamp      = {2020-02-25 23:24},
}

@Article{Taliaferro-2012,
  author       = {Ryan Taliaferro},
  date         = {2012},
  journaltitle = {The Journal of Investment Strategies},
  title        = {Understanding risk-based portfolios},
  number       = {2},
  pages        = {119--131},
  volume       = {1},
  abstract     = {Equity portfolio managers traditionally form portfolios using forecasts of returns. They seek to identify and hold stocks that will have high returns and to avoid stocks that will have low returns. In contrast, a number of equity strategies that have received attention recently do not employ return forecasts at all. Instead, investors form portfolios based on stocks' risk characteristics.

This paper reviews three non-returns-based (ie, risk-based) alternative equity strategies: minimum variance, equal risk contribution and maximum diversification. Minimum variance strategies build intuitive, low-risk portfolios.

Equal risk contribution strategies are hybrids of risk minimizing and equal weighting strategies, and they, too, have intuitive characteristics and can have lower risk. In contrast, maximum diversification strategies are puzzling. Maximum diversification strategies prefer stocks with low correlations, even if they are risky, and since they do not target risk reductions, return increases or Sharpe ratio increases per se, maximum diversification strategies can produce portfolios with desirable characteristics only by accident.},
  groups       = {FrcstQWIM_Hybrid, FrcstQWIM_Equity},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 23:24},
}

@PhdThesis{Tenyakov-2014,
  author               = {Tenyakov, Anton},
  date                 = {2014},
  institution          = {The University of Western Ontario},
  title                = {Estimation of Hidden Markov Models and Their Applications in Finance},
  abstract             = {Movements of financial variables exhibit extreme fluctuations during periods of economic crisis and times of market uncertainty. They are also affected by institutional policies and intervention of regulatory authorities. These structural changes driving prices and other economic indicators can be captured reasonably by models featuring regime-switching capabilities. Hidden Markov models (HMM) modulating the model parameters to incorporate such regime-switching dynamics have been put forward in recent years, but many of them could still be further improved.

In this research, we aim to address some of the inadequacies of previous regime-switching models in terms of their capacity to provide better forecasts and efficiency in estimating parameters. New models are developed, and their corresponding filtering results are obtained and tested on financial data sets.

The contributions of this research work include the following:

(i) Recursive filtering algorithms are constructed for a regime-switching financial model consistent with no-arbitrage pricing. An application to the filtering and forecasting of futures prices under a multivariate set-up is presented.

(ii) The modelling of risk due to market and funding liquidity is considered by capturing the joint dynamics of three time series (Treasury-Eurodollar spread, VIX and S\&P 500 spread-derived metric), which mirror liquidity levels in the financial markets. HMM filters under a multi-regime mean- reverting model are established.

(iii) Kalman filtering techniques and the change of reference probability-based filtering methods are integrated to obtain hybrid algorithms. A pairs trading investment strategy is supported by the combined power of both HMM and Kalman filters. It is shown that an investor is able to benefit from the proposed interplay of the two filtering methods.

(iv) A zero-delay HMM is devised for the evolution of multivariate foreign exchange rate data under a high-frequency trading environment. Recursive filters for quantities that are functions of a Markov chain are derived, which in turn provide optimal parameter estimates.

(v) An algorithm is designed for the efficient calculation of the joint probability function for the occupation time in a Markov-modulated model for asset returns under a general number of economic regimes. The algorithm is constructed with accessible implementation and practical considerations in mind.},
  citeulike-article-id = {13922428},
  groups               = {FrcstQWIM_Hybrid},
  howpublished         = {Available at http://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=3670 and context=etd},
  owner                = {cristi},
  posted-at            = {2016-01-31 01:00:30},
  school               = {The University of Western Ontario},
  timestamp            = {2020-02-25 23:24},
}

@Article{Tkac-Verner-2016,
  author               = {Tkac, Michal and Verner, Robert},
  date                 = {2016-01},
  journaltitle         = {Applied Soft Computing},
  title                = {Artificial neural networks in business: Two decades of research},
  doi                  = {10.1016/j.asoc.2015.09.040},
  issn                 = {1568-4946},
  pages                = {788--804},
  volume               = {38},
  abstract             = {We introduce a literature review considering articles on artificial neural networks in business published in last two decades. 412 suitable articles are identified and classified according to defined methodology. We focus on date, area of interest, type of neural network, benchmark method, journal and citations. The most applied are multilayer feedforward networks with backpropagation learning performed by gradient descent algorithm. Majority (25.73 of the examined articles were published in Expert Systems with Applications. In recent two decades, artificial neural networks have been extensively used in many business applications. Despite the growing number of research papers, only few studies have been presented focusing on the overview of published findings in this important and popular area. Moreover, the majority of these reviews were introduced more than 15 years ago. The aim of this work is to expand the range of earlier surveys and provide a systematic overview of neural network applications in business between 1994 and 2015. We have covered a total of 412 articles and classified them according to the year of publication, application area, type of neural network, learning algorithm, benchmark method, citations and journal. Our investigation revealed that most of the research has aimed at financial distress and bankruptcy problems, stock price forecasting, and decision support, with special attention to classification tasks. Besides conventional multilayer feedforward network with gradient descent backpropagation, various hybrid networks have been developed in order to improve the performance of standard models. Even though neural networks have been established as well-known method in business, there is enormous space for additional research in order to improve their functioning and increase our understanding of this influential area.},
  citeulike-article-id = {14166642},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.asoc.2015.09.040},
  groups               = {FrcstQWIM_Hybrid},
  owner                = {cristi},
  posted-at            = {2016-10-17 03:40:07},
  timestamp            = {2020-02-25 23:24},
}

@Article{Vlachas-et-al-2018,
  author         = {Vlachas, Pantelis R. and Byeon, Wonmin and Wan, Zhong Y. and Sapsis, Themistoklis P. and Koumoutsakos, Petros},
  date           = {2018-05-23},
  journaltitle   = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title          = {Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks.},
  doi            = {10.1098/rspa.2017.0844},
  number         = {2213},
  pages          = {20170844},
  volume         = {474},
  abstract       = {We introduce a data-driven forecasting method for high-dimensional chaotic systems using long short-term memory (LSTM) recurrent neural networks. The proposed LSTM neural networks perform inference of high-dimensional dynamical systems in their reduced order space and are shown to be an effective set of nonlinear approximators of their attractor. We demonstrate the forecasting performance of the LSTM and compare it with Gaussian processes (GPs) in time series obtained from the Lorenz 96 system, the Kuramoto-Sivashinsky equation and a prototype climate model. The LSTM networks outperform the GPs in short-term forecasting accuracy in all applications considered. A hybrid architecture, extending the LSTM with a mean stochastic model (MSM-LSTM), is proposed to ensure convergence to the invariant measure. This novel hybrid method is fully data-driven and extends the forecasting capabilities of LSTM networks.},
  day            = {23},
  f1000-projects = {QuantInvest},
  groups         = {Forecast_Hybrid, FrcstQWIM_Hybrid},
  pmcid          = {PMC5990702},
  pmid           = {29887750},
  timestamp      = {2020-02-25 23:24},
}

@Article{Wu-Lee-2015a,
  author               = {Wu, Shin-Fu and Lee, Shie-Jue},
  date                 = {2015-01},
  journaltitle         = {Expert Systems with Applications},
  title                = {Employing local modeling in machine learning based methods for time-series prediction},
  doi                  = {10.1016/j.eswa.2014.07.032},
  issn                 = {0957-4174},
  number               = {1},
  pages                = {341--354},
  volume               = {42},
  abstract             = {Propose a local modeling strategy for time series prediction. Consider the trend of a time series by the use of hybrid distance. Proper lags are selected by the use of mutual information. Develop algorithms to extract training patterns from historical data. Show the effectiveness of local modeling by experiments on real-world datasets. Time series prediction has been widely used in a variety of applications in science, engineering, finance, etc. There are two different modeling options for constructing forecasting models in time series prediction. Global modeling constructs a model which is independent from user queries. On the contrary, local modeling constructs a local model for each different query from the user. In this paper, we propose a local modeling strategy and investigate the effectiveness of incorporating local modeling with three popular machine learning based forecasting methods, Neural Network (NN), Adaptive Neuro-Fuzzy Inference System (ANFIS), and Least Squares Support Vector Machine (LS-SVM), for time series prediction. Given a series of historical data, a local context of the user query is located and an appropriate number of lags are selected. Then forecasting models are constructed by applying NN, ANFIS, and LS-SVM, respectively. A number of experiments are conducted and the results show that local modeling can enhance the estimation performance of a forecasting method for time series prediction.},
  citeulike-article-id = {14072230},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2014.07.032},
  groups               = {FrcstQWIM_ML, FrcstQWIM_Hybrid, ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-06-20 02:05:28},
  timestamp            = {2020-02-25 23:24},
}

@Article{Zhou-et-al-2019a,
  author         = {Zhou, Feng and Zhou, Hao-min and Yang, Zhihua and Yang, Lihua},
  date           = {2019-01},
  journaltitle   = {Expert Systems with Applications},
  title          = {EMD2FNN: A strategy combining empirical mode decomposition and factorization machine based neural network for stock market trend prediction},
  doi            = {10.1016/j.eswa.2018.07.065},
  issn           = {0957-4174},
  pages          = {136--151},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0957417418304901},
  volume         = {115},
  abstract       = {Stock market forecasting is a vital component of financial systems. However, the stock prices are highly noisy and non-stationary due to the fact that stock markets are affected by a variety of factors. Predicting stock market trend is usually subject to big challenges. The goal of this paper is to introduce a new hybrid, end-to-end approach containing two stages, the Empirical Mode Decomposition and Factorization Machine based Neural Network (EMD2FNN), to predict the stock market trend. To illustrate the method, we apply EMD2FNN to predict the daily closing prices from the Shanghai Stock Exchange Composite () index, the National Association of Securities Dealers Automated Quotations (NASDAQ) index and the Standard \& Poor 500 Composite Stock Price Index (S\&P 500), which respectively exhibit oscillatory, upward and downward patterns. The results are compared with predictions obtained by other methods, including the neural network (NN) model, the factorization machine based neural network (FNN) model, the empirical mode decomposition based neural network (EMD2NN) model and the wavelet de-noising-based back propagation (WDBP) neural network model. Under the same conditions, the experiments indicate that the proposed methods perform better than the other ones according to the metrics of Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE). Furthermore, we compute the profitability with a simple long-short trading strategy to examine the trading performance of our models in the metrics of Average Annual Return (AAR), Maximum Drawdown (MD), Sharpe Ratio (SR) and AAR/MD. The performances in two different scenarios, when taking or not taking the transaction cost into consideration, are found economically significant.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_Hybrid, Scenario_ExpertView, Invest_Network},
  timestamp      = {2020-02-25 23:24},
}

@Article{Freyberger-et-al-2018,
  author               = {Freyberger, Joachim and Neuhierl, Andreas and Weber, Michael},
  title                = {Dissecting Characteristics Nonparametrically},
  abstract             = {We propose a nonparametric method to test which characteristics provide independent information for the cross section of expected returns. We use the adaptive group LASSO to select characteristics and to estimate how they affect expected returns nonparametrically. Our method can handle a large number of characteristics, allows for a flexible functional form, and is insensitive to outliers. Many of the previously identified return predictors do not provide incremental information for expected returns, and nonlinearities are important. Our proposed method has higher out-of-sample explanatory power compared to linear panel regressions, and increases Sharpe ratios by 50 percent.},
  citeulike-article-id = {14363824},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2820700},
  date                 = {2018},
  groups               = {Characteristics and return prediction, Regression_Nonlinear},
  journaltitle         = {SSRN Electronic Journal},
  posted-at            = {2017-05-27 19:04:24},
  timestamp            = {2020-02-25 23:44},
  url                  = {https://ssrn.com/abstract=2820700},
}

@Article{Liew-Mayster-2017,
  author               = {Liew, Jim K. and Mayster, Boris},
  date                 = {2017-11},
  journaltitle         = {The Journal of Alternative Investments},
  title                = {Forecasting ETFs with Machine Learning Algorithms},
  doi                  = {10.3905/jai.2017.2017.1.058},
  issn                 = {1520-3255},
  abstract             = {In this article, the authors apply cutting-edge machine learning algorithms to one of the oldest challenges in finance: predicting returns. For the sake of simplicity, they focus on predicting the direction (either up or down) of several liquid exchange-traded funds (ETFs) and do not attempt to predict the magnitude of price changes. The ETFs serve as asset class proxies. The authors employ approximately five years (from January 2011 to January 2016) of historical, daily data obtained through Yahoo Finance. Using supervised learning classification algorithms, readily available from Python's Scikit-Learn, they employ three powerful techniques: (1) deep neural networks, (2) random forests, and (3) support vector machines (linear and radial basis function). They document the performance of the three algorithms across four information sets: past returns, past volume, dummies for days/months, and a combination of all three. They use a gain criterion to compare classifiers' performance. First, they find that these algorithms work well over one- to three-month horizons. Short-horizon predictability, over days, is extremely difficult, and thus the results support the short-term random walk hypothesis. Second, they document the importance of cross-sectional and intertemporal volume as a powerful information set. Third, they show that many features are needed for predictability because each feature makes very small contributions toward predictability. The authors conclude that ETF returns can be predicted with machine learning algorithms, but practitioners should incorporate prior knowledge of markets and intuition on asset class behavior.},
  citeulike-article-id = {14485368},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jai.2017.2017.1.058},
  day                  = {24},
  groups               = {Machine learning and investment strategies, ML_Forecast_QWIM, FrcstQWIM_ML, ML_PerfMetrics, FrcstQWIM_ShortTerm, ML_AssetPricing, DeepLearning_QWIM},
  posted-at            = {2017-11-28 22:25:39},
  timestamp            = {2020-02-25 23:46},
}

@Article{Popescu-2019,
  author       = {Popescu, Andreea Victoria},
  date         = {2019},
  journaltitle = {SSRN Electronic Journal},
  title        = {The Macroeconomy and the Cross-Section of International Equity Index Returns: A Machine Learning Approach},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3480042},
  abstract     = {The paper evaluates the out-of-sample predictive ability of machine learning methods in the cross section of international equity index returns using both firm fundamentals and macroeconomic predictors. The study performs a horserace between classical forecasting methods and the machine learning repertoire, including principal component analysis, partial least squares, and neural networks. Macroeconomic signals seem to substantially improve out-of-sample performance, especially when nonlinear features are incorporated via neural networks.},
  timestamp    = {2020-02-25 23:48},
}

@Article{Messmer-Audrino-2017,
  author               = {Messmer, Marcial and Audrino, Francesco},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The (Adaptive) Lasso in the Zoo - Firm Characteristic Selection in the Cross-Section of Expected Returns},
  url                  = {https://ssrn.com/abstract=2930436},
  abstract             = {We find short-term reversal, the twelve-months momentum and research spending scaled by market-value to be the firm characteristics (FC) most robustly selected by the adaptive Lasso in the US cross-section of stock returns. Moreover, the majority of the 68 FC included in our analysis are not considered. Nonetheless, the return process we identify is multi-dimensional, comprising 14 FC. Additionally, our Monte Carlo Simulations indicate that the adaptive Lasso is superior to Lasso and OLS-based selection in panel specifications with a low signal-to-noise ratio. The results are robust to various assumptions. These findings gain support by an empirical out-of-sample factor analysis.},
  citeulike-article-id = {14387297},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2930436},
  groups               = {Characteristics and return prediction},
  posted-at            = {2017-07-03 18:40:02},
  timestamp            = {2020-02-25 23:48},
}

@Article{Feng-et-al-2018b,
  author         = {Feng, Guanhao and Polson, Nick and Xu, Jianeng},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Deep learning factor alpha},
  doi            = {10.2139/ssrn.3243683},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3243683},
  abstract       = {Does a factor model exist to absorb all existing anomalies? We provide a deep learning automated solution to generate long-short factors using a high-dimensional firm characteristic. Sorting securities on firm characteristics is a common practice in finance and a nonlinear activation function built into deep learning. Our algorithm performs a nonlinear search and finds the optimal transformation of characteristics used for security sorting, with one asset pricing objective: minimizing alphas. Our deep factors, hidden neurons in the neural network, are trained greedily with the backward propagation feedback from the loss function that considers both time series and cross-sectional variations. Our conditional forecast generalizes a benchmark, such as CAPM, and includes Fama-French type models as special cases. We have designed a train-validation-test study for monthly U.S. equity returns from 1975 to 2017 and 57 published firm characteristics. In an out-of-sample evaluation, the conditional deep factor model shows a forecasting improvement over the benchmark with factors that offer significant alphas. The conclusion is the improvement of insignificant alphas for some anomalies as well as sorted portfolios.},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Forecast_QWIM, FrcstQWIM_ML, ML_Test_OOS, ML_Test_CrossVal, ML_Validation, FrcstQWIM_ShortTerm, FcstQWIM_Equity, ML_ForcstTimeSrs, ML_AssetPricing},
  timestamp      = {2020-02-25 23:49},
}

@Article{Feng-et-al-2019,
  author               = {Feng, Guanhao and Giglio, Stefano and Xiu, Dacheng},
  date                 = {2019},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Taming the Factor Zoo: A Test of New Factors},
  url                  = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2934020},
  abstract             = {We propose a model selection method to systematically evaluate the contribution to asset pricing of any new factor, above and beyond what a high dimensional set of existing factors explains. Our methodology explicitly accounts for potential model selection mistakes that produce a bias due to the omitted variables, unlike the standard approaches that assume perfect variable selection, which rarely occurs in practice. We apply our procedure to a set of factors recently discovered in the literature. While most of these new factors are found to be redundant relative to the existing factors, a few, such as profitability, have statistically significant explanatory power beyond the hundreds of factors proposed in the past. In addition, we show that our estimates and their significance are stable, whereas the model selected by simple LASSO is not. Finally, we provide additional applications of our procedure that illustrate how it could help control the proliferation of factors in the zoo.},
  citeulike-article-id = {14328023},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2934020},
  groups               = {Factor_Selection, Factor_Test, Stat_Test},
  posted-at            = {2017-04-03 20:33:32},
  timestamp            = {2020-02-25 23:49},
}

@Article{Simonian-et-al-2019,
  author         = {Simonian, Joseph and Wu, Chenwei and Itano, Daniel and Narayanam, Vyshaal},
  date           = {2019-01-31},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {A Machine Learning Approach to Risk Factors: A Case Study Using the Fama-French-Carhart Model},
  url            = {https://jfds.iijournals.com/content/1/1/32},
  urldate        = {2019-02-28},
  day            = {31},
  f1000-projects = {QuantInvest},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-25 23:50},
}

@Article{Brogaard-Zareei-2018,
  author         = {Brogaard, Jonathan and Zareei, Abalfazl},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Machine learning and the stock market},
  doi            = {10.2139/ssrn.3233119},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3233119},
  abstract       = {Recent advances in machine learning methodologies have improved the usefulness of the technology. This paper examines whether machine learning using only past prices as the input can detect mispricings. Generally searching for mispricings is a slow process and can easily suffer from data-snooping. This paper provides a machine learning algorithm to search for mispricings while controlling for data-snooping. The process generates significant out-of-sample alpha. Overall, the results show that mispricings still exist, but have decreased over time, implying that markets have recently become more efficient.},
  f1000-projects = {QuantInvest},
  groups         = {ML_Test_OOS},
  timestamp      = {2020-02-25 23:51},
}

@Article{Li-et-al-2020a,
  author         = {Li, Yimou and Turkington, David and Yazdani, Alireza},
  date           = {2020},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Beyond the Black Box: An Intuitive Approach to Investment Prediction with Machine Learning},
  doi            = {10.2139/ssrn.3475538},
  issn           = {1556-5068},
  url            = {https://jfds.pm-research.com/content/2/1/61},
  urldate        = {2019-11-07},
  abstract       = {The complexity of machine learning models presents a substantial barrier to their adoption for many investors. The algorithms that generate machine learning predictions are sometimes regarded as  box, demanding interpretation and additional explanation. In this paper, we present a framework for demystifying the behavior of machine learning models and decomposing their predictions into linear, nonlinear, and interaction components. We also show how to decompose a model predictive efficacy into these same components. Together, this analysis forms a  fingerprint which we use to summarize its key characteristics and illustrate its similarities and differences compared to other models. We present a case study of this approach applying random forest, gradient boosting machine, and neural network models to the challenge of predicting monthly currency returns. We find that all three models reliably identify intuitive effects in the currency market, and that they also find new relationships attributable to nonlinearities and variable interactions. We argue that an understanding of these predictive components may help astute investors generate superior risk-adjusted returns.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 23:51},
}

@Article{Bryzgalova-et-al-2019,
  author         = {Bryzgalova, Svetlana and Huang, Jiantao and Julliard, Christian},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Bayesian solutions for the factor zoo: we just ran two quadrillion models},
  doi            = {10.2139/ssrn.3481736},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3481736},
  urldate        = {2019-12-26},
  abstract       = {We propose a novel, and simple, Bayesian estimation and model selection procedure for cross-sectional asset pricing. Our approach, that allows for both tradable and non-tradable factors, and is applicable to high dimensional cases, has several desirable properties. First, weak and spurious factors lead to diffuse, and centered at zero, posteriors for their market price of risk, making such factors easily detectable. Second, posterior inference is robust to the presence of such factors. Third, we show that flat priors for risk premia lead to improper marginal likelihoods, rendering model selection invalid. Therefore, we provide a novel prior, that is diffuse for strong factors but shrinks away useless ones, under which posterior probabilities are well behaved, and can be used for factor and (non necessarily nested) model selection, as well as model averaging, in large scale problems. We apply our method to a very large set of factors proposed in the literature, and analyse 2.25 quadrillion possible models, gaining novel insights on the empirical drivers of asset returns.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 23:52},
}

@Article{Bryzgalova-et-al-2019a,
  author         = {Bryzgalova, Svetlana and Pelger, Markus and Zhu, Jason},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Forest Through the Trees: Building Cross-Sections of Stock Returns},
  doi            = {10.2139/ssrn.3493458},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3493458},
  urldate        = {2019-12-25},
  abstract       = {We show how to build a set of basis assets that captures complex information contained in a given list of stock characteristics. Our cross-section of portfolios is a small number of long-only strategies that (a) fully reflect the information in the cross-sectional return predictors, allowing for conditional interactions and non-linearities, (b) provide a small set of interpretable test assets for evaluating asset pricing models, (c) are substantially harder to price than conventional double or triple sorted portfolios constructed from the same information set, and (d) are the building blocks for a stochastic discount factor (SDF) projected on the characteristic space. We use decision trees to generalize the concept of conventional sorting, and develop a novel approach to the robust recovery of a sparse set of the SDF basis assets. Empirically, we show that traditionally sorted portfolios and factors present a too low hurdle for candidate models as they miss the complex information structure of the original returns. Our results have important implications for evaluating asset pricing models, and modeling expected returns.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 23:53},
}

@Article{Choi-et-al-2019b,
  author         = {Choi, Darwin and Jiang, Wenxi and Zhang, Chao},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Alpha go everywhere: machine learning and international stock returns},
  doi            = {10.2139/ssrn.3489679},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3489679},
  urldate        = {2019-12-25},
  abstract       = {We apply machine learning techniques and use stock characteristics to predict the cross-section of stock returns in 33 international markets. We conduct a stringent test to allay concerns about overfitting: the models are trained with past U.S. data and used to predict international stock returns. With fewer variables (based on past returns, size, volume, and accounting information) as inputs, we arrive at a conclusion similar to that in previous studies predicting U.S. stock returns with hundreds of stock characteristics and macroeconomic variables; complex methods outperform linear models in terms of both predicting returns and generating profits. We achieve even stronger results when the models are trained separately for each market, allowing for country-specific return-characteristic relationships. In most markets, we obtain out-of-sample R2 and Sharpe ratios that are comparable to those in previous studies. Neural network models, which can handle complicated interactions among the predictors, produce the best results among various machine learning methods.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 23:53},
}

@Article{Harvey-Liu-2019a,
  author               = {Harvey, Campbell R. and Liu, Yan},
  date                 = {2019},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Lucky Factors},
  url                  = {https://ssrn.com/abstract=2528780},
  abstract             = {We propose a new method to select amongst a large group of candidate factors -- many of which might arise as a result of data mining -- that purport to explain the cross-section of expected returns. The method is robust to general distributional characteristics of both factor and asset returns. We allow for the possibility of time-series as well as cross-sectional dependence. The technique accommodates a wide range of test statistics. Our method can be applied to both asset pricing tests based on portfolio sorts as well as tests using individual asset returns. In contrast to recent asset pricing research, our study of individual stocks finds that the original market factor is by far the most important factor in explaining the cross-section of expected returns.},
  citeulike-article-id = {13926644},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2528780},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2706985code16198.pdf?abstractid=2528780 and mirid=1},
  day                  = {22},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2528780},
  owner                = {zkgst0c},
  posted-at            = {2016-02-06 05:31:25},
  timestamp            = {2020-02-25 23:53},
}

@Article{Kozak-et-al-2019,
  author         = {Kozak, Serhiy and Nagel, Stefan and Santosh, Shrihari},
  date           = {2019-06},
  journaltitle   = {Journal of financial economics},
  title          = {Shrinking the cross-section},
  doi            = {10.1016/j.jfineco.2019.06.008},
  issn           = {0304-405X},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S0304405X19301655}},
  urldate        = {2019-09-07},
  abstract       = {We construct a robust stochastic discount factor (SDF) summarizing the joint explanatory power of a large number of cross-sectional stock return predictors. Our method achieves robust out-of-sample performance in this high-dimensional setting by imposing an economically motivated prior on SDF coefficients that shrinks contributions of low-variance principal components of the candidate characteristics-based factors. We find that characteristics-sparse SDFs formed from a few such factors.g., the four- or five-factor models in the recent literature adequately summarize the cross-section of expected stock returns. However, an SDF formed from a small number of principal components performs well.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 23:54},
}

@Article{Joshi-2019,
  author         = {Joshi, Sudiksha},
  date           = {2019-05-20},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Time Series Analysis and Forecasting of the US Housing Starts using Econometric and Machine Learning Model},
  url            = {https://arxiv.org/abs/1905.07848},
  urldate        = {2019-09-20},
  abstract       = {In this research paper, I have performed time series analysis and forecasted the monthly value of housing starts for the year 2019 using several econometric methods - ARIMA(X), VARX, (G)ARCH and machine learning algorithms - artificial neural networks, ridge regression, K-Nearest Neighbors, and support vector regression, and created an ensemble model. The ensemble model stacks the predictions from various individual models, and gives a weighted average of all predictions. The analyses suggest that the ensemble model has performed the best among all the models as the prediction errors are the lowest, while the econometric models have higher error rates.},
  day            = {20},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Karathanasopoulos-et-al-2017,
  author               = {Karathanasopoulos, Andreas and Mitra, Sovan and Skindilias, Konstantinos and Lo, Chia C.},
  date                 = {2017},
  journaltitle         = {Journal of Forecasting},
  title                = {Modelling and Trading the English and German Stock Markets with Novelty Optimization Techniques},
  doi                  = {10.1002/for.2445},
  abstract             = {The motivation for this paper was the introduction of novel short-term models to trade the FTSE 100 and DAX 30 exchange-traded funds (ETF) indices. There are major contributions in this paper which include the introduction of an input selection criterion when utilizing an expansive universe of inputs, a hybrid combination of partial swarm optimizer (PSO) with radial basis function (RBF) neural networks, the application of a PSO algorithm to a traditional autoregressive moving model (ARMA), the application of a PSO algorithm to a higher-order neural network and, finally, the introduction of a multi-objective algorithm to optimize statistical and trading performance when trading an index. All the machine learning-based methodologies and the conventional models are adapted and optimized to model the index. A PSO algorithm is used to optimize the weights in a traditional RBF neural network, in a higher-order neural network (HONN) and the AR and MA terms of an ARMA model. In terms of checking the statistical and empirical accuracy of the novel models, we benchmark them with a traditional HONN, with an ARMA, with a moving average convergence/divergence model (MACD) and with a naive strategy. More specifically, the trading and statistical performance of all models is investigated in a forecast simulation of the FTSE 100 and DAX 30 ETF time series over the period January 2004 to December 2015 using the last 3 years for out-of-sample testing. Finally, the empirical and statistical results indicate that the PSO-RBF model outperforms all other examined models in terms of trading accuracy and profitability, even with mixed inputs and with only autoregressive inputs.},
  citeulike-article-id = {14270567},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2445},
  groups               = {ML_Forecast_QWIM, FrcstQWIM_Hybrid, ML_NumOptimiz, ML_ForcstTimeSrs, ML_InvestSelect},
  posted-at            = {2017-02-02 19:13:30},
  timestamp            = {2020-02-25 23:57},
}

@Article{Koenecke-Gajewar-2019,
  author         = {Koenecke, Allison and Gajewar, Amita},
  date           = {2019-04-29},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Curriculum Learning in Deep Neural Networks for Financial Forecasting},
  url            = {https://arxiv.org/abs/1904.12887},
  abstract       = {For any financial organization, computing accurate quarterly forecasts for various products is one of the most critical operations. As the granularity at which forecasts are needed increases, traditional statistical time series models may not scale well. We apply deep neural networks in the forecasting domain by experimenting with techniques from Natural Language Processing (Encoder-Decoder LSTMs) and Computer Vision (Dilated CNNs), as well as incorporating transfer learning. A novel contribution of this paper is the application of curriculum learning to neural network models built for time series forecasting. We illustrate the performance of our models using Microsoft's revenue data corresponding to Enterprise, and Small, Medium and Corporate products, spanning approximately 60 regions across the globe for 8 different business segments, and totaling in the order of tens of billions of USD. We compare our models' performance to the ensemble model of traditional statistics and machine learning techniques currently used by Microsoft Finance. With this in-production model as a baseline, our experiments yield an approximately 30\% improvement in overall accuracy on test data. We find that our curriculum learning LSTM-based model performs best, showing that it is reasonable to implement our proposed methods without overfitting on medium-sized data.},
  day            = {29},
  f1000-projects = {QuantInvest},
  groups         = {ML_TransferLrng, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Kuznetsov-Mariet-2018,
  author         = {Kuznetsov, Vitaly and Mariet, Zelda},
  date           = {2018-05-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Foundations of Sequence-to-Sequence Modeling for Time Series},
  url            = {https://arxiv.org/abs/1805.03714},
  abstract       = {The availability of large amounts of time series data, paired with the performance of deep-learning algorithms on a broad class of problems, has recently led to significant interest in the use of sequence-to-sequence models for time series forecasting. We provide the first theoretical analysis of this time series forecasting framework. We include a comparison of sequence-to-sequence modeling to classical time series models, and as such our theory can serve as a quantitative guide for practitioners choosing between different modeling methodologies.},
  day            = {9},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@InCollection{Kuznetsov-Mohri-2016,
  author               = {Kuznetsov, Vitaly and Mohri, Mehryar},
  booktitle            = {29th Annual Conference on Learning Theory},
  date                 = {2016-06},
  title                = {Time series prediction and online learning},
  editor               = {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
  location             = {Columbia University, New York, New York, USA},
  pages                = {1190--1213},
  publisher            = {PMLR},
  series               = {Proceedings of Machine Learning Research},
  url                  = {http://proceedings.mlr.press/v49/kuznetsov16.html},
  volume               = {49},
  abstract             = {We present a series of theoretical and algorithmic results combining the benefits of the statistical learning approach to time series prediction with that of on-line learning. We prove new generalization guarantees for hypotheses derived from regret minimization algorithms in the general scenario where the data is generated by a non-stationary non-mixing stochastic process. Our theory enables us to derive model selection techniques with favorable theoretical guarantees in the scenario of time series, thereby solving a problem that is notoriously difficult in that scenario. It also helps us devise new ensemble methods with favorable theoretical guarantees for the task of forecasting non-stationary time series.},
  citeulike-article-id = {14506872},
  citeulike-linkout-0  = {http://proceedings.mlr.press/v49/kuznetsov16.html},
  groups               = {ML_ForcstTimeSrs},
  keywords             = {*file-import-17-12-21},
  posted-at            = {2017-12-21 21:50:55},
  timestamp            = {2020-02-25 23:57},
}

@Article{Lakshmi-Visalakshmi-2016,
  author               = {Lakshmi, P. and Visalakshmi, S.},
  date                 = {2016},
  journaltitle         = {International Journal of Mathematics in Operational Research},
  title                = {Exploring the usage of econometric techniques in nonlinear machine learning and data mining},
  doi                  = {10.1504/ijmor.2016.10000130},
  issn                 = {1757-5850},
  number               = {3},
  pages                = {349+},
  volume               = {9},
  abstract             = {The present study, investigates the inter-linkage of the Indian spot market with other global markets and the predictability of S and P CNX NIFTY Index returns with a set of five new international market returns as input variables in artificial neural networks (ANNs). Identifying the right set of exogenous input variables using conventional techniques like OLS, Granger causality and cross correlation substantially increased the predictability of financial time series like stock return in the Indian context. The performance of the ANN model in forecasting NIFTY index returns is evaluated by comparing it for different sample periods in terms of forecasting error functions with statistical measures like mean absolute error, root mean square error, mean absolute percentage error and mean square error. The findings suggest that higher accuracy of the predictive power of neural network is largely influenced by the input variables.},
  citeulike-article-id = {14166952},
  citeulike-linkout-0  = {http://dx.doi.org/10.1504/ijmor.2016.10000130},
  groups               = {ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-10-17 16:34:42},
  timestamp            = {2020-02-25 23:57},
}

@Article{Lim-et-al-2019,
  author         = {Lim, Bryan and Zohren, Stefan and Roberts, Stephen},
  date           = {2019-01-23},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Recurrent Neural Filters: Learning Independent Bayesian Filtering Steps for Time Series Prediction},
  url            = {https://arxiv.org/abs/1901.08096},
  abstract       = {Despite the recent popularity of deep generative state space models, few comparisons have been made between network architectures and the inference steps of the Bayesian filtering framework -- with most models simultaneously approximating both state transition and update steps with a single recurrent neural network (RNN). In this paper, we introduce the Recurrent Neural Filter (RNF), a novel recurrent variational autoencoder architecture that learns distinct representations for each Bayesian filtering step, captured by a series of encoders and decoders. Testing this on three real-world time series datasets, we demonstrate that decoupling representations not only improves the accuracy of one-step-ahead forecasts while providing realistic uncertainty estimates, but also facilitates multistep prediction through the separation of encoder stages.},
  day            = {23},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@InProceedings{Li-Ngan-2019,
  author         = {Li, Lin and Ngan, Chun-Kit},
  booktitle      = {Proceedings of the 2019 3rd International Conference on Information System and Data Mining - ICISDM 2019},
  date           = {2019-04-06},
  title          = {A Weight-adjusting Approach on an Ensemble of Classifiers for Time Series Forecasting},
  doi            = {10.1145/3325917.3325920},
  isbn           = {9781450366359},
  location       = {New York, New York, USA},
  pages          = {65--69},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3325917.3325920},
  urldate        = {2019-09-22},
  abstract       = {In this study, we present a hybrid heterogeneous forecasting model that combines autoregressive integrated moving average (ARIMA) model and two machine learning classifiers (i.e. support vector machine (SVM) and artificial neural network (ANN)) for time series forecasting. The approach adjusts each model's weight based on their ability and history of predicting numerical values. A weighted numerical value based on each model's numerical output and their weight is calculated as the final output. An air quality dataset is used to evaluate our approach. We conduct the experimental comparison among our proposed weight-adjusting approach on the heterogeneous forecasting model, each individual model in the ensemble, and a hybrid homogenous forecasting model (e.g. random forest). It turns out that our proposed approach has a better performance than each single model in the ensemble and the hybrid homogeneous forecasting model in terms of mean absolute error (MAE) and mean absolute percentage error (MAPE).},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Loning-et-al-2019,
  author         = {Loning, Markus and Bagnall, Anthony and Ganesh, Sajaysurya and Kazakov, Viktor and Lines, Jason and Kiraly, Franz J.},
  date           = {2019-09-17},
  journaltitle   = {arXiv Electronic Journal},
  title          = {sktime: A Unified Interface for Machine Learning with Time Series},
  url            = {https://arxiv.org/abs/1909.07872},
  urldate        = {2019-09-21},
  abstract       = {We present sktime -- a new scikit-learn compatible Python library with a unified interface for machine learning with time series. Time series data gives rise to various distinct but closely related learning tasks, such as forecasting and time series classification, many of which can be solved by reducing them to related simpler tasks. We discuss the main rationale for creating a unified interface, including reduction, as well as the design of sktime's core API, supported by a clear overview of common time series tasks and reduction approaches.},
  day            = {17},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Maheu-Song-2017,
  author               = {Maheu, John M. and Song, Yong},
  date                 = {2017-10-06},
  journaltitle         = {Journal of Applied Econometrics},
  title                = {An efficient Bayesian approach to multiple structural change in multivariate time series},
  doi                  = {10.1002/jae.2606},
  issn                 = {0883-7252},
  abstract             = {This paper provides a feasible approach to estimation and forecasting of multiple structural breaks for vector autoregressions and other multivariate models. Owing to conjugate prior assumptions we obtain a very efficient sampler for the regime allocation variable. A new hierarchical prior is introduced to allow for learning over different structural breaks. The model is extended to independent breaks in regression coefficients and the volatility parameters. Two empirical applications show the improvements the model has over benchmarks. In a macro application with seven variables we empirically demonstrate the benefits from moving from a multivariate structural break model to a set of univariate structural break models to account for heterogeneous break patterns across data series.},
  citeulike-article-id = {14503528},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/jae.2606},
  day                  = {06},
  groups               = {ML_ForcstTimeSrs},
  posted-at            = {2017-12-15 17:44:50},
  timestamp            = {2020-02-25 23:57},
}

@Article{Makridakis-et-al-2019,
  author         = {Makridakis, Spyros and Hyndman, Rob J. and Petropoulos, Fotios},
  date           = {2019-09},
  journaltitle   = {International journal of forecasting},
  title          = {Forecasting in social settings: The state of the art},
  doi            = {10.1016/j.ijforecast.2019.05.011},
  issn           = {0169-2070},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301876},
  urldate        = {2019-10-02},
  abstract       = {This paper provides a non-systematic review of the progress of forecasting in social settings. It is aimed at someone outside the field of forecasting who wants to understand and appreciate the results of the M4 Competition, and forms a survey paper regarding the state of the art of this discipline. It discusses the recorded improvements in forecast accuracy over time, the need to capture forecast uncertainty, and things that can go wrong with predictions. Subsequently, the review classifies the knowledge achieved over recent years into (i) what we know, (ii) what we are not sure about, and (iii) what we don knowIn the first two areas, we explore the difference between explanation and prediction, the existence of an optimal model, the performance of machine learning methods on time series forecasting tasks, the difficulties of predicting non-stable environments, the performance of judgment, and the value added by exogenous variables. The article concludes with the importance of (thin and) fat tails, the challenges and advances in causal inference, and the role of luck.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Makridakis-et-al-2019a,
  author         = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  date           = {2019-07},
  journaltitle   = {International Journal of Forecasting},
  title          = {The M4 Competition: 100,000 time series and 61 forecasting methods},
  doi            = {10.1016/j.ijforecast.2019.04.014},
  issn           = {0169-2070},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301128},
  urldate        = {2019-09-01},
  abstract       = {The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@InCollection{McDonald-et-al-2014,
  author               = {McDonald, Scott and Coleman, Sonya and McGinnity, T. M. and Li, Yuhua and Belatreche, Ammar},
  booktitle            = {IEEE Conference on Computational Intelligence for Financial Engineering and Economics (CIFEr)},
  date                 = {2014-03},
  title                = {A comparison of forecasting approaches for capital markets},
  doi                  = {10.1109/cifer.2014.6924051},
  isbn                 = {978-1-4799-2380-9},
  location             = {London, UK},
  pages                = {32--39},
  publisher            = {IEEE},
  abstract             = {In recent years, machine learning algorithms have become increasingly popular in financial forecasting. Their flexible, data-driven nature makes them ideal candidates for dealing with complex financial data. This paper investigates the effectiveness of a number of machine learning algorithms, and combinations of these algorithms, at generating one-step ahead forecasts of a number of financial time series. We find that hybrid models consisting of a linear statistical model and a nonlinear machine learning algorithm are effective at forecasting future values of the series, particularly in terms of the future direction of the series.},
  citeulike-article-id = {14310022},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/cifer.2014.6924051},
  groups               = {Forecast_Hybrid, FrcstQWIM_Hybrid, ML_ForcstTimeSrs},
  posted-at            = {2017-03-13 03:34:33},
  timestamp            = {2020-02-25 23:57},
}

@Article{Nguyen-et-al-2019b,
  author         = {Nguyen, Nghia and Tran, Minh-Ngoc and Gunawan, David and Kohn, R.},
  date           = {2019-06-07},
  journaltitle   = {arXiv Electronic Journal},
  title          = {A long short-term memory stochastic volatility model},
  url            = {https://arxiv.org/abs/1906.02884},
  urldate        = {2019-10-11},
  abstract       = {Stochastic Volatility (SV) models are widely used in the financial sector while Long Short-Term Memory (LSTM) models are successfully used in many large-scale industrial applications of Deep Learning. Our article combines these two methods in a non-trivial way and proposes a model, which we call the LSTM-SV model, to capture the dynamics of stochastic volatility. The proposed model overcomes the short-term memory problem in conventional SV models, is able to capture non-linear dependence in the latent volatility process, and often has a better out-of-sample forecast performance than SV models. These properties are illustrated through simulation study and applications to three financial time series datasets: The US stock market weekly index SP500, the Australian stock weekly index ASX200 and the Australian-US dollar daily exchange rates. Based on our analysis, we argue that there are significant differences in the underlying dynamics between the volatility process of the SP500 and ASX200 datasets and that of the exchange rate dataset. For the stock index data, there is strong evidence of long-term memory and non-linear dependence in the volatility process, while this is not the case for the exchange rates. An user-friendly software package together with the examples reported in the paper are available at this https URL.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Nicholson-et-al-2017,
  author               = {Nicholson, William and Matteson, David and Bien, Jacob},
  date                 = {2017-02},
  journaltitle         = {arXiv Electronic Journal},
  title                = {BigVAR: Tools for Modeling Sparse High-Dimensional Multivariate Time Series},
  eprint               = {1702.07094},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1702.07094},
  abstract             = {The R package BigVAR allows for the simultaneous estimation of high-dimensional time series by applying structured penalties to the conventional vector autoregression (VAR) and vector autoregression with exogenous variables (VARX) frameworks. Our methods can be utilized in many forecasting applications that make use of time-dependent data such as macroeconomics, finance, and internet traffic. Our package extends solution algorithms from the machine learning and signal processing literatures to a time dependent setting: selecting the regularization parameter by sequential cross validation and provides substantial improvements in forecasting performance over conventional methods. We offer a user-friendly interface that utilizes R's s4 object class structure which makes our methodology easily accessible to practicioners.

In this paper, we present an overview of our notation, the models that comprise BigVAR, and the functionality of our package with a detailed example using publicly available macroeconomic data. In addition, we present a simulation study comparing the performance of several procedures that refit the support selected by a BigVAR procedure according to several variants of least squares and conclude that refitting generally degrades forecast performance.},
  citeulike-article-id = {14403723},
  citeulike-linkout-0  = {http://arxiv.org/abs/1702.07094},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1702.07094},
  day                  = {23},
  groups               = {ML_Test_CrossVal, ML_Validation, ML_ForcstTimeSrs},
  posted-at            = {2017-07-31 17:35:05},
  timestamp            = {2020-02-25 23:57},
}

@Article{Okuno-et-al-2019,
  author         = {Okuno, Shunya and Aihara, Kazuyuki and Hirata, Yoshito},
  date           = {2019-03},
  journaltitle   = {Chaos},
  title          = {Combining multiple forecasts for multivariate time series via state-dependent weighting.},
  doi            = {10.1063/1.5057379},
  number         = {3},
  pages          = {033128},
  urldate        = {2019-09-22},
  volume         = {29},
  abstract       = {We present a model-free forecast algorithm that dynamically combines multiple forecasts using multivariate time series data. The underlying principle is based on the fact that forecast performance depends on the position in the state space. This property is exploited to weight multiple forecasts via a local loss function. Specifically, additional weights are assigned to appropriate forecasts depending on their positions in a state space reconstructed via delay coordinates. The function evaluates the forecast error discounted by the distance in the space, whereas most existing methods discount the error in relation to time. In addition, forecasts are selected with the function for each time step based on the existing multiview embedding approach; by contrast, the original multiview embedding selects the reconstructions in advance before starting the forecast. The proposed prediction method has the smallest mean squared error among conventional ensemble methods for the Rossler and the Lorenz 96I models. The results of comparison of the proposed method with conventional machine-learning methods using a flood forecast example indicate that the proposed method yields superior accuracy. We also demonstrate that the proposed method might even correctly forecast the maximum water level of rivers without any prior knowledge.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  pmid           = {30927862},
  timestamp      = {2020-02-25 23:57},
}

@Article{Oreshkin-et-al-2019,
  author         = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  date           = {2019-05-24},
  journaltitle   = {arXiv Electronic Journal},
  title          = {N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},
  url            = {https://arxiv.org/abs/1905.10437},
  urldate        = {2019-08-24},
  abstract       = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on the well-known M4 competition dataset containing 100k time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on the M4 dataset strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without loss in accuracy.},
  day            = {24},
  f1000-projects = {QuantInvest},
  groups         = {ML_Interpretability, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Orozco-et-al-2018,
  author         = {Orozco, Bernardo Perez and Abbati, Gabriele and Roberts, Stephen},
  date           = {2018-03-26},
  journaltitle   = {arXiv Electronic Journal},
  title          = {MOrdReD: Memory-based Ordinal Regression Deep Neural Networks for Time Series Forecasting},
  url            = {https://arxiv.org/abs/1803.09704},
  abstract       = {Time series forecasting is ubiquitous in the modern world. Applications range from health care to astronomy, and include climate modelling, financial trading and monitoring of critical engineering equipment. To offer value over this range of activities, models must not only provide accurate forecasts, but also quantify and adjust their uncertainty over time. In this work, we directly tackle this task with a novel, fully end-to-end deep learning method for time series forecasting. By recasting time series forecasting as an ordinal regression task, we develop a principled methodology to assess long-term predictive uncertainty and describe rich multimodal, non-Gaussian behaviour, which arises regularly in applied settings. Notably, our framework is a wholly general-purpose approach that requires little to no user intervention to be used. We showcase this key feature in a large-scale benchmark test with 45 datasets drawn from both, a wide range of real-world application domains, as well as a comprehensive list of synthetic maps. This wide comparison encompasses state-of-the-art methods in both the Machine Learning and Statistics modelling literature, such as the Gaussian Process. We find that our approach does not only provide excellent predictive forecasts, shadowing true future values, but also allows us to infer valuable information, such as the predictive distribution of the occurrence of critical events of interest, accurately and reliably even over long time horizons.},
  day            = {26},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Abrevaya-et-al-2018,
  author         = {Abrevaya, German and Aravkin, Aleksandr and Cecchi, Guillermo and Rish, Irina and Polosecki, Pablo and Zheng, Peng and Dawson, Silvina},
  date           = {2018-05-25},
  journaltitle   = {BioRxiv Electronic Journal},
  title          = {Learning nonlinear brain dynamics: Van der Pol meets LSTM},
  doi            = {10.1101/330548},
  urldate        = {2019-04-10},
  abstract       = {Many real-world data sets, especially in biology, are produced by highly multivariate and nonlinear complex dynamical systems. In this paper, we focus on brain imaging data, including both calcium imaging and functional MRI data. Standard vector-autoregressive models are limited by their linearity assumptions, while nonlinear general-purpose, large-scale temporal models, such as LSTM networks, typically require large amounts of training data, not always readily available in biological applications; furthermore, such models have limited interpretability. We introduce here a novel approach for learning a nonlinear differential equation model aimed at capturing brain dynamics. Specifically, we propose a variable-projection optimization approach to estimate the parameters of the multivariate (coupled) van der Pol oscillator, and demonstrate that such a model can accurately represent nonlinear dynamics of the brain data. Furthermore, in order to improve the predictive accuracy when forecasting future brain-activity time series, we use this analytical model as an unlimited source of simulated data for pretraining LSTM; such model-specific data augmentation approach consistently improves LSTM performance on both calcium and fMRI imaging data.},
  day            = {25},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Alexandrov-et-al-2019,
  author         = {Alexandrov, Alexander and Benidis, Konstantinos and Bohlke-Schneider, Michael and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Maddix, Danielle C. and Rangapuram, Syama and Salinas, David and Schulz, Jasper and Stella, Lorenzo and Turkmen, Ali Caner and Wang, Yuyang},
  date           = {2019-06-12},
  journaltitle   = {arXiv Electronic Journal},
  title          = {GluonTS: Probabilistic Time Series Models in Python},
  url            = {https://arxiv.org/abs/1906.05264},
  urldate        = {2019-09-28},
  abstract       = {We introduce Gluon Time Series (GluonTS, available at this https URL), a library for deep-learning-based time series modeling. GluonTS simplifies the development of and experimentation with time series models for common tasks such as forecasting or anomaly detection. It provides all necessary components and tools that scientists need for quickly building new models, for efficiently running and analyzing experiments and for evaluating model accuracy.},
  day            = {12},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Ali-et-al-2018,
  author         = {Ali, Abbas Raza and Gabrys, Bogdan and Budka, Marcin},
  date           = {2018},
  journaltitle   = {Procedia Computer Science},
  title          = {Cross-domain Meta-learning for Time-series Forecasting},
  doi            = {10.1016/j.procs.2018.07.204},
  issn           = {1877-0509},
  pages          = {9--18},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918311785},
  urldate        = {2019-09-11},
  volume         = {126},
  abstract       = {There are many algorithms that can be used for the time-series forecasting problem, ranging from simple (e.g. Moving Average) to sophisticated Machine Learning approaches (e.g. Neural Networks). Most of these algorithms require a number of user-defined parameters to be specified, leading to exponential explosion of the space of potential solutions. Since the trial-and-error approach to finding a good algorithm for solving a given problem is typically intractable, researchers and practitioners need to resort to a more intelligent search strategy, with one option being to constraint the search space using past experience - an approach known as Meta-learning. Although potentially attractive, Meta-learning comes with its own challenges. Gathering a sufficient number of Meta-examples, which in turn requires collecting and processing multiple datasets from each problem domain under consideration is perhaps the most prominent issue. In this paper, we are investigating the situations in which the use of additional data can improve performance of a Meta-learning system, with focus on cross-domain transfer of Meta-knowledge. A similarity-based cluster analysis of Meta-features has also been performed in an attempt to discover homogeneous groups of time-series with respect to Meta-learning performance. Although the experiments revealed limited room for improvement over the overall best base-learner, the Meta-learning approach turned out to be a safe choice, minimizing the risk of selecting the least appropriate base-learner.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs, ML_ClustTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@InCollection{Allende-Valle-2017,
  author         = {Allende, Hector and Valle, Carlos},
  booktitle      = {Claudio Moraga: A Passion for Multi-Valued Logic and Soft Computing},
  date           = {2017},
  title          = {Ensemble methods for time series forecasting},
  doi            = {10.1007/978-3-319-48317-7\_13},
  editor         = {Seising, Rudolf and Allende-Cid, Hector},
  isbn           = {978-3-319-48316-0},
  pages          = {217--232},
  publisher      = {Springer International Publishing},
  series         = {Studies in fuzziness and soft computing},
  url            = {http://link.springer.com/10.1007/978-3-319-48317-7\_13},
  urldate        = {2019-09-22},
  volume         = {349},
  abstract       = {Improvement of time series forecasting accuracy is an active research area that has significant importance in many practical domains. Ensemble methods have gained considerable attention from machine learning and soft computing communities in recent years. There are several practical and theoretical reasons, mainly statistical reasons, why an ensemble may be preferred. Ensembles are recognized as one of the most successful approaches to prediction tasks. Previous theoretical studies of ensembles have shown that one of the key reasons for this performance is diversity among ensemble members. Several methods exist to generate diversity. Extensive works in literature suggest that substantial improvements in accuracy can be achieved by combining forecasts from different models. The focus of this chapter will be on ensemble for time series prediction. We describe the use of ensemble methods to compare different models for time series prediction and extensions to the classical ensemble methods for neural networks for classification and regression prediction by using different model architectures. Design, implementation and application will be the main topics of the chapter, and more specifically: conditions under which ensemble based systems may be more beneficial than their single machine; algorithms for generating individual components of ensemble systems; and various procedures through which they can be combined. Various ensemble based algorithms will be analyzed: Bagging, Adaboost and Negative Correlation; as well as combination rules and decision templates. Finally, future directions will be time series forecasting, machine fusion and others areas in which ensemble of machines have shown great promise.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  issn           = {1434-9922},
  timestamp      = {2020-02-25 23:57},
}

@Article{Bandara-et-al-2019a,
  author         = {Bandara, Kasun and Shi, Peibei and Bergmeir, Christoph and Hewamalage, Hansika and Tran, Quoc and Seaman, Brian},
  date           = {2019-01-13},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Sales Demand Forecast in E-commerce using a Long Short-Term Memory Neural Network Methodology},
  url            = {https://arxiv.org/abs/1901.04028},
  urldate        = {2019-09-28},
  abstract       = {Generating accurate and reliable sales forecasts is crucial in the E-commerce business. The current state-of-the-art techniques are typically univariate methods, which produce forecasts considering only the historical sales data of a single product. However, in a situation where large quantities of related time series are available, conditioning the forecast of an individual time series on past behaviour of similar, related time series can be beneficial. Since the product assortment hierarchy in an E-commerce platform contains large numbers of related products, in which the sales demand patterns can be correlated, our attempt is to incorporate this cross-series information in a unified model. We achieve this by globally training a Long Short-Term Memory network (LSTM) that exploits the non-linear demand relationships available in an E-commerce product assortment hierarchy. Aside from the forecasting framework, we also propose a systematic pre-processing framework to overcome the challenges in the E-commerce business. We also introduce several product grouping strategies to supplement the LSTM learning schemes, in situations where sales patterns in a product portfolio are disparate. We empirically evaluate the proposed forecasting framework on a real-world online marketplace dataset from this http URL. Our method achieves competitive results on category level and super-departmental level datasets, outperforming state-of-the-art techniques.},
  day            = {13},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Bandara-et-al-2019b,
  author         = {Bandara, Kasun and Bergmeir, Christoph and Hewamalage, Hansika},
  date           = {2019-09-10},
  journaltitle   = {arXiv Electronic Journal},
  title          = {LSTM-MSNet: Leveraging Forecasts on Sets of Related Time Series with Multiple Seasonal Patterns},
  url            = {https://arxiv.org/abs/1909.04293},
  urldate        = {2019-10-11},
  abstract       = {Generating forecasts for time series with multiple seasonal cycles is an important use-case for many industries nowadays. Accounting for the multi-seasonal patterns becomes necessary to generate more accurate and meaningful forecasts in these contexts. In this paper, we propose Long Short-Term Memory Multi-Seasonal Net (LSTM-MSNet), a decompositionbased, unified prediction framework to forecast time series with multiple seasonal patterns. The current state of the art in this space are typically univariate methods, in which the model parameters of each time series are estimated independently. Consequently, these models are unable to include key patterns and structures that may be shared by a collection of time series. In contrast, LSTM-MSNet is a globally trained Long Short-Term Memory network (LSTM), where a single prediction model is built across all the available time series to exploit the crossseries knowledge in a group of related time series. Furthermore, our methodology combines a series of state-of-the-art multiseasonal decomposition techniques to supplement the LSTM learning procedure. In our experiments, we are able to show that on datasets from disparate data sources, like e.g. the popular M4 forecasting competition, a decomposition step is beneficial, whereas in the common real-world situation of homogeneous series from a single application, exogenous seasonal variables or no seasonal preprocessing at all are better choices. All options are readily included in the framework and allow us to achieve competitive results for both cases, outperforming many state-ofthe-art multi-seasonal forecasting methods},
  day            = {10},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Barak-et-al-2019,
  author         = {Barak, Sasan and Nasiri, Mahdi and Rostamzadeh, Mehrdad},
  date           = {2019-08-22},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Time series model selection with a meta-learning approach; evidence from a pool of forecasting algorithms},
  url            = {https://arxiv.org/abs/1908.08489},
  urldate        = {2019-09-06},
  abstract       = {One of the challenging questions in time series forecasting is how to find the best algorithm. In recent years, a recommender system scheme has been developed for time series analysis using a meta-learning approach. This system selects the best forecasting method with consideration of the time series characteristics. In this paper, we propose a novel approach to focusing on some of the unanswered questions resulting from the use of meta-learning in time series forecasting. Therefore, three main gaps in previous works are addressed including, analyzing various subsets of top forecasters as inputs for meta-learners; evaluating the effect of forecasting error measures; and assessing the role of the dimensionality of the feature space on the forecasting errors of meta-learners. All of these objectives are achieved with the help of a diverse state-of-the-art pool of forecasters and meta-learners. For this purpose, first, a pool of forecasting algorithms is implemented on the NN5 competition dataset and ranked based on the two error measures. Then, six machine-learning classifiers known as meta-learners, are trained on the extracted features of the time series in order to assign the most suitable forecasting method for the various subsets of the pool of forecasters. Furthermore, two-dimensionality reduction methods are implemented in order to investigate the role of feature space dimension on the performance of meta-learners. In general, it was found that meta-learners were able to defeat all of the individual benchmark forecasters; this performance was improved even after applying the feature selection method.},
  day            = {22},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Barrow-Crone-2016a,
  author               = {Barrow, Devon K. and Crone, Sven F.},
  date                 = {2016-10},
  journaltitle         = {International Journal of Forecasting},
  title                = {A comparison of AdaBoost algorithms for time series forecast combination},
  doi                  = {10.1016/j.ijforecast.2016.01.006},
  issn                 = {0169-2070},
  number               = {4},
  pages                = {1103--1119},
  volume               = {32},
  abstract             = {Recently, combination algorithms from machine learning classification have been extended to time series regression, most notably seven variants of the popular AdaBoost algorithm. Despite their theoretical promise their empirical accuracy in forecasting has not yet been assessed, either against each other or against any established approaches of forecast combination, model selection, or statistical benchmark algorithms. Also, none of the algorithms have been assessed on a representative set of empirical data, using only few synthetic time series. We remedy this omission by conducting a rigorous empirical evaluation using a representative set of 111 industry time series and a valid and reliable experimental design. We develop a full-factorial design over derived Boosting meta-parameters, creating 42 novel Boosting variants, and create a further 47 novel Boosting variants using research insights from forecast combination. Experiments show that only few Boosting meta-parameters increase accuracy, while meta-parameters derived from forecast combination research outperform others.},
  citeulike-article-id = {14071839},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2016.01.006},
  groups               = {ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-06-18 18:55:19},
  timestamp            = {2020-02-25 23:57},
}

@Article{BenTaieb-et-al-2016,
  author               = {Ben Taieb, Souhaib and Atiya, Amir F.},
  date                 = {2016-01},
  journaltitle         = {IEEE Transactions on Neural Networks and Learning Systems},
  title                = {A Bias and Variance Analysis for Multistep-Ahead Time Series Forecasting},
  doi                  = {10.1109/tnnls.2015.2411629},
  issn                 = {2162-237X},
  number               = {1},
  pages                = {62--76},
  volume               = {27},
  abstract             = {Multistep-ahead forecasts can either be produced recursively by iterating a one-step-ahead time series model or directly by estimating a separate model for each forecast horizon. In addition, there are other strategies; some of them combine aspects of both aforementioned concepts. In this paper, we present a comprehensive investigation into the bias and variance behavior of multistep-ahead forecasting strategies. We provide a detailed review of the different multistep-ahead strategies. Subsequently, we perform a theoretical study that derives the bias and variance for a number of forecasting strategies. Finally, we conduct a Monte Carlo experimental study that compares and evaluates the bias and variance performance of the different strategies. From the theoretical and the simulation studies, we analyze the effect of different factors, such as the forecast horizon and the time series length, on the bias and variance components, and on the different multistep-ahead strategies. Several lessons are learned, and recommendations are given concerning the advantages, disadvantages, and best conditions of use of each strategy.},
  citeulike-article-id = {14072244},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tnnls.2015.2411629},
  citeulike-linkout-1  = {http://ieeexplore.ieee.org/xpls/absall.jsp?arnumber=7064712},
  groups               = {ML_ForcstTimeSrs, [nbkcbu3:]},
  institution          = {Dept. of Comput. Sci., Univ. Libre de Bruxelles, Brussels, Belgium},
  owner                = {cristi},
  posted-at            = {2016-06-20 02:35:57},
  publisher            = {IEEE},
  timestamp            = {2020-02-25 23:57},
}

@Article{Bergmeir-Benitez-2012,
  author               = {Bergmeir, Christoph and Bentez, Jose M.},
  date                 = {2012-05},
  journaltitle         = {Information Sciences},
  title                = {On the use of cross-validation for time series predictor evaluation},
  doi                  = {10.1016/j.ins.2011.12.028},
  issn                 = {0020-0255},
  pages                = {192--213},
  volume               = {191},
  abstract             = {In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand. In traditional forecasting, it is common practice to reserve a part from the end of each time series for testing, and to use the rest of the series for training. Thus it is not made full use of the data, but theoretical problems with respect to temporal evolutionary effects and dependencies within the data as well as practical problems regarding missing values are eliminated. On the other hand, when evaluating machine learning and other regression methods used for time series forecasting, often cross-validation is used for evaluation, paying little attention to the fact that those theoretical problems invalidate the fundamental assumptions of cross-validation. To close this gap and examine the consequences of different model selection procedures in practice, we have developed a rigorous and extensive empirical study. Six different model selection procedures, based on (i) cross-validation and (ii) evaluation using the series' last part, are used to assess the performance of four machine learning and other regression techniques on synthetic and real-world time series. No practical consequences of the theoretical flaws were found during our study, but the use of cross-validation techniques led to a more robust model selection. To make use of the - best of both worlds-, we suggest that the use of a blocked form of cross-validation for time series evaluation became the standard procedure, thus using all available information and circumventing the theoretical problems.},
  citeulike-article-id = {10220267},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ins.2011.12.028},
  groups               = {ML_Test_CrossVal, ML_Validation, ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-06-20 02:40:40},
  timestamp            = {2020-02-25 23:57},
}

@TechReport{Bergmeir-et-al-2015,
  author               = {Bergmeir, Christoph and Hyndman, Rob J. and Koo, Bonsoo},
  date                 = {2015},
  institution          = {Monash University},
  title                = {A Note on the Validity of Cross-Validation for Evaluating Time Series Prediction},
  url                  = {https://www.sciencedirect.com/science/article/pii/S0167947317302384},
  abstract             = {One of the most widely used standard procedures for model evaluation in classification and regression is K-fold cross-validation (CV). However, when it comes to time series forecasting, because of the inherent serial correlation and potential non-stationarity of the data, its application is not straightforward and often omitted by practitioners in favor of an out-of-sample (OOS) evaluation. In this paper, we show that the particular setup in which time series forecasting is usually performed using Machine Learning methods renders the use of standard K-fold CV possible. We present theoretical insights supporting our arguments. Furthermore, we present a simulation study where we show empirically that K-fold CV performs favorably compared to both OOS evaluation and other time-series-specific techniques such as non-dependent cross-validation.},
  citeulike-article-id = {14147217},
  groups               = {ML_Test_OOS, ML_Test_CrossVal, ML_Validation, ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-09-27 15:26:28},
  timestamp            = {2020-02-25 23:57},
}

@Article{Bhanja-Das-2018,
  author         = {Bhanja, Samit and Das, Abhishek},
  date           = {2018-12-13},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Impact of Data Normalization on Deep Neural Network for Time Series Forecasting},
  url            = {https:://arxiv.org/abs/1812.05519},
  urldate        = {2019-03-07},
  abstract       = {For the last few years it has been observed that the Deep Neural Networks (DNNs) has achieved an excellent success in image classification, speech recognition. But DNNs are suffer great deal of challenges for time series forecasting because most of the time series data are nonlinear in nature and highly dynamic in behaviour. The time series forecasting has a great impact on our socio-economic environment. Hence, to deal with these challenges its need to be redefined the DNN model and keeping this in mind, data pre-processing, network architecture and network parameters are need to be consider before feeding the data into DNN models. Data normalization is the basic data pre-processing technique form which learning is to be done. The effectiveness of time series forecasting is heavily depend on the data normalization technique. In this paper, different normalization methods are used on time series data before feeding the data into the DNN model and we try to find out the impact of each normalization technique on DNN to forecast the time series. Here the Deep Recurrent Neural Network (DRNN) is used to predict the closing index of Bombay Stock Exchange (BSE) and New York Stock Exchange (NYSE) by using BSE and NYSE time series data.},
  day            = {13},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Brissimis-Migiakis-2016,
  author         = {Brissimis, Sophocles N. and Migiakis, Petros M.},
  date           = {2016-11},
  journaltitle   = {Empirical economics},
  title          = {Inflation persistence, learning dynamics and the rationality of inflation expectations},
  doi            = {10.1007/s00181-015-1033-9},
  issn           = {0377-7332},
  number         = {3},
  pages          = {963--979},
  volume         = {51},
  abstract       = {Abstract The rational expectations hypothesis for survey and model-based inflation forecasts the Survey of Professional Forecasters and the Greenbook respectively examined by properly taking into account the persistence characteristics of the data. The finding of near-unit-root effects in the inflation and inflation expectations series motivates the use of a local-to-unity specification of the inflation process that enables us to test whether the data are generated by locally non-stationary or stationary processes. Thus, we test, rather than assume, stationarity of near-unit-root processes. The paper combines the concept of localities in the underlying time series, such as those that may exist in the sample but not in the population, with cointegration analysis which permits the distinction between short-run and long-run structures. Thus, we examine possible in-sample departures from rationality both in the short run and the long run. Our empirical results indicate that the rational expectations hypothesis holds in the long run, while forecasters adjust their expectations slowly in the short run. This finding lends support to the hypothesis that the persistence of inflation comes from the dynamics of expectations.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_Inflation, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@InCollection{Buda-et-al-2018,
  author         = {Buda, Teodora Sandra and Caglayan, Bora and Assem, Haytham},
  booktitle      = {Advances in knowledge discovery and data mining},
  date           = {2018},
  title          = {DeepAD: A generic framework based on deep learning for time series anomaly detection},
  doi            = {10.1007/978-3-319-93034-3\_46},
  editor         = {Phung, Dinh and Tseng, Vincent S. and Webb, Geoffrey I. and Ho, Bao and Ganji, Mohadeseh and Rashidi, Lida},
  isbn           = {978-3-319-93033-6},
  pages          = {577--588},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-319-93034-3\_46},
  urldate        = {2019-10-09},
  volume         = {10937},
  abstract       = {This paper presents a generic anomaly detection approach for time-series data. Existing anomaly detection approaches have several drawbacks such as a large number of false positives, parameters tuning difficulties, the need for a labeled dataset for training, use-case restrictions, or difficulty of use. We propose DeepAD, an anomaly detection framework that leverages a plethora of time-series forecasting models in order to detect anomalies more accurately, irrespective of the underlying complex patterns to be learnt. Our solution does not rely on the labels of the anomalous class for training the model, nor for optimizing the threshold based on highest detection given the labels in the training data. We compare our framework against EGADS framework on real and synthetic data with varying time-series characteristics. Results show significant improvements on average of 25\% and up to 40-50\% inF-score, precision, and recall on the Yahoo Webscope Benchmark.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  issn           = {0302-9743},
  timestamp      = {2020-02-25 23:57},
}

@Article{Burns-Whyne-2018,
  author       = {David M. Burns and Cari M. Whyne},
  date         = {2018},
  journaltitle = {Journal of Machine Learning Research},
  title        = {Seglearn: A Python Package for Learning Sequences and Time Series},
  number       = {83},
  pages        = {1--7},
  url          = {http://www.jmlr.org/papers/v19/18-160.html},
  volume       = {19},
  abstract     = {seglearn is an open-source Python package for performing machine learning on time series or sequences. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. Sequences and series may be learned directly with deep learning models or via feature representation with classical machine learning estimators. This package is compatible with scikit-learn and is listed under scikit-learn Related Projects. The package depends on numpy, scipy, and scikit-learn. seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage. Source code and documentation can be downloaded from https://github.com/dmbee/seglearn.},
  groups       = {ML_ForcstTimeSrs},
  timestamp    = {2020-02-25 23:57},
}

@InCollection{Castellani-dosSantos-2014,
  author               = {Castellani, Marco and dos Santos, EmanuelA},
  booktitle            = {Innovations in Intelligent Machines-4},
  date                 = {2014},
  title                = {Prediction of Long-Term Government Bond Yields Using Statistical and Artificial Intelligence Methods},
  doi                  = {10.1007/978-3-319-01866-9\_11},
  editor               = {Faucher, Colette and Jain, Lakhmi C.},
  pages                = {341--367},
  publisher            = {Springer International Publishing},
  series               = {Studies in Computational Intelligence},
  volume               = {514},
  abstract             = {This chapter investigates the use of different artificial intelligence and classical techniques for forecasting the monthly yield of the US 10-year Treasury bonds from a set of four economic indicators. The task is particularly challenging due to the sparseness of the data samples and the complex interactions amongst the variables. At the same time, it is of high significance because of the important and paradigmatic role played by the US market in the world economy. Four data-driven artificial intelligence approaches are considered: a manually built fuzzy logic model, a machine learned fuzzy logic model, a self-organising map model, and a multi-layer perceptron model. Their prediction accuracy is compared with that of two classical approaches: a statistical ARIMA model and an econometric error correction model. The algorithms are evaluated on a complete series of end-month US 10-year Treasury bonds yields and economic indicators from 1986:1 to 2004:12. In terms of prediction accuracy and reliability, the best results are obtained by the three parametric regression algorithms, namely the econometric, the statistical, and the multi-layer perceptron model. Due to the sparseness of the learning data samples, the manual and the automatic fuzzy logic approaches fail to follow with adequate precision the range of variations of the US 10-year Treasury bonds. For similar reasons, the self-organising map model performs unsatisfactorily. Analysis of the results indicates that the econometric model has a slight edge over the statistical and the multi-layer perceptron models. This suggests that pure data-driven induction may not fully capture the complicated mechanisms ruling the changes in interest rates. Overall, the prediction accuracy of the best models is only marginally better than the prediction accuracy of a basic one-step lag predictor. This result highlights the difficulty of the modelling task and, in general, the difficulty of building reliable predictors for financial markets.},
  citeulike-article-id = {14503477},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-01866-911},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-01866-911},
  groups               = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  posted-at            = {2017-12-15 16:28:25},
  timestamp            = {2020-02-25 23:57},
}

@Article{Cerqueira-et-al-2018,
  author         = {Cerqueira, Vitor and Torgo, Luis and Pinto, Fabio and Soares, Carlos},
  date           = {2018-12-04},
  journaltitle   = {Machine learning},
  title          = {Arbitrage of forecasting experts},
  doi            = {10.1007/s10994-018-05774-y},
  issn           = {0885-6125},
  number         = {6},
  pages          = {1--32},
  urldate        = {2019-09-22},
  volume         = {108},
  abstract       = {Forecasting is an important task across several domains. Its generalised interest is related to the uncertainty and complex evolving structure of time series. Forecasting methods are typically designed to cope with temporal dependencies among observations, but it is widely accepted that none is universally applicable. Therefore, a common solution to these tasks is to combine the opinion of a diverse set of forecasts. In this paper we present an approach based on arbitrating, in which several forecasting models are dynamically combined to obtain predictions. Arbitrating is a metalearning approach that combines the output of experts according to predictions of the loss that they will incur. We present an approach for retrieving out-of-bag predictions that significantly improves its data efficiency. Finally, since diversity is a fundamental component in ensemble methods, we propose a method for explicitly handling the inter-dependence between experts when aggregating their predictions. Results from extensive empirical experiments provide evidence of the method competitiveness relative to state of the art approaches. The proposed method is publicly available in a software package.},
  day            = {4},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@InCollection{Cerqueira-et-al-2019,
  author         = {Cerqueira, Vitor and Pinto, Fabio and Torgo, Luis and Soares, Carlos and Moniz, Nuno},
  booktitle      = {Machine learning and knowledge discovery in databases: european conference, ECML PKDD 2018},
  date           = {2019},
  title          = {Constructive Aggregation and Its Application to Forecasting with Dynamic Ensembles},
  doi            = {10.1007/978-3-030-10925-7\_38},
  editor         = {Berlingerio, Michele and Bonchi, Francesco and Gartner, Thomas and Hurley, Neil and Ifrim, Georgiana},
  isbn           = {978-3-030-10924-0},
  pages          = {620--636},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-10925-7\_38},
  urldate        = {2019-09-22},
  volume         = {11051},
  abstract       = {While the predictive advantage of ensemble methods is nowadays widely accepted, the most appropriate way of estimating the weights of each individual model remains an open research question. Meanwhile, several studies report that combining different ensemble approaches leads to improvements in performance, due to a better trade-off between the diversity and the error of the individual models in the ensemble. We contribute to this research line by proposing an aggregation framework for a set of independently created forecasting models, i.e. heterogeneous ensembles. The general idea is to, instead of directly aggregating these models, first rearrange them into different subsets, creating a new set of combined models which is then aggregated into a final decision. We present this idea as constructive aggregation, and apply it to time series forecasting problems. Results from empirical experiments show that applying constructive aggregation to state of the art dynamic aggregation methods provides a consistent advantage. Constructive aggregation is publicly available in a software package. Data related to this paper are available at: https://github.com/vcerqueira/timeseriesdata. Code related to this paper is available at: https://github.com/vcerqueira/tsensembler.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  issn           = {0302-9743},
  timestamp      = {2020-02-25 23:57},
}

@Article{Cerqueira-et-al-2019a,
  author         = {Cerqueira, Vitor and Torgo, Luis and Soares, Carlos},
  date           = {2019-09-29},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Machine Learning vs Statistical Methods for Time Series Forecasting: Size Matters},
  url            = {https://arxiv.org/abs/1909.13316},
  urldate        = {2019-10-11},
  abstract       = {Time series forecasting is one of the most active research topics. Machine learning methods have been increasingly adopted to solve these predictive tasks. However, in a recent work, these were shown to systematically present a lower predictive performance relative to simple statistical methods. In this work, we counter these results. We show that these are only valid under an extremely low sample size. Using a learning curve method, our results suggest that machine learning methods improve their relative predictive performance as the sample size grows. The code to reproduce the experiments is available at this https URL.},
  day            = {29},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Chen-et-al-2019i,
  author         = {Chen, Yitian and Kang, Yanfei and Chen, Yixiong and Wang, Zizhuo},
  date           = {2019-06-11},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Probabilistic Forecasting with Temporal Convolutional Neural Network},
  url            = {https://arxiv.org/abs/1906.04397},
  urldate        = {2019-09-20},
  abstract       = {We present a probabilistic forecasting framework based on convolutional neural network for multiple related time series forecasting. The framework can be applied to estimate probability density under both parametric and non-parametric settings. More specifically, stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. Combined with representation learning, our approach is able to learn complex patterns such as seasonality, holiday effects within and across series, and to leverage those patterns for more accurate forecasts, especially when historical data is sparse or unavailable. Extensive empirical studies are performed on several real-world datasets, including datasets from this http URL, China's largest online retailer. The results show that our framework outperforms other state-of-the-art methods in both accuracy and efficiency.},
  day            = {11},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Cirstea-et-al-2018,
  author         = {Cirstea, Razvan-Gabriel and Micu, Darius-Valer and Muresan, Gabriel-Marcel and Guo, Chenjuan and Yang, Bin},
  date           = {2018-08-29},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Correlated Time Series Forecasting using Deep Neural Networks: A Summary of Results},
  url            = {https:://arxiv.org/abs/1808.09794},
  urldate        = {2019-03-07},
  abstract       = {Cyber-physical systems often consist of entities that interact with each other over time. Meanwhile, as part of the continued digitization of industrial processes, various sensor technologies are deployed that enable us to record time-varying attributes (a.k.a., time series) of such entities, thus producing correlated time series. To enable accurate forecasting on such correlated time series, this paper proposes two models that combine convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The first model employs a CNN on each individual time series, combines the convoluted features, and then applies an RNN on top of the convoluted features in the end to enable forecasting. The second model adds additional auto-encoders into the individual CNNs, making the second model a multi-task learning model, which provides accurate and robust forecasting. Experiments on two real-world correlated time series data set suggest that the proposed two models are effective and outperform baselines in most settings. This report extends the paper "Correlated Time Series Forecasting using Multi-Task Deep Neural Networks," to appear in ACM CIKM 2018, by providing additional experimental results.},
  day            = {29},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Creamer-Lee-2019,
  author         = {Creamer, German G. and Lee, Chihoon},
  date           = {2019-07-10},
  journaltitle   = {Quantitative Finance},
  title          = {A multivariate distance nonlinear causality test based on partial distance correlation: a machine learning application to energy futures},
  doi            = {10.1080/14697688.2019.1622300},
  issn           = {1469-7688},
  pages          = {1--12},
  abstract       = {This paper proposes a multivariate distance nonlinear causality test (MDNC) using the partial distance correlation in a time series framework. Partial distance correlation as an extension of the Brownian distance correlation calculates the distance correlation between random vectors X and Y controlling for a random vector Z. Our test can detect nonlinear lagged relationships between time series, and when integrated with machine learning methods it can improve the forecasting power. We apply our method as a feature selection procedure and combine it with the support vector machine and random forests algorithms to study the forecast of the main energy financial time series (oil, coal, and natural gas futures). It shows substantial improvement in forecasting the fuel energy time series in comparison to the classical Granger causality method in time series.},
  day            = {10},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:57},
}

@Article{Dang-et-al-2018,
  author         = {Dang, Xuan-Hong and Shah, Syed Yousaf and Zerfos, Petros},
  date           = {2018-12-07},
  journaltitle   = {arXiv Electronic Journal},
  title          = {seq2graph: Discovering Dynamic Dependencies from Multivariate Time Series with Multi-level Attention},
  url            = {https:://arxiv.org/abs/1812.04448},
  urldate        = {2019-03-07},
  abstract       = {Discovering temporal lagged and inter-dependencies in multivariate time series data is an important task. However, in many real-world applications, such as commercial cloud management, manufacturing predictive maintenance, and portfolios performance analysis, such dependencies can be non-linear and time-variant, which makes it more challenging to extract such dependencies through traditional methods such as Granger causality or clustering. In this work, we present a novel deep learning model that uses multiple layers of customized gated recurrent units (GRUs) for discovering both time lagged behaviors as well as inter-timeseries dependencies in the form of directed weighted graphs. We introduce a key component of Dual-purpose recurrent neural network that decodes information in the temporal domain to discover lagged dependencies within each time series, and encodes them into a set of vectors which, collected from all component time series, form the informative inputs to discover inter-dependencies. Though the discovery of two types of dependencies are separated at different hierarchical levels, they are tightly connected and jointly trained in an end-to-end manner. With this joint training, learning of one type of dependency immediately impacts the learning of the other one, leading to overall accurate dependencies discovery. We empirically test our model on synthetic time series data in which the exact form of (non-linear) dependencies is known. We also evaluate its performance on two real-world applications, (i) performance monitoring data from a commercial cloud provider, which exhibit highly dynamic, non-linear, and volatile behavior and, (ii) sensor data from a manufacturing plant. We further show how our approach is able to capture these dependency behaviors via intuitive and interpretable dependency graphs and use them to generate highly accurate forecasts.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {ML_ClustTimeSrs, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:58},
}

@Article{Daniel-2019,
  author         = {Daniel, Fabrice},
  date           = {2019-07-03},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Financial Time Series Data Processing for Machine Learning},
  url            = {https://arxiv.org/abs/1907.03010},
  urldate        = {2019-09-06},
  abstract       = {This article studies the financial time series data processing for machine learning. It introduces the most frequent scaling methods, then compares the resulting stationarity and preservation of useful information for trend forecasting. It proposes an empirical test based on the capability to learn simple data relationship with simple models. It also speaks about the data split method specific to time series, avoiding unwanted overfitting and proposes various labelling for classification and regression.},
  day            = {3},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:58},
}

@InCollection{DeStefani-et-al-2019,
  author         = {{De Stefani}, Jacopo and Caelen, Olivier and Hattab, Dalila and Le Borgne, Yann-Ael and Bontempi, Gianluca},
  booktitle      = {ECML PKDD 2018 Workshops: MIDAS 2018 and PAP 2018},
  date           = {2019},
  title          = {A Multivariate and Multi-step Ahead Machine Learning Approach to Traditional and Cryptocurrencies Volatility Forecasting},
  doi            = {10.1007/978-3-030-13463-1\_1},
  isbn           = {978-3-030-13462-4},
  pages          = {7--22},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-13463-1\_1},
  urldate        = {2019-09-07},
  volume         = {11054},
  abstract       = {Multivariate time series forecasting involves the learning of historical multivariate information in order to predict the future values of several quantities of interests, accounting for interdependencies among them. In finance, several of this quantities of interests (stock valuations, return, volatility) have been shown to be mutually influencing each other, making the prediction of such quantities a difficult task, especially while dealing with an high number of variables and multiple horizons in the future. Here we propose a machine learning based framework, the DFML, based on the Dynamic Factor Model, to first perform a dimensionality reduction and then perform a multiple step ahead forecasting of a reduced number of components. Finally, the components are transformed again into an high dimensional space, providing the desired forecast. Our results, comparing the DFML with several state of the art techniques from different domanins (PLS, RNN, LSTM, DFM), on both traditional stock markets and cryptocurrencies market and for different families of volatility proxies show that the DFML outperforms the concurrent methods, especially for longer horizons. We conclude by explaining how we wish to further improve the performances of the framework, both in terms of accuracy and computational efficiency.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  issn           = {0302-9743},
  timestamp      = {2020-02-25 23:58},
}

@Article{Duda-2018,
  author         = {Duda, Jarek},
  date           = {2018-07-11},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Exploiting statistical dependencies of time series with hierarchical correlation reconstruction},
  url            = {https:://arxiv.org/abs/1807.04119},
  urldate        = {2019-03-07},
  abstract       = {While we are usually focused on forecasting future values of time series, it is often valuable to additionally predict their entire probability distributions, e.g. to evaluate risk, Monte Carlo simulations. On example of time series of 30000 Dow Jones Industrial Averages, there will be presented application of hierarchical correlation reconstruction for this purpose: MSE estimating polynomial as joint density for (current value, context), where context is for example a few previous values. Then substituting the currently observed context and normalizing density to 1, we get predicted probability distribution for the current value. In contrast to standard machine learning approaches like neural networks, optimal polynomial coefficients here have inexpensive direct formula, have controllable accuracy, are unique and independently calculated, each has a specific cumulant-like interpretation, and such approximation can asymptotically approach complete description of any real joint distribution - providing universal tool to quantitatively describe and exploit statistical dependencies in time series, systematically enhancing ARMA/ARCH-like approaches, also based on different distributions than Gaussian which turns out improper for daily log returns. There is also discussed application for non-stationary time series like calculating linear time trend, or adapting coefficients to local statistical behavior.},
  day            = {11},
  f1000-projects = {QuantInvest},
  groups         = {NonStatry_FinTimeSrs, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:58},
}

@InProceedings{Faloutsos-et-al-2019,
  author         = {Faloutsos, Christos and Gasthaus, Jan and Januschowski, Tim and Wang, Yuyang},
  booktitle      = {Proceedings of the 2019 International Conference on Management of Data - SIGMOD '19},
  date           = {2019-06-30},
  title          = {Classical and contemporary approaches to big time series forecasting},
  doi            = {10.1145/3299869.3314033},
  isbn           = {9781450356435},
  location       = {New York, New York, USA},
  pages          = {2042--2047},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3299869.3314033},
  urldate        = {2019-09-20},
  abstract       = {Time series forecasting is a key ingredient in the automation and optimization of business processes: in retail, deciding which products to order and where to store them depends on the forecasts of future demand in different regions; in cloud computing, the estimated future usage of services and infrastructure components guides capacity planning; and workforce scheduling in warehouses and factories requires forecasts of the future workload. Recent years have witnessed a paradigm shift in forecasting techniques and applications, from computer-assisted model- and assumption-based to data-driven and fully-automated. This shift can be attributed to the availability of large, rich, and diverse time series corpora and result in a set of challenges that need to be addressed such as the following. How can we build statistical models to efficiently and effectively learn to forecast from large and diverse data sources? How can we leverage the statistical power of "similar'' time series to improve forecasts in the case of limited observations? What are the implications for building forecasting systems that can handle large data volumes? The objective of this tutorial is to provide a concise and intuitive overview of the most important methods and tools available for solving large-scale forecasting problems. We review the state of the art in three related fields: (1) classical modeling of time series, (2) scalable tensor methods, and (3) deep learning for forecasting. Further, we share lessons learned from building scalable forecasting systems. While our focus is on providing an intuitive overview of the methods and practical issues which we will illustrate via case studies, we also present some technical details underlying these powerful tools.},
  day            = {30},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:58},
}

@Article{Fan-et-al-2017a,
  author               = {Fan, Jianqing and Xue, Lingzhou and Yao, Jiawei},
  date                 = {2017-12},
  journaltitle         = {Journal of Econometrics},
  title                = {Sufficient forecasting using factor models},
  doi                  = {10.1016/j.jeconom.2017.08.009},
  issn                 = {0304-4076},
  number               = {2},
  pages                = {292--306},
  volume               = {201},
  abstract             = {We consider forecasting a single time series when there is a large number of predictors and a possible nonlinear effect. The dimensionality was first reduced via a high-dimensional factor model implemented by the principal component analysis. Using the extracted factors, we develop a novel forecasting method called the sufficient forecasting, which provides a set of sufficient predictive indices, inferred from high-dimensional predictors, to deliver additional predictive power. The projected principal component analysis will be employed to enhance the accuracy of inferred factors when a semi-parametric factor model is assumed. Our method is also applicable to cross-sectional sufficient regression using extracted factors. The connection between the sufficient forecasting and the deep learning architecture is explicitly stated. The sufficient forecasting correctly estimates projection indices of the underlying factors even in the presence of a nonparametric forecasting function. The proposed method extends the sufficient dimension reduction to high-dimensional regimes by condensing the cross-sectional information through factor models. We derive asymptotic properties for the estimate of the central subspace spanned by these projection directions as well as the estimates of the sufficient predictive indices. We further show that the natural method of running multiple regression of target on estimated factors yields a linear estimate that actually falls into this central subspace. Our method and theory allow the number of predictors to be larger than the number of observations. We finally demonstrate that the sufficient forecasting improves upon the linear forecasting in both simulation studies and an empirical study of forecasting macroeconomic variables.},
  citeulike-article-id = {14503772},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jeconom.2017.08.009},
  groups               = {Regression_Nonlinear, ML_ForcstTimeSrs},
  posted-at            = {2017-12-16 16:41:38},
  timestamp            = {2020-02-25 23:58},
}

@InProceedings{Fan-et-al-2019a,
  author         = {Fan, Chenyou and Huang, Heng and Zhang, Yuze and Pan, Yi and Li, Xiaoyue and Zhang, Chi and Yuan, Rong and Wu, Di and Wang, Wensheng and Pei, Jian},
  booktitle      = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining - KDD '19},
  date           = {2019-08-04},
  title          = {Multi-Horizon Time Series Forecasting with Temporal Attention Learning},
  doi            = {10.1145/3292500.3330662},
  isbn           = {9781450362016},
  location       = {New York, New York, USA},
  pages          = {2527--2535},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3292500.3330662},
  urldate        = {2019-09-20},
  abstract       = {We propose a novel data-driven approach for solving multi-horizon probabilistic forecasting tasks that predicts the full distribution of a time series on future horizons. We illustrate that temporal patterns hidden in historical information play an important role in accurate forecasting of long time series. Traditional methods rely on setting up temporal dependencies manually to explore related patterns in historical data, which is unrealistic in forecasting long-term series on real-world data. Instead, we propose to explicitly learn constructing hidden patterns' representations with deep neural networks and attending to different parts of the history for forecasting the future. In this paper, we propose an end-to-end deep-learning framework for multi-horizon time series forecasting, with temporal attention mechanisms to better capture latent patterns in historical data which are useful in predicting the future. Forecasts of multiple quantiles on multiple future horizons can be generated simultaneously based on the learned latent pattern features. We also propose a multimodal fusion mechanism which is used to combine features from different parts of the history to better represent the future. Experiment results demonstrate our approach achieves state-of-the-art performance on two large-scale forecasting datasets in different domains.},
  day            = {4},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:58},
}

@Article{Fawaz-et-al-2019b,
  author         = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  date           = {2019-09-12},
  journaltitle   = {Data Mining and Knowledge Discovery},
  title          = {Deep learning for time series classification: a review},
  url            = {https://arxiv.org/abs/1809.04356v1},
  abstract       = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state of the art performance for document classification and speech recognition. In this article, we study the current state of the art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
  day            = {12},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Forecast_QWIM, FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:58},
}

@Article{Feng-et-al-2019b,
  author         = {Feng, Guanhao and Polson, Nick and Xu, Jianeng},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Deep Learning in Characteristics-Sorted Factor Models},
  issn           = {1556-5068},
  url            = {https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=3243683},
  abstract       = {To study the characteristics-sorted factor model in asset pricing, we develop a bottom-up approach with state-of-the-art deep learning optimization. With an economic objective to minimize pricing errors, we train a non-reduced-form neural network using firm characteristics [inputs], and generate factors [intermediate features], to fit security returns [outputs]. Sorting securities on firm characteristics provides a nonlinear activation to create long-short portfolio weights, as a hidden layer, from lag characteristics to realized returns. Our model offers an alternative perspective for dimension reduction on firm characteristics [inputs], rather than factors [intermediate features], and allows for both nonlinearity and interactions on inputs. Our empirical findings are twofold. We find robust statistical and economic evidence in out-of-sample portfolios and individual stock returns. To interpret our deep factors, we show highly significant results in factor investing via the squared Sharpe ratio test, as well as improvement in dissecting anomalies.},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_Forecast_QWIM, FrcstQWIM_ML, ML_Test_OOS, ML_Test_CrossVal, ML_Validation, FrcstQWIM_ShortTerm, FcstQWIM_Equity, ML_ForcstTimeSrs, ML_AssetPricing},
  timestamp      = {2020-02-25 23:58},
}

@Article{Feuerriegel-Gordon-2018a,
  author         = {Feuerriegel, Stefan and Gordon, Julius},
  date           = {2018-01-22},
  journaltitle   = {European Journal of Operational Research},
  title          = {News-based forecasts of macroeconomic indicators: A semantic path model for interpretable predictions},
  doi            = {10.1016/j.ejor.2018.05.068},
  url            = {https://www.sciencedirect.com/science/article/abs/pii/S0377221718304879},
  abstract       = {The macroeconomic climate influences operations with regard to, e.g., raw material prices, financing, supply chain utilization and demand quotas. In order to adapt to the economic environment, decision-makers across the public and private sectors require accurate forecasts of the economic outlook. Existing predictive frameworks base their forecasts primarily on time series analysis, as well as the judgments of experts. As a consequence, current approaches are often biased and prone to error. In order to reduce forecast errors, this paper presents an innovative methodology that extends lag variables with unstructured data in the form of financial news: (1) we apply a variety of models from machine learning to word counts as a high-dimensional input. However, this approach suffers from low interpretability and overfitting, motivating the following remedies. (2) We follow the intuition that the economic climate is driven by general sentiments and suggest a projection of words onto latent semantic structures as a means of feature engineering. (3) We propose a semantic path model, together with estimation technique based on regularization, in order to yield full interpretability of the forecasts. We demonstrate the predictive performance of our approach by utilizing 80,813 ad hoc announcements in order to make long-term forecasts of up to 24 months ahead regarding key macroeconomic indicators. Back-testing reveals a considerable reduction in forecast errors.},
  day            = {22},
  f1000-projects = {QuantInvest},
  groups         = {ML_Interpretability, Scenario_ExpertView, ML_Overfitting, ML_ForcstTimeSrs, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:58},
}

@Article{Gasthaus-et-al-2019,
  author       = {Gasthaus, Jan and Benidis, Konstantinos and Wang, Yuyang and Rangapuram, Syama Sundar and Salinas, David and Flunkert, Valentin and Januschowski, Tim},
  journaltitle = {Proceedings of Machine Learning Research},
  title        = {Probabilistic Forecasting with Spline Quantile Function RNNs},
  pages        = {1901--1910},
  url          = {http://proceedings.mlr.press/v89/gasthaus19a.html},
  volume       = {89},
  abstract     = {In this paper, we propose a flexible method for probabilistic modeling with conditional quantile functions using monotonic regression splines. The shape of the spline is parameterized by a neural network whose parameters are learned by minimizing the continuous ranked probability score. Within this framework, we propose a method for probabilistic time series forecasting, which combines the modeling capacity of recurrent neural networks with the flexibility of a spline-based representation of the output distribution. Unlike methods based on parametric probability density functions and maximum likelihood estimation, the proposed method can flexibly adapt to different output distributions without manual intervention. We empirically demonstrate the effectiveness of the approach  on synthetic and real-world data sets.},
  booktitle    = {Proceedings of Machine Learning Research},
  groups       = {ML_ForcstTimeSrs},
  month        = {16--18 Apr},
  pdf          = {http://proceedings.mlr.press/v89/gasthaus19a/gasthaus19a.pdf},
  publisher    = {PMLR},
  timestamp    = {2020-02-25 23:58},
  year         = {2019},
}

@Article{Gilliland-2019,
  author         = {Gilliland, Michael},
  date           = {2019-07},
  journaltitle   = {International Journal of Forecasting},
  title          = {The value added by machine learning approaches in forecasting},
  doi            = {10.1016/j.ijforecast.2019.04.016},
  issn           = {0169-2070},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301165},
  urldate        = {2019-09-01},
  abstract       = {This discussion reflects on the results of the M4 forecasting competition, and in particular, the impact of machine learning (ML) methods. Unlike the M3, which included only one ML method (an automatic artificial neural network that performed poorly), M4 49 participants included eight that used either pure ML approaches, or ML in conjunction with statistical methods. The six pure (or combination of pure) ML methods again fared poorly, with all of them falling below the Comb benchmark that combined three simple time series methods. However, utilizing ML either in combination with statistical methods (and for selecting weightings) or in a hybrid model with exponential smoothing not only exceeded the benchmark, but performed at the top. While these promising results by no means prove ML to be a panacea, they do challenge the notion that complex methods do not add value to the forecasting process.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:58},
}

@Article{Giusto-Piger-2016,
  author               = {Giusto, Andrea and Piger, Jeremy},
  date                 = {2016-07},
  journaltitle         = {International Journal of Forecasting},
  title                = {Identifying business cycle turning points in real time with vector quantization},
  doi                  = {10.1016/j.ijforecast.2016.04.006},
  issn                 = {0169-2070},
  abstract             = {We propose a simple machine-learning algorithm known as Learning Vector Quantization (LVQ) for the purpose of identifying new U.S. business cycle turning points quickly in real time. LVQ is used widely for real-time statistical classification in many other fields, but has not previously been applied to the classification of economic variables, to the best of our knowledge. The algorithm is intuitive and simple to implement, and easily incorporates salient features of the real-time nowcasting environment, such as differences in data reporting lags across series. We evaluate the algorithm's real-time ability to establish new business cycle turning points in the United States quickly and accurately over the past five NBER recessions. Despite its relative simplicity, the algorithm's performance appears to be very competitive with those of commonly used alternatives.},
  citeulike-article-id = {14147219},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2016.04.006},
  groups               = {ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-09-27 15:28:45},
  timestamp            = {2020-02-25 23:58},
}

@Article{GrushkaCockayne-Jose-2019,
  author         = {Grushka-Cockayne, Yael and Jose, Victor Richmond R.},
  date           = {2019-07},
  journaltitle   = {International Journal of Forecasting},
  title          = {Combining prediction intervals in the M4 competition},
  doi            = {10.1016/j.ijforecast.2019.04.015},
  issn           = {0169-2070},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301141},
  urldate        = {2019-09-01},
  abstract       = {The 2018 M4 Forecasting Competition was the first M Competition to elicit prediction intervals in addition to point estimates. We take a closer look at the twenty valid interval submissions by examining the calibration and accuracy of the prediction intervals and evaluating their performances over different time horizons. Overall, the submissions fail to estimate the uncertainty properly. Importantly, we investigate the benefits of interval combination using six recently-proposed heuristics that can be applied prior to learning about the realizations of the quantities. Our results suggest that interval aggregation offers improvements in terms of both calibration and accuracy. While averaging interval endpoints maintains its practical appeal as being simple to implement and performs quite well when data sets are large, the median and the interior trimmed average are found to be robust aggregators for the prediction interval submissions across all 100,000 time series.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:58},
}

@InProceedings{Guo-et-al-2017b,
  author         = {Guo, Xuqi and Pang, Yusong and Yan, Gaowei and Qiao, Tiezhu},
  booktitle      = {2017 29th Chinese Control And Decision Conference (CCDC)},
  date           = {2017-05-28},
  title          = {Time series forecasting based on deep extreme learning machine},
  doi            = {10.1109/{CCDC}.2017.7978277},
  isbn           = {978-1-5090-4657-7},
  pages          = {6151--6156},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/7978277/},
  urldate        = {2019-09-11},
  day            = {28},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:58},
}

@PhdThesis{Habibnia-2016,
  author               = {Habibnia, Ali},
  date                 = {2016},
  institution          = {London School of Economics and Political Science},
  title                = {Essays in high-dimensional nonlinear time series analysis},
  url                  = {http://etheses.lse.ac.uk/3485/},
  abstract             = {In this thesis, I study high-dimensional nonlinear time series analysis, and its applications in financial forecasting and identifying risk in highly interconnected financial networks. The first chapter is devoted to the testing for nonlinearity in financial time series. I present a tentative classification of the various linearity tests that have been proposed in the literature. Then I investigate nonlinear features of real financial series to determine if the data justify the use of nonlinear techniques, such as those inspired by machine learning theories. In Chapter 3 and 5, I develop forecasting strategies with a high-dimensional panel of predictors while considering nonlinear dynamics. Combining these two elements is a developing area of research. In the third chapter, I propose a nonlinear generalization of the statistical factor models. As a first step, factor estimation, I employ an auto-associative neural network to estimate nonlinear factors from predictors. In the second step, forecasting equation, I apply a nonlinear function -feedforward neural networkon estimated factors for prediction. I show that these features can go beyond covariance analysis and enhance forecast accuracy. I apply this approach to forecast equity returns, and show that capturing nonlinear dynamics between equities significantly improves the quality of forecasts over current univariate and multivariate factor models. In Chapter 5, I propose a high-dimensional learning based on a shrinkage estimation of a backpropagation algorithm for skip-layer neural networks. This thesis emphasizes that linear models can be represented as special cases of these two aforementioned models, which basically means that if there is no nonlinearity between series, the proposed models will reduce to a linear model. This thesis also includes a chapter (chapter 4, with Negar Kiyavash and Seyedjalal Etesami), which in this chapter, we propose a new approach for identifying and measuring systemic risk in financial networks by introducing a nonlinearly modified Granger-causality network based on directed information graphs. The suggested method allows for nonlinearity and has predictive power over future economic activity through a time-varying network of interconnections. We apply the method to the daily returns of U.S. financial Institutions including banks, brokers and insurance companiesto identifythe level of systemic risk inthe financial sector and the contribution of each financial institution.},
  citeulike-article-id = {14334328},
  groups               = {Data_NonLinear, ML_ForcstTimeSrs},
  posted-at            = {2017-04-08 19:54:56},
  timestamp            = {2020-02-25 23:58},
}

@InCollection{He-et-al-2019b,
  author         = {He, Qi-Qiao and Pang, Patrick Cheong-Iao and Si, Yain-Whar},
  booktitle      = {PRICAI 2019: trends in artificial intelligence: 16th Pacific Rim international conference on artificial intelligence},
  date           = {2019},
  title          = {Transfer learning for financial time series forecasting},
  doi            = {10.1007/978-3-030-29911-8\_3},
  editor         = {Nayak, Abhaya C. and Sharma, Alok},
  isbn           = {978-3-030-29910-1},
  pages          = {24--36},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-29911-8\_3},
  urldate        = {2019-09-20},
  volume         = {11671},
  abstract       = {Time-series are widely used for representing non-stationary data such as weather information, health related data, economic and stock market indexes. Many statistical methods and traditional machine learning techniques are commonly used for forecasting time series. With the development of deep learning in artificial intelligence, many researchers have adopted new models from artificial neural networks for forecasting time series. However, poor performance of applying deep learning models in short time series hinders the accuracy in time series forecasting. In this paper, we propose a novel approach to alleviate this problem based on transfer learning. Existing work on transfer learning uses extracted features from a source dataset for prediction task in a target dataset. In this paper, we propose a new training strategy for time-series transfer learning with two source datasets that outperform existing approaches. The effectiveness of our approach is evaluated on financial time series extracted from stock markets. Experiment results show that transfer learning based on 2 data sets is superior than other base-line methods.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  issn           = {0302-9743},
  timestamp      = {2020-02-25 23:58},
}

@Article{Hsu-et-al-2016b,
  author               = {Hsu, Ming-Wei and Lessmann, Stefan and Sung, Ming-Chien and Ma, Tiejun and Johnson, Johnnie E. V.},
  date                 = {2016-11},
  journaltitle         = {Expert Systems with Applications},
  title                = {Bridging the divide in financial market forecasting: machine learners vs. financial economists},
  doi                  = {10.1016/j.eswa.2016.05.033},
  issn                 = {0957-4174},
  pages                = {215--234},
  volume               = {61},
  abstract             = {An extensive benchmark in financial time series forecasting is performed. Best machine learning(ML) methods out-perform best econometric methods. The ML methodology employed significantly affects forecasting accuracy. Market maturity, forecast horizon and model-assessment method affect forecast accuracy. Evidence against the informational value of technical indicators. Financial time series forecasting is a popular application of machine learning methods. Previous studies report that advanced forecasting methods predict price changes in financial markets with high accuracy and that profit can be made trading on these predictions. However, financial economists point to the informational efficiency of financial markets, which questions price predictability and opportunities for profitable trading. The objective of the paper is to resolve this contradiction. To this end, we undertake an extensive forecasting simulation, based on data from thirty-four financial indices over six years. These simulations confirm that the best machine learning methods produce more accurate forecasts than the best econometric methods. We also examine the methodological factors that impact the predictive accuracy of machine learning forecasting experiments. The results suggest that the predictability of a financial market and the feasibility of profitable model-based trading are significantly influenced by the maturity of the market, the forecasting method employed, the horizon for which it generates predictions and the methodology used to assess the model and simulate model-based trading. We also find evidence against the informational value of indicators from the field of technical analysis. Overall, we confirm that advanced forecasting methods can be used to predict price changes in some financial markets and we discuss whether these results question the prevailing view in the financial economics literature that financial markets are efficient.},
  citeulike-article-id = {14070589},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2016.05.033},
  groups               = {Scenario_ExpertView, ML_ForcstTimeSrs},
  owner                = {zkgst0c},
  posted-at            = {2016-06-16 23:16:26},
  timestamp            = {2020-02-25 23:58},
}

@Article{Huang-et-al-2019d,
  author         = {Huang, Biwei and Zhang, Kun and Gong, Mingming and Glymour, Clark},
  date           = {2019-06},
  journaltitle   = {Proceedings of machine learning research},
  title          = {Causal Discovery and Forecasting in Nonstationary Environments with State-Space Models.},
  pages          = {2901--2910},
  url            = {https://www.ncbi.nlm.nih.gov/pubmed/31497778},
  urldate        = {2019-09-22},
  volume         = {97},
  abstract       = {In many scientific fields, such as economics and neuroscience, we are often faced with nonstationary time series, and concerned with both finding causal relations and forecasting the values of variables of interest, both of which are particularly challenging in such nonstationary environments. In this paper, we study causal discovery and forecasting for nonstationary time series. By exploiting a particular type of state-space model to represent the processes, we show that nonstationarity helps to identify causal structure and that forecasting naturally benefits from learned causal knowledge. Specifically, we allow changes in both causal strengths and noise variances in the nonlinear state-space models, which, interestingly, renders both the causal structure and model parameters identifiable. Given the causal model, we treat forecasting as a problem in Bayesian inference in the causal model, which exploits the timevarying property of the data and adapts to new observations in a principled manner. Experimental results on synthetic and real-world data sets demonstrate the efficacy of the proposed methods.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  pmcid          = {PMC6730644},
  pmid           = {31497778},
  timestamp      = {2020-02-25 23:58},
}

@Article{Passalis-et-al-2019,
  author         = {Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
  date           = {2019-02-21},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep Adaptive Input Normalization for Time Series Forecasting},
  url            = {https://arxiv.org/abs/1902.07892},
  urldate        = {2019-10-02},
  abstract       = {Deep Learning (DL) models can be used to tackle time series analysis tasks with great success. However, the performance of DL models can degenerate rapidly if the data are not appropriately normalized. This issue is even more apparent when DL is used for financial time series forecasting tasks, where the non-stationary and multimodal nature of the data pose significant challenges and severely affect the performance of DL models. In this work, a simple, yet effective, neural layer, that is capable of adaptively normalizing the input time series, while taking into account the distribution of the data, is proposed. The proposed layer is trained in an end-to-end fashion using back-propagation and leads to significant performance improvements compared to other evaluated normalization schemes. The proposed method differs from traditional normalization methods since it learns how to perform normalization for a given task instead of using a fixed normalization scheme. At the same time, it can be directly applied to any new time series without requiring re-training. The effectiveness of the proposed method is demonstrated using a large-scale limit order book dataset, as well as a load forecasting dataset.},
  day            = {21},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:59},
}

@Article{Philps-et-al-2018,
  author         = {Philps, Daniel and Weyde, Tillman and Garcez, Artur d'Avila and Batchelor, Roy},
  date           = {2018-12-06},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Continual Learning Augmented Investment Decisions},
  url            = {https://arxiv.org/abs/1812.02340},
  urldate        = {2019-09-06},
  abstract       = {Investment decisions can benefit from incorporating an accumulated knowledge of the past to drive future decision making. We introduce Continual Learning Augmentation (CLA) which is based on an explicit memory structure and a feed forward neural network (FFNN) base model and used to drive long term financial investment decisions. We demonstrate that our approach improves accuracy in investment decision making while memory is addressed in an explainable way. Our approach introduces novel remember cues, consisting of empirically learned change points in the absolute error series of the FFNN. Memory recall is also novel, with contextual similarity assessed over time by sampling distances using dynamic time warping (DTW). We demonstrate the benefits of our approach by using it in an expected return forecasting task to drive investment decisions. In an investment simulation in a broad international equity universe between 2003-2017, our approach significantly outperforms FFNN base models. We also illustrate how CLA's memory addressing works in practice, using a worked example to demonstrate the explainability of our approach.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:59},
}

@InCollection{Qiu-et-al-2014,
  author               = {Qiu, Xueheng and Zhang, Le and Ren, Ye and Suganthan, P. and Amaratunga, Gehan},
  booktitle            = {2014 IEEE Symposium on Computational Intelligence in Ensemble Learning (CIEL)},
  date                 = {2014-12},
  title                = {Ensemble deep learning for regression and time series forecasting},
  doi                  = {10.1109/ciel.2014.7015739},
  isbn                 = {978-1-4799-4512-2},
  location             = {Orlando, FL, USA},
  pages                = {1--6},
  publisher            = {IEEE},
  abstract             = {In this paper, for the first time, an ensemble of deep learning belief networks (DBN) is proposed for regression and time series forecasting. Another novel contribution is to aggregate the outputs from various DBNs by a support vector regression (SVR) model. We show the advantage of the proposed method on three electricity load demand datasets, one artificial time series dataset and three regression datasets over other benchmark methods.},
  citeulike-article-id = {14510348},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ciel.2014.7015739},
  groups               = {FrcstQWIM_ML, ML_ForcstTimeSrs},
  posted-at            = {2017-12-30 12:31:54},
  timestamp            = {2020-02-25 23:59},
}

@Article{Rangapuram-et-al-2018,
  author         = {Rangapuram, Syama Sundar and Seeger, Matthias W. and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
  date           = {2018},
  journaltitle   = {Advances in Neural Information Processing Systems 31 (NIPS 2018)},
  title          = {Deep State Space Models for Time Series Forecasting},
  url            = {https://papers.nips.cc/paper/8004-deep-state-space-models-for-time-series-forecasting},
  urldate        = {2019-09-28},
  abstract       = {We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and interpretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from millions of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:59},
}

@Article{Rapach-Zhou-2019,
  author         = {Rapach, David and Zhou, Guofu},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Time-Series and Cross-Sectional Stock Return Forecasting: New Machine Learning Methods},
  doi            = {10.2139/ssrn.3428095},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3428095},
  urldate        = {2019-08-10},
  abstract       = {This paper extends the machine learning methods developed in Han et al. (2019) for forecasting cross-sectional stock returns to a time-series context. The methods use the elastic net to refine the simple combination return forecast from Rapach et al. (2010). In a time-series application focused on forecasting the US market excess return using a large number of potential predictors, we find that the elastic net refinement substantively improves the simple combination forecast, thereby providing one of the best market excess return forecasts to date. We also discuss the cross-sectional return forecasts developed in Han et al. (2019), highlighting how machine learning methods can be used to improve combination forecasts in both the time-series and cross-sectional dimensions. Overall, because many important questions in finance are related to time-series or cross-sectional return forecasts, the machine learning methods discussed in this paper should provide valuable tools to researchers and practitioners alike.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:59},
}

@Article{Ryll-Seidens-2019,
  author         = {Ryll, Lukas and Seidens, Sebastian},
  date           = {2019-06-18},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Evaluating the Performance of Machine Learning Algorithms in Financial Market Forecasting: A Comprehensive Survey},
  url            = {https://arxiv.org/abs/1906.07786},
  urldate        = {2019-09-06},
  abstract       = {With increasing competition and pace in the financial markets, robust forecasting methods are becoming more and more valuable to investors. While machine learning algorithms offer a proven way of modeling non-linearities in time series, their advantages against common stochastic models in the domain of financial market prediction are largely based on limited empirical results. The same holds true for determining advantages of certain machine learning architectures against others. This study surveys more than 150 related articles on applying machine learning to financial market forecasting. Based on a comprehensive literature review, we build a table across seven main parameters describing the experiments conducted in these studies. Through listing and classifying different algorithms, we also introduce a simple, standardized syntax for textually representing machine learning algorithms. Based on performance metrics gathered from papers included in the survey, we further conduct rank analyses to assess the comparative performance of different algorithm classes. Our analysis shows that machine learning algorithms tend to outperform most traditional stochastic methods in financial market forecasting. We further find evidence that, on average, recurrent neural networks outperform feed forward neural networks as well as support vector machines which implies the existence of exploitable temporal dependencies in financial time series across multiple asset classes and geographies.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs, ML_Text_QWIM},
  timestamp      = {2020-02-25 23:59},
}

@Article{Salinas-et-al-2019,
  author         = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan},
  date           = {2019-04-13},
  journaltitle   = {arXiv Electronic Journal},
  title          = {DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks},
  url            = {https://arxiv.org/abs/1704.04110},
  urldate        = {2019-09-28},
  abstract       = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
  day            = {13},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:59},
}

@Article{Sambasivan-Das-2017,
  author               = {Sambasivan, Rajiv and Das, Sourish},
  date                 = {2017-03-04},
  journaltitle         = {arXiv Electronic Journal},
  title                = {A Statistical Machine Learning Approach to Yield Curve Forecasting},
  eprint               = {1703.01536},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1703.01536},
  abstract             = {Yield curve forecasting is an important problem in finance. In this work we explore the use of Gaussian Processes in conjunction with a dynamic modeling strategy, much like the Kalman Filter, to model the yield curve. Gaussian Processes have been successfully applied to model functional data in a variety of applications. A Gaussian Process is used to model the yield curve. The hyper-parameters of the Gaussian Process model are updated as the algorithm receives yield curve data. Yield curve data is typically available as a time series with a frequency of one day. We compare existing methods to forecast the yield curve with the proposed method. The results of this study showed that while a competing method (a multivariate time series method) performed well in forecasting the yields at the short term structure region of the yield curve, Gaussian Processes perform well in the medium and long term structure regions of the yield curve. Accuracy in the long term structure region of the yield curve has important practical implications. The Gaussian Process framework yields uncertainty and probability estimates directly in contrast to other competing methods. Analysts are frequently interested in this information. In this study the proposed method has been applied to yield curve forecasting, however it can be applied to model high frequency time series data or data streams in other domains.},
  citeulike-article-id = {14510859},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.01536},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.01536},
  day                  = {4},
  groups               = {FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_ForcstTimeSrs},
  posted-at            = {2018-01-02 02:11:16},
  timestamp            = {2020-02-25 23:59},
}

@Article{Sepp-2018a,
  author         = {Sepp, Artur},
  date           = {2018-05-14},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Machine Learning for Volatility Trading},
  url            = {https://ssrn.com/abstract=3186401},
  abstract       = {Academics and practitioners have developed many models for volatility measurement and forecast - I estimate that the total number of available models to be about 200-300 if we count all modifications of intraday estimators, GARCH-type and continuous-time models. In practice, the estimate and forecast of the volatility serves provide vital inputs to many applications ranging from signal construction to algorithmic strategies and quantitative methods for portfolio allocation. By applying machine learning to the volatility modeling, we can reduce the back-test bias and, as a result, improve the performance of live strategies. First, I implemented about 40 different volatility models from 4 separate model classes including intraday estimators, GARCH-type and Bayesian models, and Hidden Markov Chain (HMC) models. Then, I applied the supervised learning for each of the volatility models with the goal is to analyze the out-of-sample fit of the model prediction to the time series data. I propose a few regression-based tests which are applied to gauge the performance of all volatility models. The final step is the reinforcement learning that includes aggregation and analysis of the test results from the supervised learning. The goal is to dynamically select the best model out of 40 that provides the best predicative power out-of-sample. I use the analogy to the web-search to weight the importance of the test results when producing volatility forecasts for specific trading algorithms. One of key discoveries is that Hidden Markov Chain model is one of the best model for volatility forecast across many asset classes. I also observe the cyclical pattern in the rankings of the best models. On one hand, Hidden Chain models perform the best in periods with strong trends. On the other hand, simple intraday estimators perform the best in periods with range-bound markets. The machine learning enables to dynamically choose the best model for the present cycle.},
  day            = {14},
  f1000-projects = {QuantInvest},
  groups         = {ML_Forecast_QWIM, FrcstQWIM_ML, ML_PerfMetrics, ML_Test_OOS, ML_ForcstTimeSrs, ML_InvestSelect},
  timestamp      = {2020-02-25 23:59},
}

@Article{Sergio-et-al-2016,
  author               = {Sergio, Anderson T. and de Lima, Tiago P. F. and Ludermir, Teresa B.},
  date                 = {2016-12},
  journaltitle         = {Neurocomputing},
  title                = {Dynamic selection of forecast combiners},
  doi                  = {10.1016/j.neucom.2016.08.072},
  issn                 = {0925-2312},
  pages                = {37--50},
  volume               = {218},
  abstract             = {Time series forecasting is an important research field in machine learning. Since the literature shows several techniques for the solution of this problem, combining outputs of different models is a simple and robust strategy. However, even when using combiners, the experimenter may face the following dilemma: which technique should one use to combine the individual predictors? Inspired by classification and pattern recognition algorithms, this work presents a dynamic selection method of forecast combiners. In the dynamic selection, each test pattern is submitted to a certain combiner according to a nearest neighbor rule. The proposed method was used to forecast eight time series with chaotic behavior in short and long term. In general, the dynamic selection presented satisfactory results for all datasets.},
  citeulike-article-id = {14503762},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neucom.2016.08.072},
  groups               = {FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  posted-at            = {2017-12-16 16:27:27},
  timestamp            = {2020-02-25 23:59},
}

@PhdThesis{Sewell-2017,
  author         = {Sewell, MV},
  date           = {2017},
  institution    = {University College London},
  title          = {Application of Machine Learning to Financial Time Series Analysis},
  type           = {THESIS.DOCTORAL},
  url            = {http://discovery.ucl.ac.uk/1551670/},
  abstract       = {This multidisciplinary thesis investigates the application of machine learning to financial time series analysis. The research is motivated by the following thesis question: one improve upon the state of the art in financial time series analysis through the application of machine learning? The work is split according to the following time series trichotomy: 1) characterization - determine the fundamental properties of the time series; 2) modelling - find a description that accurately captures features of the long-term behaviour of the system; and 3) forecasting -accurately predict the short-term evolution of the system.},
  f1000-projects = {QuantInvest},
  groups         = {ML_Forecast_QWIM, FrcstQWIM_ML, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:59},
}

@Article{Shynkevich-et-al-2017a,
  author               = {Shynkevich, Yauheniya and McGinnity, T. M. and Coleman, Sonya A. and Belatreche, Ammar and Li, Yuhua},
  date                 = {2017-11},
  journaltitle         = {Neurocomputing},
  title                = {Forecasting price movements using technical indicators: Investigating the impact of varying input window length},
  doi                  = {10.1016/j.neucom.2016.11.095},
  issn                 = {0925-2312},
  pages                = {71--88},
  volume               = {264},
  abstract             = {The creation of a predictive system that correctly forecasts future changes of a stock price is crucial for investment management and algorithmic trading. The use of technical analysis for financial forecasting has been successfully employed by many researchers. Input window length is a time frame parameter required to be set when calculating many technical indicators. This study explores how the performance of the predictive system depends on a combination of a forecast horizon and an input window length for forecasting variable horizons. Technical indicators are used as input features for machine learning algorithms to forecast future directions of stock price movements. The dataset consists of ten years daily price time series for fifty stocks. The highest prediction performance is observed when the input window length is approximately equal to the forecast horizon. This novel pattern is studied using multiple performance metrics: prediction accuracy, winning rate, return per trade and Sharpe ratio.},
  citeulike-article-id = {14500441},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neucom.2016.11.095},
  groups               = {Machine learning and investment strategies, ML_Forecast_QWIM, ML_ForcstTimeSrs},
  posted-at            = {2017-12-11 07:47:09},
  timestamp            = {2020-02-25 23:59},
}

@Article{SiamiNamini-Namin-2018,
  author         = {Siami-Namini, Sima and Namin, Akbar Siami},
  date           = {2018},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Forecasting Economics and Financial Time Series: ARIMA vs. LSTM},
  url            = {https://arxiv.org/abs/1803.06386},
  abstract       = {Forecasting time series data is an important subject in economics, business, and finance. Traditionally, there are several techniques to effectively forecast the next lag of time series data such as univariate Autoregressive (AR), univariate Moving Average (MA), Simple Exponential Smoothing (SES), and more notably Autoregressive Integrated Moving Average (ARIMA) with its many variations. In particular, ARIMA model has demonstrated its outperformance in precision and accuracy of predicting the next lags of time series. With the recent advancement in computational power of computers and more importantly developing more advanced machine learning algorithms and approaches such as deep learning, new algorithms are developed to forecast time series data. The research question investigated in this article is that whether and how the newly developed deep learning-based algorithms for forecasting time series data, such as "Long Short-Term Memory (LSTM)", are superior to the traditional algorithms. The empirical studies conducted and reported in this article show that deep learning-based algorithms such as LSTM outperform traditional-based algorithms such as ARIMA model. More specifically, the average reduction in error rates obtained by LSTM is between 84 - 87 percent when compared to ARIMA indicating the superiority of LSTM to ARIMA. Furthermore, it was noticed that the number of training times, known as "epoch" in deep learning, has no effect on the performance of the trained forecast model and it exhibits a truly random behavior.},
  f1000-projects = {QuantInvest},
  groups         = {ML_Forecast_QWIM, FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:59},
}

@Article{Singh-Srivastava-2017,
  author               = {Singh, Ritika and Srivastava, Shashi},
  date                 = {2017},
  journaltitle         = {Multimedia Tools and Applications},
  title                = {Stock prediction using deep learning},
  doi                  = {10.1007/s11042-016-4159-7},
  number               = {18},
  pages                = {18569--18584},
  volume               = {76},
  abstract             = {Stock market is considered chaotic, complex, volatile and dynamic. Undoubtedly, its prediction is one of the most challenging tasks in time series forecasting. Moreover existing Artificial Neural Network (ANN) approaches fail to provide encouraging results. Meanwhile advances in machine learning have presented favourable results for speech recognition, image classification and language processing. Methods applied in digital signal processing can be applied to stock data as both are time series. Similarly, learning outcome of this paper can be applied to speech time series data. Deep learning for stock prediction has been introduced in this paper and its performance is evaluated on Google stock price multimedia data (chart) from NASDAQ. The objective of this paper is to demonstrate that deep learning can improve stock market forecasting accuracy. For this, (2D)2PCA + Deep Neural Network (DNN) method is compared with state of the art method 2-Directional 2-Dimensional Principal Component Analysis (2D)2PCA + Radial Basis Function Neural Network (RBFNN). It is found that the proposed method is performing better than the existing method RBFNN with an improved accuracy of 4.8\% for Hit Rate with a window size of 20. Also the results of the proposed model are compared with the Recurrent Neural Network (RNN) and it is found that the accuracy for Hit Rate is improved by 15.6\%. The correlation coefficient between the actual and predicted return for DNN is 17.1\% more than RBFNN and it is 43.4\% better than RNN.},
  citeulike-article-id = {14500320},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11042-016-4159-7},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11042-016-4159-7},
  groups               = {FrcstQWIM_TimeSrs, FrcstQWIM_ML, FrcstQWIM_Equity, ML_ForcstTimeSrs},
  posted-at            = {2017-12-11 04:15:22},
  publisher            = {Springer US},
  timestamp            = {2020-02-25 23:59},
}

@Article{Sirignano-Cont-2018,
  author         = {Sirignano, Justin and Cont, Rama},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Universal features of price formation in financial markets: perspectives from deep learning},
  doi            = {10.2139/ssrn.3141294},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3141294},
  abstract       = {Using a large-scale Deep Learning approach applied to a high-frequency database containing billions of electronic market quotes and transactions for US equities, we uncover nonparametric evidence for the existence of a universal and stationary price formation mechanism relating the dynamics of supply and demand for a stock, as revealed through the order book, to subsequent variations in its market price. We assess the model by testing its out-of-sample predictions for the direction of price moves given the history of price and order flow, across a wide range of stocks and time periods. The universal price formation model is shown to exhibit a remarkably stable out-of-sample prediction accuracy across time, for a wide range of stocks from different sectors. Interestingly, these results also hold for stocks which are not part of the training sample, showing that the relations captured by the model are universal and not asset-specific. The universal model --- trained on data from all stocks --- outperforms, in terms of out-of-sample prediction accuracy, asset-specific linear and nonlinear models trained on time series of any given stock, showing that the universal nature of price formation weighs in favour of pooling together financial data from various stocks, rather than designing asset- or sector-specific models as commonly done. Standard data normalizations based on volatility, price level or average spread, or partitioning the training data into sectors or categories such as large/small tick stocks, do not improve training results. On the other hand, inclusion of price and order flow history over many past observations is shown to improve forecasting performance, showing evidence of path-dependence in price dynamics.},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_ForcstTimeSrs, ML_AssetPricing},
  timestamp      = {2020-02-25 23:59},
}

@Article{Sirignano-Cont-2019,
  author         = {Sirignano, Justin and Cont, Rama},
  date           = {2018},
  journaltitle   = {Quantitative Finance},
  title          = {Universal features of price formation in financial markets: perspectives from deep learning},
  doi            = {10.2139/ssrn.3141294},
  issn           = {1556-5068},
  url            = {https://www.tandfonline.com/doi/abs/10.1080/14697688.2019.1622295},
  abstract       = {Using a large-scale Deep Learning approach applied to a high-frequency database containing billions of electronic market quotes and transactions for US equities, we uncover nonparametric evidence for the existence of a universal and stationary price formation mechanism relating the dynamics of supply and demand for a stock, as revealed through the order book, to subsequent variations in its market price. We assess the model by testing its out-of-sample predictions for the direction of price moves given the history of price and order flow, across a wide range of stocks and time periods. The universal price formation model is shown to exhibit a remarkably stable out-of-sample prediction accuracy across time, for a wide range of stocks from different sectors. Interestingly, these results also hold for stocks which are not part of the training sample, showing that the relations captured by the model are universal and not asset-specific. The universal model --- trained on data from all stocks --- outperforms, in terms of out-of-sample prediction accuracy, asset-specific linear and nonlinear models trained on time series of any given stock, showing that the universal nature of price formation weighs in favour of pooling together financial data from various stocks, rather than designing asset- or sector-specific models as commonly done. Standard data normalizations based on volatility, price level or average spread, or partitioning the training data into sectors or categories such as large/small tick stocks, do not improve training results. On the other hand, inclusion of price and order flow history over many past observations is shown to improve forecasting performance, showing evidence of path-dependence in price dynamics.},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM, ML_ForcstTimeSrs, ML_AssetPricing},
  timestamp      = {2020-02-25 23:59},
}

@InCollection{Sun-et-al-2018,
  author         = {Sun, Shaolong and Wei, Yunjie and Wang, Shouyang},
  booktitle      = {Computational science - ICCS 2018},
  date           = {2018},
  title          = {AdaBoost-LSTM Ensemble Learning for Financial Time Series Forecasting},
  doi            = {10.1007/978-3-319-93713-7\_55},
  editor         = {Shi, Yong and Fu, Haohuan and Tian, Yingjie and Krzhizhanovskaya, Valeria V. and Lees, Michael Harold and Dongarra, Jack and Sloot, Peter M. A.},
  isbn           = {978-3-319-93712-0},
  pages          = {590--597},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-319-93713-7\_55},
  urldate        = {2019-09-22},
  volume         = {10862},
  abstract       = {A hybrid ensemble learning approach is proposed to forecast financial time series combining AdaBoost algorithm and Long Short-Term Memory (LSTM) network. Firstly, by using AdaBoost algorithm the database is trained to get the training samples. Secondly, the LSTM is utilized to forecast each training sample separately. Thirdly, AdaBoost algorithm is used to integrate the forecasting results of all the LSTM predictors to generate the ensemble results. Two major daily exchange rate datasets and two stock market index datasets are selected for model evaluation and comparison. The empirical results demonstrate that the proposed AdaBoost-LSTM ensemble learning approach outperforms some other single forecasting models and ensemble learning approaches. This suggests that the AdaBoost-LSTM ensemble learning approach is a highly promising approach for financial time series data forecasting, especially for the time series data with nonlinearity and irregularity, such as exchange rates and stock indexes.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  issn           = {0302-9743},
  timestamp      = {2020-02-25 23:59},
}

@Article{Taieb-et-al-2017,
  author         = {Taieb, Souhaib Ben and Taylor, James W. and Hyndman, Rob J.},
  date           = {2017-07-17},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {Coherent Probabilistic Forecasts for Hierarchical Time Series},
  url            = {http://proceedings.mlr.press/v70/taieb17a.html},
  urldate        = {2019-09-22},
  abstract       = {Many applications require forecasts for a hierarchy comprising a set of time series along with aggregates of subsets of these series. Hierarchical forecasting require not only good prediction accuracy at each level of the hierarchy, but also the coherency between different levels - the property that forecasts add up appropriately across the hierarchy. A fundamental limitation of prior research is the focus on forecasting the mean of each time series. We consider the situation where probabilistic forecasts are needed for each series in the hierarchy, and propose an algorithm to compute predictive distributions rather than mean forecasts only. Our algorithm has the advantage of synthesizing information from different levels in the hierarchy through a sparse forecast combination and a probabilistic hierarchical aggregation. We evaluate the accuracy of our forecasting algorithm on both simulated data and large-scale electricity smart meter data. The results show consistent performance gains compared to state-of-the art methods.},
  day            = {17},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:59},
}

@Article{Tran-et-al-2019,
  author         = {Tran, Dat Thanh and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
  date           = {2019-03-05},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Data-driven Neural Architecture Learning For Financial Time-series Forecasting},
  url            = {https://arxiv.org/abs/1903.06751},
  urldate        = {2019-04-10},
  abstract       = {Forecasting based on financial time-series is a challenging task since most real-world data exhibits nonstationary property and nonlinear dependencies. In addition, different data modalities often embed different nonlinear relationships which are difficult to capture by human-designed models. To tackle the supervised learning task in financial time-series prediction, we propose the application of a recently formulated algorithm that adaptively learns a mapping function, realized by a heterogeneous neural architecture composing of Generalized Operational Perceptron, given a set of labeled data. With a modified objective function, the proposed algorithm can accommodate the frequently observed imbalanced data distribution problem. Experiments on a large-scale Limit Order Book dataset demonstrate that the proposed algorithm outperforms related algorithms, including tensor-based methods which have access to a broader set of input information.},
  day            = {5},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:59},
}

@InCollection{Tsantekidis-et-al-2017,
  author               = {Tsantekidis, Avraam and Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
  booktitle            = {IEEE 19th Conference on Business Informatics (CBI)},
  date                 = {2017-07},
  title                = {Forecasting Stock Prices from the Limit Order Book Using Convolutional Neural Networks},
  doi                  = {10.1109/cbi.2017.23},
  isbn                 = {978-1-5386-3035-8},
  location             = {Thessaloniki, Greece},
  pages                = {7--12},
  publisher            = {IEEE},
  abstract             = {In today's financial markets, where most trades are performed in their entirety by electronic means and the largest fraction of them is completely automated, an opportunity has risen from analyzing this vast amount of transactions. Since all the transactions are recorded in great detail, investors can analyze all the generated data and detect repeated patterns of the price movements. Being able to detect them in advance, allows them to take profitable positions or avoid anomalous events in the financial markets. In this work we proposed a deep learning methodology, based on Convolutional Neural Networks (CNNs), that predicts the price movements of stocks, using as input large-scale, high-frequency time-series derived from the order book of financial exchanges. The dataset that we use contains more than 4 million limit order events and our comparison with other methods, like Multilayer Neural Networks and Support Vector Machines, shows that CNNs are better suited for this kind of task.},
  citeulike-article-id = {14447801},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/cbi.2017.23},
  groups               = {DeepLearning_QWIM, FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  posted-at            = {2017-10-09 12:25:44},
  timestamp            = {2020-02-25 23:59},
}

@InCollection{Tsantekidis-et-al-2017b,
  author               = {Tsantekidis, Avraam and Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
  booktitle            = {25th European Signal Processing Conference (EUSIPCO)},
  date                 = {2017-08},
  title                = {Using deep learning to detect price change indications in financial markets},
  doi                  = {10.23919/eusipco.2017.8081663},
  isbn                 = {978-0-9928626-7-1},
  location             = {Kos, Greece},
  pages                = {2511--2515},
  publisher            = {IEEE},
  abstract             = {Forecasting financial time-series has long been among the most challenging problems in financial market analysis. In order to recognize the correct circumstances to enter or exit the markets investors usually employ statistical models (or even simple qualitative methods). However, the inherently noisy and stochastic nature of markets severely limits the forecasting accuracy of the used models. The introduction of electronic trading and the availability of large amounts of data allow for developing novel machine learning techniques that address some of the difficulties faced by the aforementioned methods. In this work we propose a deep learning methodology, based on recurrent neural networks, that can be used for predicting future price movements from large-scale high-frequency time-series data on Limit Order Books. The proposed method is evaluated using a large-scale dataset of limit order book events.},
  citeulike-article-id = {14503717},
  citeulike-linkout-0  = {http://dx.doi.org/10.23919/eusipco.2017.8081663},
  groups               = {FrcstQWIM_TimeSrs, FrcstQWIM_ML, ML_ForcstTimeSrs, DeepLearning_QWIM},
  posted-at            = {2017-12-16 12:44:30},
  timestamp            = {2020-02-25 23:59},
}

@PhdThesis{Varneskov-2014,
  author               = {Varneskov, Rasmus T.},
  date                 = {2014},
  institution          = {University of Aarhus},
  title                = {Econometric Analysis of Volatility in Financial Additive Noise Models},
  url                  = {https://pure.au.dk/ws/files/83573064/RasmusTangsgaard_Varneskov_PhD_thesis.pdf},
  abstract             = {This thesis comprises four self-contained chapters on the econometric analysis of stochastic asset return volatility.

The first chapter treats estimation of asset return volatility using high-frequency, i.e. tick-by-tick, financial data. In this context, one may quantify the ex-post variability of logarithmic asset returns using the concept of quadratic variation and estimate the latter precisely over a fixed horizon, say one trading day, as the sampling interval between successive observations progressively shrinks. However, when using tick-by-tick data, the observable asset prices deviate from their theoretical counterparts due to market microstructure (MMS) noise, which summarizes an array of market imperfections such as bid-ask bounce effects, asymmetric information and strategic learning, execution of block trades, etc. As a result, previous estimators in this literature are either inconsistent, consistent with optimal asymptotic properties under high-level assumptions on the MMS noise, or consistent with sub-optimal asymptotic properties under general assumptions on the MMS noise.

The second chapter extends the analysis in Chapter 1 to a multivariate setting where the asset prices are not only contaminated by MMS noise, but they are also allowed to be observed randomly and non-synchronously. In this setting, the flat-top realized kernels are shown to estimate the quadratic covariation matrix with asymptotic properties that resemble those described above for the univariate case. A simple finite sample correction based on projections of symmetric matrices that ensures positive definiteness of the estimators without altering their asymptotic properties is also proposed. An empirically motivated simulation study demonstrates that the flat-top realized kernels have a desirable combination of robustness and efficiency relative to competing estimators. Finally, an empirical analysis for a portfolio of six stocks of varying size and liquidity illustrates that the use of flat-top realized kernels leads to better in-sample signal detection and out-of-sample predictions of the quadratic covariation matrix compared to other estimators in the literature. Once we have a time series of, for example, daily asset return volatility estimates, the third chapter - co-authored with Professor Pierre Perron (Boston University) - is concerned with its dynamic modeling and forecasting. In particular, we propose a parametric state space model of asset return volatility with an accompanying estimation and forecasting framework that allows for ARFIMA dynamics, random level shifts, and measurement errors. The Kalman filter is first used to construct the state-augmented likelihood function and subsequently to generate forecasts, which are mean- and path-corrected. We apply our model to eight daily volatility series constructed from both high-frequency and daily returns, covering three asset classes. We find that random level shifts are present in all series, while genuine long memory is only present in the series constructed from high-frequency data. Finally, we show that our model provides significant gains in forecast accuracy relative to competing models in the literature. These gains can be very pronounced, especially at longer forecast horizons},
  citeulike-article-id = {14025509},
  groups               = {FrcstQWIM_MedLngTerm, ML_ForcstTimeSrs, ML_AssetPricing},
  owner                = {cristi},
  posted-at            = {2016-05-01 15:30:42},
  timestamp            = {2020-02-25 23:59},
}

@Article{Wang-et-al-2019f,
  author         = {Wang, Yuyang and Smola, Alex and Maddix, Danielle and Gasthaus, Jan and Foster, Dean and Januschowski, Tim},
  date           = {2019-05-24},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {Deep Factors for Forecasting},
  url            = {http://proceedings.mlr.press/v97/wang19k},
  urldate        = {2019-09-20},
  abstract       = {Producing probabilistic forecasts for large collections of similar and/or dependent time series is a practically highly relevant, yet challenging task. Classical time series models fail to capture complex patterns in the data and multivariate techniques struggle to scale to large problem sizes, but their reliance on strong structural assumptions makes them data-efficient and allows them to provide estimates of uncertainty. The converse is true for models based on deep neural networks, which can learn complex patterns and dependencies given enough data. In this paper, we propose a hybrid model that incorporates the benefits of both approaches. Our new method is data-driven and scalable via a latent, global, deep component. It also handles uncertainty through a local classical model. We provide both theoretical and empirical evidence for the soundness of our approach through a necessary and sufficient decomposition of exchangeable time series into a global and a local part and extensive experiments. Our experiments demonstrate the advantages of our model both in term of data efficiency and computational complexity.},
  day            = {24},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:59},
}

@Article{Wang-Leung-2018,
  author         = {Wang, Ziwei and Leung, Nelson},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Factor Selection with Deep Reinforcement Learning for Financial Forecasting},
  doi            = {10.2139/ssrn.3128678},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3128678},
  abstract       = {The selection of relevant factor set is essential for obtaining a financial forecasting model with high information coefficient (IC), and is challenging due to the non-trivial interaction between factors and their vast combinatorial search space. We introduce a factor selection algorithm using a value network to evaluate the ICs of factor sets. With the fitness function provided by the network evaluation, a genetic algorithm is employed to select a population of candidate factor sets. The ICs of these sets are computed with historical time series data, and the results are subsequently used to train the value network. Such reinforcement iteration can be employed repeatedly to simultaneously improve both the best IC and value network evaluation accuracy. Our approach is generic to any prediction model involving factor selection. To characterize the performance of our algorithm, we employed a typical robust linear regression model with commonly available factors. Although the algorithm's learning process is much like a black-box, the optimal results it delivered can in fact provide useful insights to quantitative researchers. With standard factors available in the financial industry, we achieved a monthly IC of 0.070 on the global market. In comparison, the forward selection and backward elimination algorithms achieved monthly IC of less than 0.05. With a simple portfolio rebalance strategy (monthly rebalance [long/short] with equal weight of the stocks in the [top/bottom] decile of the predicted return), the investment portfolio resulted from our prediction model achieved an average annualized return of 31.17\% with annualized standard deviation of 18.39\% on the global market for 2004-2016.},
  f1000-projects = {QuantInvest},
  groups         = {Factor_Selection, FrcstQWIM_ML, FrcstQWIM_Equity, ML_ReinfoLrng, ML_ForcstTimeSrs, ML_InvestSelect, DeepLearning_QWIM},
  timestamp      = {2020-02-25 23:59},
}

@Article{Widodo-et-al-2016,
  author               = {Widodo, Agus and Budi, Indra and Widjaja, Belawati},
  date                 = {2016},
  journaltitle         = {International Journal of Machine Learning and Cybernetics},
  title                = {Automatic lag selection in time series forecasting using multiple kernel learning},
  doi                  = {10.1007/s13042-015-0409-7},
  number               = {1},
  pages                = {95--110},
  volume               = {7},
  abstract             = {This paper reports the feasibility of employing the recent approach on kernel learning, namely the multiple kernel learning (MKL), for time series forecasting to automatically select the optimal lag length or size of sliding windows. MKL is an approach to choose suitable kernels from a given pool of kernels by exploring the combination of multiple kernels. In this paper, we extend the MKL capability to select the optimal size of sliding windows for time series domain by adopting the data integration approach which has been previously studied in the domain of image processing. In this study, each kernel represents the different lengths of time series lag. In addition, we also examine the feasibility of MKL for decomposed time series. We use the dataset from previous time series competitions as our benchmark. Our experimental results indicate that our approaches perform competitively compared to the previous methods using the same dataset. Furthermore, MKL may predict the detrended time series without explicitly computing the seasonality. The advantage of our method is in its ability in automatically selecting the optimal size of sliding windows and finding the pattern of time series.},
  citeulike-article-id = {13995871},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s13042-015-0409-7},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s13042-015-0409-7},
  groups               = {ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-04-04 01:05:14},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-25 23:59},
}

@Article{Xie-et-al-2017a,
  author               = {Xue, Jingming and Zhou, SiHang and Liu, Qiang and Liu, Xinwang and Yin, Jianping},
  date                 = {2017-09},
  journaltitle         = {Neurocomputing},
  title                = {Financial time series prediction using l 2,1 RF-ELM},
  doi                  = {10.1016/j.neucom.2017.04.076},
  issn                 = {0925-2312},
  abstract             = {Financial time series forecasting is a complicated task because the behavior of investors can be influenced by lots of tiny and unpredictable factors. In this paper, in order to maximize the return of capital and manage liquidity risk effectively, an l2,1-norm and Random Fourier Mapping based Extreme Learning Machine(l2,1 RF-ELM) is applied to the problem of financial time series prediction. The advantages of ELM in efficiency and generalization performance over traditional fuzzy neural network(FNN) algorithms have been demonstrated on a wide range of problems from different fields, thanks to the integration of l2,1-norm, the l2,1 RF-ELM is able to automatically prune the irrelevant and redundant hidden neurons to form a more discriminative and compact hidden layer. The performance of the l2,1RF-ELM is compared with other hidden layer enforcement algorithms, two long-term time series data sets, including TianChi and BCS, are used for this comparison. The performance of the l2,1RF-ELM was comparable to those of other widely used machine learning techniques like support vector machines (SVM), artificial neural networks (ANN) and other popular ELM method. The experiments demonstrate favorable prediction results of the l2,1RF-ELM in terms of annualized return, prediction error and running time. In addition, we also find that the underlying rules of the correlation between cash inflow and outflow that can help us improve accuracy, which is valuable for financial institutions to predict the trend of liquidity.},
  citeulike-article-id = {14500344},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neucom.2017.04.076},
  groups               = {ML_Forecast_QWIM, FrcstQWIM_TimeSrs, ML_ForcstTimeSrs},
  posted-at            = {2017-12-11 05:10:40},
  timestamp            = {2020-02-25 23:59},
}

@InCollection{Yang-et-al-2019a,
  author         = {Yang, Maoxin and Hu, Qinghua and Wang, Yun},
  booktitle      = {Artificial neural networks and machine learning - ICANN 2019},
  date           = {2019},
  title          = {Multi-task Learning Method for Hierarchical Time Series Forecasting},
  doi            = {10.1007/978-3-030-30490-4\_38},
  editor         = {Tetko, Igor V. and Kurkova, Vera and Karpov, Pavel and Theis, Fabian},
  isbn           = {978-3-030-30489-8},
  pages          = {474--485},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-30490-4\_38},
  urldate        = {2019-09-22},
  volume         = {11730},
  abstract       = {Hierarchical time series is a set of time series organized by aggregation constraints and it is widely used in many real-world applications. Usually, hierarchical time series forecasting can be realized with a two-step method, in which all time series are forecasted independently and then the forecasting results are reconciled to satisfy aggregation consistency. However, these two-step methods have a high computational complexity and are unable to ensure optimal forecasts for all time series. In this paper, we propose a novel hierarchical forecasting approach to solve the above problems. Based on multi-task learning, we construct an integrated model that combines features of the bottom level series and the hierarchical structure. Then forecasts of all time series are output simultaneously and they are aggregated consistently. The model has the advantage of utilizing the correlation between time series. And the forecasting results are overall optimal by optimizing a global loss function. In order to avoid the curse of dimensionality as the number of time series grows larger, we further learn a sparse model with group sparsity and element-wise sparsity constraints according to data characteristics. The experimental results on simulation data and tourism data demonstrate that our method has a better overall performance while simplifying forecasting process.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  issn           = {0302-9743},
  timestamp      = {2020-02-25 23:59},
}

@Article{Ye-Dai-2018,
  author         = {Ye, Rui and Dai, Qun},
  date           = {2018-09},
  journaltitle   = {Knowledge-Based Systems},
  title          = {A novel transfer learning framework for time series forecasting},
  doi            = {10.1016/j.knosys.2018.05.021},
  issn           = {0950-7051},
  pages          = {74--99},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S095070511830251X}},
  urldate        = {2019-10-05},
  volume         = {156},
  abstract       = {Abstract Recently, many excellent algorithms for time series prediction issues have been proposed, most of which are developed based on the assumption that sufficient training data and testing data under the same distribution are available. However, in reality, time-series data usually exhibit some kind of time-varying characteristic, which may lead to a wide variability between old data and new data. Hence, how to transfer knowledge over a long time span, when addressing time series prediction issues, poses serious challenges. To solve this problem, in this paper, a hybrid algorithm based on transfer learning, Online Sequential Extreme Learning Machine with Kernels (OS-ELMK), and ensemble learning, abbreviated as TrEnOS-ELMK, is proposed, along with its precise mathematic derivation. It aims to make the most of, rather than discard, the adequate long-ago data, and constructs an algorithm framework for transfer learning in time series forecasting, which is groundbreaking. Inspired by the preferable performance of models ensemble, ensemble learning scheme is also incorporated into our proposed algorithm, where the weights of the constituent models are adaptively updated according to their performances on fresh samples. Compared to many existing time series prediction methods, the newly proposed algorithm takes long-ago data into consideration and can effectively leverage the latent knowledge implied in these data for current prediction. In addition, TrEnOS-ELMK naturally inherits merits of both OS-ELMK and ensemble learning due to its incorporation of the two techniques. Experimental results on three synthetic and six real-world datasets demonstrate the effectiveness of the proposed algorithm.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs},
  timestamp      = {2020-02-25 23:59},
}

@Article{Yumlu-et-al-2015,
  author               = {Yumlu, M. Serdar and G urgen, Fikret S. and Cemgil, A. Taylan and Okay, Nesrin},
  date                 = {2015-05},
  journaltitle         = {Digital Signal Processing},
  title                = {Bayesian changepoint and time-varying parameter learning in regime switching volatility models},
  doi                  = {10.1016/j.dsp.2015.02.001},
  issn                 = {1051-2004},
  pages                = {198--212},
  volume               = {40},
  abstract             = {This paper proposes a combined state and piecewise time-varying parameter learning technique in regime switching volatility models using multiple changepoint detection. This approach is a Sequential Monte Carlo method for estimating GARCH and EGARCH based volatility models with an unknown number of changepoints. Modern auxiliary particle filtering techniques are used to calculate the posterior densities and online forecasts. This approach also automatically deals with the common ancestral path dependence problem faced in these type volatility models. The model is tested on Borsa Istanbul (BIST) formerly known as Istanbul Stock Exchange (ISE) market data using daily log returns. A full structural changepoint specification is defined in which all parameters of the conditional variance of the volatility models are dynamic. Finally, it is shown with simulation experiments that the proposed approach partitions the series into several regimes and learns the parameters of each regime's volatility model in parallel with the multiple changepoint detection process.},
  citeulike-article-id = {13995938},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.dsp.2015.02.001},
  groups               = {ChngPoints_TimeSrs, ML_ForcstTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-04-04 03:10:29},
  timestamp            = {2020-02-25 23:59},
}

@Article{Zang-2017,
  author               = {Zang, Chuanyun},
  date                 = {2017-10-12},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Deep Learning in Multiple Multistep Time Series Prediction},
  eprint               = {1710.04373},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1710.04373},
  abstract             = {The project aims to research on combining deep learning specifically Long-Short Memory (LSTM) and basic statistics in multiple multistep time series prediction. LSTM can dive into all the pages and learn the general trends of variation in a large scope, while the well selected medians for each page can keep the special seasonality of different pages so that the future trend will not fluctuate too much from the reality. A recent Kaggle competition on 145K Web Traffic Time Series Forecasting [1] is used to thoroughly illustrate and test this idea.},
  citeulike-article-id = {14503298},
  citeulike-linkout-0  = {http://arxiv.org/abs/1710.04373},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1710.04373},
  day                  = {12},
  groups               = {FrcstQWIM_ML, ML_ForcstTimeSrs},
  posted-at            = {2017-12-15 07:25:27},
  timestamp            = {2020-02-25 23:59},
}

@Article{Messmer-2017,
  author               = {Messmer, Marcial},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Deep Learning and the Cross-Section of Expected Returns},
  url                  = {https://ssrn.com/abstract=3081555},
  abstract             = {Deep learning is an active area of research in machine learning. I train deep feedforward neural networks (DFN) based on a set of 68 firm characteristics (FC) to predict the US cross-section of stock returns. After applying a network optimization strategy, I find that DFN long-short portfolios can generate attractive risk-adjusted returns compared to a linear benchmark. These findings underscore the importance of non-linear relationships among FC and expected returns. The results are robust to size, weighting schemes and portfolio cutoff points. Moreover, I show that price related FC, namely, short-term reversal and the twelve-months momentum, are among the main drivers of the return predictions. The majority of FC play a minor role in the variation of these predictions.},
  citeulike-article-id = {14499314},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3081555},
  groups               = {Machine learning and investment strategies, Characteristics and return prediction, DeepLearning_QWIM},
  posted-at            = {2017-12-08 17:35:18},
  timestamp            = {2020-02-26 00:14},
}

@Book{Hyndman-Athanasopoulos-2019,
  author    = {Rob J. Hyndman and George Athanasopoulos},
  date      = {2019},
  title     = {Forecasting: Principles and Practice},
  edition   = {Third Edition},
  publisher = {OTexts},
  url       = {https://otexts.com/fpp3/},
  abstract  = {Forecasting is required in many situations. Deciding whether to build another power generation plant in the next five years requires forecasts of future demand. Scheduling staff in a call centre next week requires forecasts of call volumes. Stocking an inventory requires forecasts of stock requirements. Telecommunication routing requires traffic forecasts a few minutes ahead. Whatever the circumstances or time horizons involved, forecasting is an important aid in effective and efficient planning. This textbook provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to use them sensibly. Examples use R with many data sets taken from the authors' own consulting experience. In this second edition, all chapters have been updated to cover the latest research and forecasting methods. Three new chapters have been added on dynamic regression forecasting, hierarchical forecasting and practical forecasting issues. The latest version of the book is freely available online at http://OTexts.com/fpp2.},
  timestamp = {2020-02-26 00:15},
}

@Article{Botchkarev-2019,
  author         = {Alexei Botchkarev},
  date           = {2019},
  journaltitle   = {Interdisciplinary Journal of Information, Knowledge, and Management},
  title          = {A new typology design of performance metrics to measure errors in machine learning regression algorithms},
  doi            = {10.28945/4184},
  issn           = {1555-1229},
  pages          = {045--076},
  url            = {https://www.informingscience.org/Publications/4184},
  urldate        = {2019-09-20},
  volume         = {14},
  abstract       = {Aim/Purpose: The aim of this study was to analyze various performance metrics and approaches to their classification. The main goal of the study was to develop a new typology that will help to advance knowledge of metrics and facilitate their use in machine learning regression algorithms Background: Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. A performance metric can be defined as a logical and mathematical construct designed to measure how close are the actual results from what has been expected or predicted. A vast variety of performance metrics have been described in academic literature. The most commonly mentioned metrics in research studies are Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), etc. Knowledge about metrics properties needs to be systematized to simplify the design and use of the metrics. Methodology: A qualitative study was conducted to achieve the objectives of identifying related peer-reviewed research studies, literature reviews, critical thinking and inductive reasoning. Contribution: The main contribution of this paper is in ordering knowledge of performance metrics and enhancing understanding of their structure and properties by proposing a new typology, generic primary metrics mathematical formula and a visualization chart Findings: Based on the analysis of the structure of numerous performance metrics, we proposed a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set. For each component, implementation options have been identified. The suggested new typology has been shown to cover a total of over 40 commonly used primary metrics Recommendations for Practitioners: Presented findings can be used to facilitate teaching performance metrics to university students and expedite metrics selection and implementation processes for practitioners Recommendation for Researchers: By using the proposed typology, researchers can streamline development of new metrics with predetermined properties Impact on Society: The outcomes of this study could be used for improving evaluation results in machine learning regression, forecasting and prognostics with direct or indirect positive impacts on innovation and productivity in a societal sense Future Research: Future research is needed to examine the properties of the extended metrics, composite metrics, and hybrid sets of metrics. Empirical study of the metrics is needed using R Studio or Azure Machine Learning Studio, to find associations between the properties of primary metrics and their behavior in a wide spectrum of data characteristics and business or research requirements},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-26 00:16},
}

@Article{Siliverstovs-2017a,
  author               = {Siliverstovs, Boriss},
  date                 = {2017-01},
  journaltitle         = {Economic Modelling},
  title                = {Dissecting models' forecasting performance},
  doi                  = {10.1016/j.econmod.2017.01.008},
  issn                 = {0264-9993},
  abstract             = {Examine differences in models' forecast accuracy observation by observation. Forecast accuracy measures based on error averaging (e.g. MSFE) may be misleading. Recursively computed measures of forecast accuracy are more informative. The fact that the predictive performance of models used in forecasting stock returns, exchange rates, and macroeconomic variables is not stable and varies over time has been widely documented in the forecasting literature. Under these circumstances excessive reliance on forecast evaluation metrics that ignores this instability in forecasting accuracy, like squared errors averaged over the whole forecast evaluation sample, masks important information regarding the temporal evolution of relative forecasting performance of competing models. In this paper we suggest an approach based on the combination of the Cumulated Sum of Squared Forecast Error Differential (CSSFED) of Welch and Goyal (2008) and the Bayesian change point analysis of Barry and Hartigan (1993) that tracks the contribution of forecast errors to the aggregate measures of forecast accuracy observation by observation. In doing so, it allows one to track the evolution of the relative forecasting performance over time. We illustrate the suggested approach by using forecasts of the GDP growth rate in Switzerland.},
  citeulike-article-id = {14351917},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.econmod.2017.01.008},
  posted-at            = {2017-05-06 05:04:26},
  timestamp            = {2020-02-26 00:16},
}

@Article{deKok-2017,
  author               = {{de Kok}, Stefan},
  date                 = {2017},
  journaltitle         = {Foresight: international journal of applied forecasting},
  title                = {The Quest for a Better Forecast Error Metric: Measuring More than the Average Error},
  number               = {46},
  url                  = {https://foresight.forecasters.org/the-quest-for-a-better-forecast-error-metric/},
  abstract             = {In an article in the Summer 2006 issue of Foresight, Tom Willemain presented the argument that "While most forecast-error metrics are averages of forecast errors, for intermittent demand series we should focus on the demand distribution and assess forecast error at each distinct level of demand. Accordingly, the appropriate accuracy metric will assess the difference between the actual and forecasted distributions of demand".

The issue has not had much traction over the years since - except perhaps in energy studies - and we do not find error metrics based on full distributions present in forecasting support systems.

Now Stefan de Kok picks up the argument and extends it to develop an error metric- Total Percentage Error- that measures the full range of uncertainty in our forecasts and, in doing so, both enables better inventory - planning and provides a more comprehensive way to gauge the quality of the forecast},
  citeulike-article-id = {14486404},
  posted-at            = {2017-11-30 18:17:01},
  timestamp            = {2020-02-26 00:17},
}

@Article{Hsiao-Wan-2014,
  author               = {Hsiao, Cheng and Wan, Shui K.},
  date                 = {2014-01},
  journaltitle         = {Journal of Econometrics},
  title                = {Is there an optimal forecast combination?},
  doi                  = {10.1016/j.jeconom.2013.11.003},
  issn                 = {0304-4076},
  pages                = {294--309},
  volume               = {178},
  abstract             = {We consider several geometric approaches for combining forecasts in large samples-a simple eigenvector approach, a mean corrected eigenvector and trimmed eigenvector approach. We give conditions where geometric approach yields identical result as the regression approach. We also consider a mean and scale corrected simple average of all predictive models for finite sample and give conditions where simple average is an optimal combination. Monte Carlos are conducted to compare the finite sample performance of these and some popular forecast combination and information combination methods and to shed light on the issues of "forecast combination"vs "information combination". We also try to shed light on whether there exists an optimal forecast combination method by comparing various forecast combination methods to predict US real output growth rate and excess equity premium.},
  citeulike-article-id = {14340740},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jeconom.2013.11.003},
  groups               = {FcstQWIM_Equity},
  posted-at            = {2017-04-20 12:13:13},
  timestamp            = {2020-02-26 00:18},
}

@Article{Cheng-et-al-2019,
  author         = {Cheng, Mingmian and Swanson, Norman R. and Yao, Chun},
  date           = {2019-01-30},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Forecast Evaluation},
  url            = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3440328},
  urldate        = {2019-09-07},
  abstract       = {The development of new tests and methods used in the evaluation of time series forecasts and forecasting models remains as important today as it has for the last 50 years. Paraphrasing what Sir Clive W.J. Granger (arguably the father of modern day time series forecasting) said in the 1990s at a conference in Svinkloev, Denmark, , the model looks like an interesting extension, but can it forecast better than existing models. Indeed, the forecast evaluation literature continues to expand, with interesting new tests and methods being developed at a rapid pace. In this chapter, we discuss a select a group of predictive accuracy tests and model selection methods that have been developed in recent years, and that are now widely used in the forecasting literature. We begin by reviewing several tests for comparing the relative forecast accuracy of different models, in the case of point forecasts. We then broaden the scope of our discussion by introducing density-based predictive accuracy tests. We conclude by noting that predictive accuracy is typically assessed in terms of a given loss function, such as mean squared forecast error or mean absolute forecast error. Most tests, including those discussed here, are consequently loss function dependent, and the relative forecast superiority of predictive models is therefore also dependent on specification of a loss function. In light of this fact, we conclude this chapter by discussing loss function robust predictive density accuracy tests that have recently been developed using principles of stochastic dominance.},
  day            = {30},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-26 00:18},
}

@Article{Chiu-et-al-2019,
  author         = {Chiu, Ching-Wai and Hayes, Simon and Kapetanios, George and Theodoridis, Konstantinos},
  date           = {2019-05},
  journaltitle   = {International Journal of Forecasting},
  title          = {A new approach for detecting shifts in forecast accuracy},
  doi            = {10.1016/j.ijforecast.2019.01.008},
  issn           = {0169-2070},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019300706},
  urldate        = {2019-09-01},
  abstract       = {Forecasts play a critical role at inflation-targeting central banks, such as the Bank of England. Breaks in the forecast performance of a model can potentially incur important policy costs. Commonly used statistical procedures, however, implicitly put a lot of weight on type I errors (or false positives), which result in a relatively low power of tests to identify forecast breakdowns in small samples. We develop a procedure which aims at capturing the policy cost of missing a break. We use data-based rules to find the test size that optimally trades off the costs associated with false positives with those that can result from a break going undetected for too long. In so doing, we also explicitly study forecast errors as a multivariate system. The covariance between forecast errors for different series, though often overlooked in the forecasting literature, not only enables us to consider testing in a multivariate setting but also increases the test power. As a result, we can tailor the choice of the critical values for each series not only to the in-sample properties of each series but also to how the series for forecast errors covary.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-26 00:19},
}

@Article{Granziera-Sekhposyan-2019,
  author         = {Granziera, Eleonora and Sekhposyan, Tatevik},
  date           = {2019-05},
  journaltitle   = {International Journal of Forecasting},
  title          = {Predicting relative forecasting performance: An empirical investigation},
  doi            = {10.1016/j.ijforecast.2019.01.010},
  issn           = {0169-2070},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019300780},
  urldate        = {2019-09-01},
  abstract       = {The relative performance of forecasting models changes over time. This empirical observation raises two questions: is the relative performance itself predictable? If so, can it be exploited to improve forecast accuracy? We address these questions by evaluating the predictive ability of a wide range of economic variables for two key US macroeconomic aggregates, industrial production and inflation, relative to simple benchmarks. We find that business indicators, financial conditions, uncertainty as well as measures of past relative performance are generally useful for explaining the relative forecasting performance of the models. We further conduct a pseudo-real-time forecasting exercise, where we use the information about the conditional performance for model selection and model averaging. The newly proposed strategies deliver sizable improvements over competitive benchmark models and commonly used combination schemes. Gains are larger when model selection and averaging are based on financial conditions as well as past performance measured at the forecast origin date.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-26 00:19},
}

@Article{Gibbs-2017,
  author         = {Gibbs, Christopher G.},
  date           = {2017},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Conditionally Optimal Weights and Forward-Looking Approaches to Combining Forecasts},
  doi            = {10.2139/ssrn.2919117},
  issn           = {1556-5068},
  url            = {http://www.ssrn.com/abstract=2919117},
  urldate        = {2019-09-22},
  abstract       = {In applied forecasting, there is a trade-off between in-sample t and out-of-sample forecast accuracy. Parsimonious model specifi cations typically outperform richer model specifications. Consequently, there is often predictable information in forecast errors that is difficult to exploit. However, we show how this predictable information can be exploited in forecast combinations. Optimal combination weights should minimize conditional mean squared error, or a conditional loss function, rather than the unconditional variance as in the commonly used framework of Bates and Granger (1969). We prove that our conditionally optimal weights lead to better forecast performance than other approaches. The conditionally optimal weights support forward-looking approaches to combining forecasts, where the forecast weights depend on the expected model performance. We show that a forward-looking approach can robustly outperform other strategies, including equal weights, in real-time out-of-sample forecasting exercises for a range of macroaggregates in advanced economies.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-26 00:20},
}

@Article{Hyndman-Koehler-2006,
  author               = {Hyndman, Rob J. and Koehler, Anne B.},
  date                 = {2006-10},
  journaltitle         = {International Journal of Forecasting},
  title                = {Another look at measures of forecast accuracy},
  doi                  = {10.1016/j.ijforecast.2006.03.001},
  issn                 = {0169-2070},
  number               = {4},
  pages                = {679--688},
  volume               = {22},
  abstract             = {We discuss and compare measures of accuracy of univariate time series forecasts. The methods used in the M-competition as well as the M3-competition, and many of the measures recommended by previous authors on this topic, are found to be degenerate in commonly occurring situations. Instead, we propose that the mean absolute scaled error become the standard measure for comparing forecast accuracy across multiple time series.},
  citeulike-article-id = {1560715},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2006.03.001},
  citeulike-linkout-1  = {http://www.sciencedirect.com/science/article/B6V92-4K18VK5-1/2/e0858d572e38e2fff534e71a15150119},
  owner                = {zkgst0c},
  posted-at            = {2016-06-06 19:35:09},
  timestamp            = {2020-02-26 00:21},
}

@Article{Jin-et-al-2015,
  author               = {Jin, Sainan and Corradi, Valentina and Swanson, Norman R.},
  date                 = {2015-05},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Robust Forecast Comparison},
  url                  = {https://ssrn.com/abstract=2605927},
  abstract             = {Forecast accuracy is typically measured in terms of a given loss function. However, as a consequence of the use of misspecified models in multiple model comparisons, relative forecast rankings are loss function dependent. This paper addresses this issue by using a novel criterion for forecast evaluation which is based on the entire distribution of forecast errors.

We introduce the concepts of general-loss (GL) forecast superiority and convex-loss (CL) forecast superiority, and we establish a mapping between GL (CL) superiority and first (second) order stochastic dominance. This allows us to develop a forecast evaluation procedure based on an out-of-sample generalization of the tests introduced by Linton, Maasoumi and Whang (2005). The asymptotic null distributions of our test statistics are nonstandard, and resampling procedures are used to obtain the critical values. Additionally, the tests are consistent and have nontrivial local power under a sequence of local alternatives.

In addition to the stationary case, we outline theory extending our tests to the case of heterogeneity induced by distributional change over time. Monte Carlo simulations suggest that the tests perform reasonably well in finite samples; and an application to exchange rate data indicates that our tests can help identify superior forecasting models, regardless of loss function.},
  citeulike-article-id = {13926602},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2605927},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2605927code76028.pdf?abstractid=2605927 and mirid=1},
  day                  = {14},
  groups               = {Robust_Stat},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2605927},
  owner                = {zkgst0c},
  posted-at            = {2016-02-06 04:21:21},
  timestamp            = {2020-02-26 00:21},
}

@Article{Kim-Durmaz-2012,
  author               = {Kim, Hyeongwoo and Durmaz, Nazif},
  date                 = {2012-07},
  journaltitle         = {International Journal of Forecasting},
  title                = {Bias correction and out-of-sample forecast accuracy},
  doi                  = {10.1016/j.ijforecast.2012.02.009},
  issn                 = {0169-2070},
  number               = {3},
  pages                = {575--586},
  volume               = {28},
  abstract             = {We evaluate the usefulness of bias-correction methods for autoregressive (AR) models in enhancing the out-of-sample forecast accuracy. We employ two popular methods, proposed by Hansen (1999) and So and Shin (1999). Our Monte Carlo simulations show that these methods do not necessarily achieve better forecasting performances than the bias-uncorrected least squares (LS) method, because bias correction increases the variance of the estimator.

Both the bias and the relative variance tend to decrease as the sample size (TT) increases, meaning that larger numbers of observations do not always imply gains from bias-correction. As the degree of persistence increases, the bias becomes greater while the relative variance becomes smaller, which implies a greater gain from correcting for bias for highly persistent data. We also provide real data applications that confirm our major findings overall.},
  citeulike-article-id = {10660038},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2012.02.009},
  owner                = {cristi},
  posted-at            = {2016-02-15 07:24:47},
  timestamp            = {2020-02-26 00:22},
}

@Article{Samuels-Sekkel-2017,
  author               = {Samuels, Jon D. and Sekkel, Rodrigo M.},
  date                 = {2017-01},
  journaltitle         = {International Journal of Forecasting},
  title                = {Model Confidence Sets and forecast combination},
  doi                  = {10.1016/j.ijforecast.2016.07.004},
  issn                 = {0169-2070},
  number               = {1},
  pages                = {48--60},
  volume               = {33},
  abstract             = {A longstanding finding in the forecasting literature is that averaging the forecasts from a range of models often improves upon forecasts based on a single model, with equal weight averaging working particularly well. This paper analyzes the effects of trimming the set of models prior to averaging. We compare different trimming schemes and propose a new approach based on Model Confidence Sets that takes into account the statistical significance of the out-of-sample forecasting performance. In an empirical application to the forecasting of U.S. macroeconomic indicators, we find significant gains in out-of-sample forecast accuracy from using the proposed trimming method.},
  citeulike-article-id = {14219685},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2016.07.004},
  owner                = {cristi},
  posted-at            = {2016-12-04 19:40:46},
  timestamp            = {2020-02-26 00:22},
}

@Article{Franses-2016,
  author               = {Franses, Philip H.},
  date                 = {2016-01},
  journaltitle         = {International Journal of Forecasting},
  title                = {A note on the Mean Absolute Scaled Error},
  doi                  = {10.1016/j.ijforecast.2015.03.008},
  issn                 = {0169-2070},
  number               = {1},
  pages                = {20--22},
  volume               = {32},
  abstract             = {Hyndman and Koehler (2006) recommend that the Mean Absolute Scaled Error (MASE) should become the standard when comparing forecast accuracies. This note supports their claim by showing that the MASE fits nicely within the standard statistical procedures initiated by Diebold and Mariano (1995) for testing equal forecast accuracies. Various other criteria do not fit, as they do not imply the relevant moment properties, and this is illustrated in some simulation experiments.},
  citeulike-article-id = {14030243},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2015.03.008},
  owner                = {cristi},
  posted-at            = {2016-05-08 23:59:44},
  timestamp            = {2020-02-26 00:23},
}

@Article{Makridakis-et-al-2018a,
  author         = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  date           = {2018-06},
  journaltitle   = {International Journal of Forecasting},
  title          = {The M4 Competition: Results, findings, conclusion and way forward},
  doi            = {10.1016/j.ijforecast.2018.06.001},
  issn           = {0169-2070},
  number         = {4},
  pages          = {802--808},
  volume         = {34},
  abstract       = {Abstract The M4 competition is the continuation of three previous competitions started more than 45 years ago whose purpose was to learn how to improve forecasting accuracy, and how such learning can be applied to advance the theory and practice of forecasting. The purpose of M4 was to replicate the results of the previous ones and extend them into three directions: First significantly increase the number of series, second include Machine Learning (ML) forecasting methods, and third evaluate both point forecasts and prediction intervals. The five major findings of the M4 Competitions are: 1. Out Of the 17 most accurate methods, 12 were of mostly statistical approaches. 2. The biggest surprise was a approach that utilized both statistical and ML features. This method average sMAPE was close to 10\% more accurate than the combination benchmark used to compare the submitted methods. 3. The second most accurate method was a combination of seven statistical methods and one ML one, with the weights for the averaging being calculated by a ML algorithm that was trained to minimize the forecasting. 4. The two most accurate methods also achieved an amazing success in specifying the 95\% prediction intervals correctly. 5. The six pure ML methods performed poorly, with none of them being more accurate than the combination benchmark and only one being more accurate than Naive2. This paper presents some initial results of M4, its major findings and a logical conclusion. Finally, it outlines what the authors consider to be the way forward for the field of forecasting.},
  f1000-projects = {QuantInvest},
  groups         = {ML_Forecast_QWIM},
  timestamp      = {2020-02-26 00:25},
}

@Comment{jabref-meta: databaseType:biblatex;}
